Introduction:
- Objectif tester l'inference
- Limitation systeme embarqué
- Benchmark carte NVIDIA jetson
- Quel type de tache choisir pour notre objectif (suivre un object, signe de la main, suivre une ligne...)
- Pas de données dispo donc il faut en acquérir
- Comment communiquer avec le robot (ssh...)
- Comment relier nos modèles avec le robot (scripts...)

Partie 1:
- Bibliographie (recherche des applications existantes avec jetson)
- Peut on utiliser des algorithmes de traitement d'image
- On se rend compte que ca ne fonctionne pas très bien avec des paramètres fixes (notebook traitement d'image)
- Recherche sur fonctionnement des CNN
- Recherche sur fonctionnement de PyTorch
- Utilisation du transfer learning
- Recherche de différents modèles de computer vision (YOLO/R-CNN/VGG/RESNET...)
- Recipe for training NN de Karpathy

Partie 2:
- Benchmark pour voir le type de tache qu'on peut faire avec la jetson
- Utilisation de dataset "jouet" (ciphar10: classification, caltech101: bbox...)
- Décision par rapport aux performances et au temps pris pour l'inference en FPS
- Utilisation du format ONNX qui multiplie environ par 2 les performances en inférence
- Décision par rapport à la mémoire de la jetson (nombre de paramètre des modèles)
- Quels sont les modèles sur lequel se reposer pour le transfer learning

Partie 3:
- On a choisit de faire des signes de mains pour controler le robot
- Est ce qu'on fait du one stage ou two stage ? (donc 3 models et dataset requis)
- Il faut qu'on ait des données donc construction de 3 dataset (script + reflexion sur la construction)
- Reflexion sur le choix des augmentations, le choix du valid set, le test en inference...
- Montrer quelques résultats + reflexion sur choix des hyper-parametres, de la regularisation...
- Comprendre comment optimiser les modèles :
	- visualisation des metrics par classe pour verifier qu'une classe n'a pas de problemes
	- visualisation des sorties (bbox ou clf) pour essayer de comprendre quels sont les cas posant problème
	- test en temps réel avec webcam pour voir quand est ce que le modèle se trompe
	- beaucoup d'experimentations en lancant des modèles sur kaggle/colab/home
- Le one-stage classification n'est pas concluant
- Choix du two stage classification avec un réseau bbox et un réseau clf

Conclusion:
- Script en inference permettant de bbox + clf et de controler le robot
- Le réseaux classifier fonctionne très bien mais le bbox en régression est plus compliqué à faire fonctionner
- En utilisant quelques tricks comme : 
	- agrandir la bbox donnée par le bbox model 
	- classifieur fonctionnant sur plusieurs images et un vote
- Amélioration possible :
	- dataset plus varié (avec data prise par d'autres personnes)
	- entraînement sur un modèle plus gros et plus long à entrainer
	- entraînement sans transfer learning
	- transfer learning avec un model type YOLO qui fait de l'object detection


Partie 1:

In order to start our project with the appropriate tools, we made some research concerning several subjects. 

Before thinking about machine learning techniques, we looked for some informations about image processing. Indeed, building a whole deep learning pipeline is useless if classic techniques can solve the problems. We looked for image recognition or segmentation techniques such as k-means, OSU etc...
<screen segmentation notebook>
The problem with those techniques is the fact that these are dependant on some parameters and the generalization is not so good. Indeed the k value for the k-means algorithm won't work in some situations. We did some test in notebooks and decided to go for machine learning instead.

First things first, we need to understand the convolutional neural network architecture since we are doing computer vision tasks. We found what we needed in books such as "Quand la machine apprend" by Yann LeCun. We also need a framework to make our experience so we chosed PyTorch, we obvioulsy looked for its documentation to understand how we were going to proceed.

Now that we are aware of CNN and PyTorch, we will need to train our models for our different task. In order to make it work, we won't build and train models from scratch, it would take too much time and ressources. Thus, we decided to use transfer learning techniques so we wouldn't need to train the whole model, but only some part of it. The PyTorch documentation helped us a lot to understand how transfer learning works. Their model zoo is useful for a lot of task, including the ones we need.

Partie 2:

Before building a pipeline to experiment the models for our tasks, we had to know which type of task could work on the NVIDIA Jetson. Indeed, some task require more computational power to work, that's the case of segmentation which needs several masks as outputs. However, we downloaded some datasets such as CIPHAR10 and CALTECH101 to benchmark different models using transfer learning. We tested the inference time on our computer GPU and on the jetson with different model doing different tasks. We show our results in the next table :
<screen google sheet benchmark>
We can see that the inference time depends on the number of parameters of the model and on the problem it is solving. The segmentation task requires too much computes which leads to a low FPS inference. The robot-car doesn't move really fast but we need it to have a minimum number of frame per seconds in order to reduce the number of errors. For instance, in a classification problem, we will use several images to decided which class we will chose by doing a hard vote.

We previously used .pth and .h5 format to save and load our models in inference, but we found out that the .onnx format speeds up the computation by a factor two. The whole benchmark was done using .onnx format and after testing it on our personal GPU, we made some scripts to execute it on the jetson.

The inference time is an important factor but we also have a memory limit which is 4GB on the jetson against 6GB on our NVIDIA RTX 2060 GPU or 24GB on our NVIDIA RTX 3090. The memory is fullfilled proportionnaly to the number of parameters. This is why we made our choice according to both FPS and number of parameters.

Finaly, after doing those benchmarks, we found out that we weren't going to use segmentation for our task. We decided that the VGG16 model was the best compromise thus we were going to use this model for transfer learning afterwards.

Partie 3:

After deciding that we were going to use our hand gesture to control the robot-car. We split the problem in 3 parts :
- Building an one-stage model which would directly classify the hand gesture with the raw image as input.
<schema one-stage>
- Building a two-stage model which would first crop the raw image, and then classify the cropped image.
<schema two-stage>
We looked for some datasets on kaggle and other datasets repositories, but we couldn't find what we wanted. However, we decided to build our own datasets. 

Since we haven't choose which type of model we were going to use, we built 3 different datasets using scripts :
- The first dataset would be simple images taken by a webcam, with a label associated which would be "FINGER", "PALM", "FIST", "LEFT", "RIGHT". It contains 4903 images for the training set and 1994 for the validation set.
<screen dataset>
- The second one is the same as the first one, but this time the image only contains the hand, without much background. It contains 3909 images for the training set and 1504 for the validation set.
<screen dataset>
- The final one is the same as the first one, but the output is constitued of 4 scalar values which represent the bounding box for the hand. It contains 2144 images for the training set and 602 for the validation set.
<screen dataset>

The first decision was about the kind of gesture we were going to use. Indeed, the more different the gestures are, the easier it is to discriminate it thanks to a network. At first try, our "LEFT", "RIGHT" and "FINGER" labels were just about one finger pointing in a direction, which lead to an higher error rate while trying to classify those three. That's why we changed that and made the hand gestures a bit more complex.

Building a dataset is not that easy, it takes a lot of time. In other words, we will need to augment our data to grow bigger datasets before feeding it to our models. To make that work, we have to think about which type of augmentation will increase our model performance. On one hand, flipping our images can lead to missclassification between "LEFT" and "RIGHT" labels. On the other hand, rotating the images leads to better generalization. The data augmentation doesn't change the labels for the classification task, but it needs to change for the bounding box task. We used a library called albumentation in order to compute the scalar labels according to the transformation applied. We also use PyTorch Dataset and Dataloader objects which directly loads the images from a folder so we don't have memory issues.
<screen image augmented>

When training a deep learning model, we have to control overfitting, the most common technique to watch that is to use a validation set. In fact the validation set is also a way to know where the model has to stop training thanks to early stopping. We first split our dataset and it led to nice performance and we had not to much overfitting even without using regularization techniques. Nevertheless, when we used our models in inference, the performance were really poor and we didn't get why so. We then decided to build two independant datasets for each tasks. For instance, we would use the images taken by Enzo as a training set, and the images taken by Damien as a validation set. This led to better performance in inference and saved use a lot of time.
<screen enzo image> <screen damien image>

On top of that we made sure that there wasn't some hand gesture classified better than other by using the classification score function of scikit-learn library. We noticed that there wasn't any problems. Visualizing the outputs of our bounding box model also made us understand better how the model works. At first, the model wasn't working well in inference and we noticed that most of the missclassification were made when the hands were far away. This allowed us get better performance by adding only images taken from far away.
<screen classification score>

After processing the datas we started training models, we used VGG16 for transfer learning for our 3 different problems. The goal is to freeze the feature layers which learns extracting features, and only train the classifier part. We adapt the output size, for exemple 5 for classification and 4 for the bounding box task. We start with a simple model, without data augmentation and precise stuff. Then we use more and more complex architecture, adding neurons and layers until the model overfits. This simple thing allows us to see if there is no bug. Once it managed to overfit, we can start thinking about regularization. A simple dropout layer and data augmentation part led to a nice generalization. The final part is finding the optimal hyper-parameters such as the learning rate, we found that a learning rate of 1e-5 gives the best results. Those experiments are computationaly expensive so we used Google Colab and Kaggle notebooks to speed up the research of optimal hyper-parameters.

After few trials we noticed that we were going to have a problem when using softmax outputs for our classifier. As a matter of fact, the softmax activation function make the model think the image must contains one of the label. In inference, while using argmax function on the output, we get a label even if there is no hand on the image. To solve that issue, we tried few tricks such as using a threshold on the softmax values. The things is we choose the threshold arbitrarily and it doesn't work in some cases. We also tried to add a "NONE" class but we finaly decided to use the sigmoid function which was a better option to use a threshold without using an additional label. We benchmarked different models, with softmax and sigmoid activation function and noticed that the sigmoid was a better choice.

The one-stage classification model didn't managed to perform in inference so we decided to focus on the two-stage one. The classifier part was easier to train since the dataset contained only hand gesture images without much background. It quickly gave 95%+ accuracy on all the labels and it was working well in inference. The bounding box was harder to train, but after optimizing it we managed to get it work.

Conclusion :

We finaly built the script combining both our models for the two-stage classification. In order to make that work, we used some simple tricks such as increasing the size of the bounding box in case the model isn't precise. We also made the algorithm wait for some inference step to take a decision so that we reduce the errors.

We thought about some improvements and came out with few ideas. One thing that would help is acquiring more various data. We could also try to use a one-stage model such as YOLO which is an object detection model. Moreover, transfer learning is a nice way the transfer the knowledge acquired and we expect the learned features map to be usefull for our task. One way to get better performance would be to train a whole model such as VGG, but without freezing feature layers. We also didn't explore the optimizer modules and we decided to rely on Adam which is one great base but there might be some better optimizers for our tasks.