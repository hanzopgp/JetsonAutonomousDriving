{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importation","metadata":{"execution":{"iopub.status.busy":"2022-04-11T16:15:07.160836Z","iopub.execute_input":"2022-04-11T16:15:07.161381Z","iopub.status.idle":"2022-04-11T16:15:08.995876Z","shell.execute_reply.started":"2022-04-11T16:15:07.161292Z","shell.execute_reply":"2022-04-11T16:15:08.995057Z"}}},{"cell_type":"code","source":"import os\nimport copy\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom time import time\nfrom sklearn import preprocessing\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import f1_score \nfrom matplotlib import pyplot as plt\n\nimport torchvision\nfrom torchvision import models, transforms\nfrom torchvision.io import read_image\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.utils.data import Dataset","metadata":{"execution":{"iopub.status.busy":"2022-04-17T19:23:33.002241Z","iopub.execute_input":"2022-04-17T19:23:33.002496Z","iopub.status.idle":"2022-04-17T19:23:33.008899Z","shell.execute_reply.started":"2022-04-17T19:23:33.002468Z","shell.execute_reply":"2022-04-17T19:23:33.008074Z"},"trusted":true},"execution_count":219,"outputs":[]},{"cell_type":"markdown","source":"# Global variables ","metadata":{}},{"cell_type":"code","source":"INPUT_SIZE = 400\nBATCH_SIZE = 32\nN_CLASS = 4\n\nPATH_LABELS = \"../input/boudingboxonlyhanddataset/index_label_bbox.csv\"\nPATH_IMG = \"../input/boudingboxonlyhanddataset/output/output\"\n\nPATH_LABELS_VALID = \"../input/boudingboxonlyhanddataset/index_label_bbox_validation.csv\"\nPATH_IMG_VALID = \"../input/boudingboxonlyhanddataset/output_validation/output_validation\"","metadata":{"execution":{"iopub.status.busy":"2022-04-17T19:23:33.352113Z","iopub.execute_input":"2022-04-17T19:23:33.352624Z","iopub.status.idle":"2022-04-17T19:23:33.357142Z","shell.execute_reply.started":"2022-04-17T19:23:33.352583Z","shell.execute_reply":"2022-04-17T19:23:33.356285Z"},"trusted":true},"execution_count":220,"outputs":[]},{"cell_type":"markdown","source":"# Data functions","metadata":{"execution":{"iopub.status.busy":"2022-04-11T16:20:53.56609Z","iopub.execute_input":"2022-04-11T16:20:53.566375Z","iopub.status.idle":"2022-04-11T16:20:53.569977Z","shell.execute_reply.started":"2022-04-11T16:20:53.566327Z","shell.execute_reply":"2022-04-11T16:20:53.569332Z"}}},{"cell_type":"code","source":"class HandGestureDataset(Dataset):\n    def __init__(self, annotations_file, img_dir, transform=None):\n        self.img_labels = pd.read_csv(annotations_file)\n        self.img_dir = img_dir\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.img_labels)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, os.listdir(self.img_dir)[idx])\n        image = read_image(img_path)\n        line = self.img_labels[\"index\"] == str(\"output/\"+os.listdir(self.img_dir)[idx])\n        x, y, end_x, end_y = self.img_labels.loc[line][\"x\"].item(),\\\n                               self.img_labels.loc[line][\"y\"].item(),\\\n                               self.img_labels.loc[line][\"x_end\"].item(),\\\n                               self.img_labels.loc[line][\"y_end\"].item()\n        if self.transform:\n            image = self.transform(image)\n#         image = F.normalize(image, dim = 0)\n        return {'image':image, 'label':[x,y,end_x,end_y]}","metadata":{"execution":{"iopub.status.busy":"2022-04-17T19:23:33.710249Z","iopub.execute_input":"2022-04-17T19:23:33.710496Z","iopub.status.idle":"2022-04-17T19:23:33.720721Z","shell.execute_reply.started":"2022-04-17T19:23:33.710468Z","shell.execute_reply":"2022-04-17T19:23:33.719907Z"},"trusted":true},"execution_count":221,"outputs":[]},{"cell_type":"code","source":"def prepare_data_vgg(data_type):\n    ## Parameters fitting vgg/imagenet\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n\n    ## pytorch transformer objects\n    transformVGGTrain=transforms.Compose([\n            transforms.ToPILImage(),\n        \n            ## can try\n#             transforms.ColorJitter(brightness=0.3, hue=0.3),\n#             transforms.RandomSolarize(threshold=192.0),\n#             transforms.RandomAdjustSharpness(sharpness_factor=3),\n#             transforms.RandomAutocontrast(),\n#             transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.01, 1.5)),\n        \n            ## don't use or it will ruin bounding box labels\n#             transforms.RandomRotation(degrees=(-15, 15)),\n#             transforms.RandomPerspective(distortion_scale=0.4, p=0.2),\n#             transforms.RandomAffine(degrees=(0, 5), translate=(0, 0.18), scale=(0.7, 1)),\n        \n            ## too much\n#             transforms.RandomPosterize(bits=2),\n#             transforms.RandomInvert(),\n#             transforms.RandomEqualize(),\n        \n            transforms.Resize(size=(INPUT_SIZE, INPUT_SIZE)),\n            transforms.ToTensor(),\n#             transforms.Normalize(mean, std) ## test with and without\n        ])\n    transformVGGValid=transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize(size=(INPUT_SIZE, INPUT_SIZE)),\n            transforms.ToTensor(),\n#             transforms.Normalize(mean, std) ## test with and without\n        ])\n\n    if data_type == \"custom\":\n        ## Custom dataset\n        VGG_dataset_train = HandGestureDataset(PATH_LABELS, PATH_IMG, transformVGGTrain)\n        VGG_dataset_valid = HandGestureDataset(PATH_LABELS_VALID, PATH_IMG_VALID, transformVGGValid)\n        VGG_trainloader = torch.utils.data.DataLoader(VGG_dataset_train, batch_size=BATCH_SIZE, pin_memory=True, shuffle=True)\n        VGG_validloader = torch.utils.data.DataLoader(VGG_dataset_valid, batch_size=BATCH_SIZE, pin_memory=True, shuffle=True)\n    \n    return VGG_trainloader, VGG_validloader","metadata":{"execution":{"iopub.status.busy":"2022-04-17T19:23:34.084873Z","iopub.execute_input":"2022-04-17T19:23:34.085577Z","iopub.status.idle":"2022-04-17T19:23:34.093718Z","shell.execute_reply.started":"2022-04-17T19:23:34.085539Z","shell.execute_reply":"2022-04-17T19:23:34.093034Z"},"trusted":true},"execution_count":222,"outputs":[]},{"cell_type":"markdown","source":"# Loading data into pytorch dataset and dataloader objects","metadata":{"execution":{"iopub.status.busy":"2022-02-15T15:49:41.69497Z","iopub.execute_input":"2022-02-15T15:49:41.695334Z","iopub.status.idle":"2022-02-15T15:49:46.639507Z","shell.execute_reply.started":"2022-02-15T15:49:41.695293Z","shell.execute_reply":"2022-02-15T15:49:46.638679Z"}}},{"cell_type":"code","source":"VGG_trainloader, VGG_validloader = prepare_data_vgg(\"custom\")","metadata":{"execution":{"iopub.status.busy":"2022-04-17T19:23:34.646388Z","iopub.execute_input":"2022-04-17T19:23:34.646936Z","iopub.status.idle":"2022-04-17T19:23:34.656955Z","shell.execute_reply.started":"2022-04-17T19:23:34.646897Z","shell.execute_reply":"2022-04-17T19:23:34.656221Z"},"trusted":true},"execution_count":223,"outputs":[]},{"cell_type":"code","source":"# for img in next(iter(VGG_trainloader)):\n#     for i in img[:20]:\n#         i = i.permute(1,2,0)\n#         plt.imshow(np.array(i))\n#         plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T19:23:35.206109Z","iopub.execute_input":"2022-04-17T19:23:35.206641Z","iopub.status.idle":"2022-04-17T19:23:35.209772Z","shell.execute_reply.started":"2022-04-17T19:23:35.206603Z","shell.execute_reply":"2022-04-17T19:23:35.209048Z"},"trusted":true},"execution_count":224,"outputs":[]},{"cell_type":"markdown","source":"# Model functions","metadata":{}},{"cell_type":"code","source":"def train(model, epochs, train_loader, valid_loader, learning_rate, patience, feature_extract=False):\n    ## Early stopping variables\n    es = EarlyStopping(patience=patience)\n    terminate_training = False\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_loss = 0.0\n    model = model.to(device)\n    ## Training only the parameters where we require gradient since we are fine-tuning\n    params_to_update = model.parameters()\n    print(\"params to learn:\")\n    if feature_extract:\n        params_to_update = []\n        for name,param in model.named_parameters():\n            if param.requires_grad == True:\n                params_to_update.append(param)\n                print(\"\\t\", name)\n    else:\n        for name,param in model.named_parameters():\n            if param.requires_grad == True:\n                print(\"\\t\", name)\n                \n    ## Setting up our optimizer\n    optim = torch.optim.Adam(params_to_update, lr=learning_rate)\n    \n    ## Setting up our loss function\n    loss = nn.MSELoss()\n    \n    ## Running the train loop\n    print(f\"running {model.name}\")\n    for epoch in range(epochs):\n        cumloss, count = 0, 0\n        model.train()\n        for item in train_loader:\n            optim.zero_grad()\n            x, y = item[\"image\"], item[\"label\"]\n            x = item[\"image\"].to(device)\n            \n            res = []\n            for e in y:\n                res.append(np.array(e))\n            res = np.array(res).T\n            y = torch.as_tensor(res)\n            y = y.to(torch.float32)\n            y = y.to(device)\n            \n            yhat = model(x)\n\n            l = loss(yhat, y)\n            l.backward()\n            optim.step()\n            cumloss += l * len(x)\n            count += len(x)\n        print(\"epoch :\", epoch, end=\"\")\n        print(\", train_loss: \", cumloss.cpu().item()/count, end=\"\")\n        if epoch % 1 == 0:\n            model.eval()\n            with torch.no_grad():\n                valid_cumloss, count = 0, 0\n                for x,y in valid_loader:\n                    x, y = item[\"image\"], item[\"label\"]\n                    x = item[\"image\"].to(device)\n                    \n                    \n                    res = []\n                    for e in y:\n                        res.append(np.array(e))\n                    res = np.array(res).T\n                    y = torch.as_tensor(res)\n                    y = y.to(torch.float32)\n                    y = y.to(device)\n                    \n                    \n                    yhat = model(x)\n                    valid_cumloss += loss(yhat,y) * len(x)\n                    count += len(x)\n                print(\", valid_loss: \", valid_cumloss.cpu().item()/count)\n                ## Early stopping\n                if valid_cumloss/count > best_loss:\n                    best_loss = valid_cumloss/count\n                    best_model_wts = copy.deepcopy(model.state_dict())\n                if es.step(valid_cumloss.cpu().item()/count):\n                    terminate_training = True\n                    break\n        if terminate_training:\n            break\n    print('Best val loss: {:4f}'.format(best_loss))\n    ## Returns the best model\n    model.load_state_dict(best_model_wts)\n    return model\n\ndef set_parameter_requires_grad(model, feature_extract):\n    if feature_extract:\n        for name,p in model.named_parameters():\n            if \"features\" in name:\n                p.requires_grad = False    \n            else:\n                p.requires_grad = True  ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-17T19:23:35.764975Z","iopub.execute_input":"2022-04-17T19:23:35.765421Z","iopub.status.idle":"2022-04-17T19:23:35.785945Z","shell.execute_reply.started":"2022-04-17T19:23:35.765384Z","shell.execute_reply":"2022-04-17T19:23:35.784973Z"},"trusted":true},"execution_count":225,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the model and modifying the classifier part\n### Maybe we could try to modify only the last classifier layer ?","metadata":{"execution":{"iopub.status.busy":"2022-04-11T16:22:20.614632Z","iopub.execute_input":"2022-04-11T16:22:20.614896Z","iopub.status.idle":"2022-04-11T16:22:20.618197Z","shell.execute_reply.started":"2022-04-11T16:22:20.614868Z","shell.execute_reply":"2022-04-11T16:22:20.617386Z"}}},{"cell_type":"code","source":"TB_PATH = \"/tmp/logs/sceance2\"\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n## Loading vgg16 model pretrained on imagenet\nvgg = models.vgg16(pretrained=True)\n\nvgg.classifier = nn.Sequential(nn.Linear(25088, 128), \n                               nn.ReLU(), \n#                                nn.Dropout(0.5),        \n                               nn.Linear(128, 64), \n                               nn.ReLU(), \n#                                nn.Dropout(0.5),        \n                               nn.Linear(64, 32),\n                               nn.ReLU(), \n#                                nn.Dropout(0.5),        \n                               nn.Linear(32, N_CLASS))\n#                                nn.Sigmoid())\n\nprint(vgg.eval())\n\n## Sets all the requires grad of the classifier layers to True\nset_parameter_requires_grad(vgg, True)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T19:23:36.337014Z","iopub.execute_input":"2022-04-17T19:23:36.337552Z","iopub.status.idle":"2022-04-17T19:23:37.838857Z","shell.execute_reply.started":"2022-04-17T19:23:36.337509Z","shell.execute_reply":"2022-04-17T19:23:37.838035Z"},"trusted":true},"execution_count":226,"outputs":[]},{"cell_type":"markdown","source":"# Implementing early stopping","metadata":{}},{"cell_type":"code","source":"class EarlyStopping(object):\n    def __init__(self, mode='min', min_delta=0, patience=10, percentage=False):\n        self.mode = mode\n        self.min_delta = min_delta\n        self.patience = patience\n        self.best = None\n        self.num_bad_epochs = 0\n        self.is_better = None\n        self._init_is_better(mode, min_delta, percentage)\n        if patience == 0:\n            self.is_better = lambda a, b: True\n            self.step = lambda a: False\n\n    def step(self, metrics):\n        if self.best is None:\n            self.best = metrics\n            return False\n        if np.isnan(metrics):\n            return True\n        if self.is_better(metrics, self.best):\n            self.num_bad_epochs = 0\n            self.best = metrics\n        else:\n            self.num_bad_epochs += 1\n        if self.num_bad_epochs >= self.patience:\n            return True\n        return False\n\n    def _init_is_better(self, mode, min_delta, percentage):\n        if mode not in {'min', 'max'}:\n            raise ValueError('mode ' + mode + ' is unknown!')\n        if not percentage:\n            if mode == 'min':\n                self.is_better = lambda a, best: a < best - min_delta\n            if mode == 'max':\n                self.is_better = lambda a, best: a > best + min_delta\n        else:\n            if mode == 'min':\n                self.is_better = lambda a, best: a < best - (\n                            best * min_delta / 100)\n            if mode == 'max':\n                self.is_better = lambda a, best: a > best + (\n                            best * min_delta / 100)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T19:23:37.840446Z","iopub.execute_input":"2022-04-17T19:23:37.841254Z","iopub.status.idle":"2022-04-17T19:23:37.865782Z","shell.execute_reply.started":"2022-04-17T19:23:37.841212Z","shell.execute_reply":"2022-04-17T19:23:37.865048Z"},"trusted":true},"execution_count":227,"outputs":[]},{"cell_type":"markdown","source":"# Training only the modified parts of the classifier","metadata":{}},{"cell_type":"code","source":"## Fine-tuning the model on our data\nvgg.name = \"VGG\"\n\nbest_model = train(model=vgg, \n                   epochs=500, \n                   train_loader=VGG_trainloader, \n                   valid_loader=VGG_validloader, \n                   learning_rate=3e-4, ## learning rate for Adam optimizer\n                   patience=5) ## metric for earlystopping : val_loss ","metadata":{"execution":{"iopub.status.busy":"2022-04-17T19:23:37.866856Z","iopub.execute_input":"2022-04-17T19:23:37.867361Z","iopub.status.idle":"2022-04-17T19:25:25.640644Z","shell.execute_reply.started":"2022-04-17T19:23:37.867326Z","shell.execute_reply":"2022-04-17T19:25:25.639929Z"},"trusted":true},"execution_count":228,"outputs":[]},{"cell_type":"markdown","source":"# Checking predictions","metadata":{}},{"cell_type":"code","source":"def draw_predictions(image, preds):\n    startX, startY, endX, endY = preds\n    h, w = image.shape[:2]\n    # scale the predicted bounding box coordinates based on the image\n    # dimensions\n    startX = int(startX * w)\n    startY = int(startY * h)\n    endX = int(endX * w)\n    endY = int(endY * h)\n    # draw the predicted bounding box on the image\n    cv2.rectangle(image, (startX, startY), (endX, endY), (0, 255, 0), 2)\n    # show the output image\n    cv2.imshow(\"Output\", image)\n    cv2.waitKey(0)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T19:25:25.642770Z","iopub.execute_input":"2022-04-17T19:25:25.643030Z","iopub.status.idle":"2022-04-17T19:25:25.648738Z","shell.execute_reply.started":"2022-04-17T19:25:25.642977Z","shell.execute_reply":"2022-04-17T19:25:25.647744Z"},"trusted":true},"execution_count":229,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    for x,y in VGG_validloader:\n        x = x.to(device)\n        y = le.fit_transform(y)\n        y = torch.as_tensor(y)\n        y = y.to(device)\n        yhat = best_model(x)\n        for i in range(50):\n            draw_predictions(x.cpu()[i].permute(1,2,0), yhat.cpu()[i])","metadata":{"execution":{"iopub.status.busy":"2022-04-17T19:25:25.650022Z","iopub.execute_input":"2022-04-17T19:25:25.650460Z","iopub.status.idle":"2022-04-17T19:25:25.937435Z","shell.execute_reply.started":"2022-04-17T19:25:25.650424Z","shell.execute_reply":"2022-04-17T19:25:25.936556Z"},"trusted":true},"execution_count":230,"outputs":[]},{"cell_type":"markdown","source":"# Saving the model in .pth and .onnx extension","metadata":{}},{"cell_type":"code","source":"PATH = \"./\"\ntorch.save(best_model.state_dict(), os.path.join(PATH,\"boundingbox_vgg.pth\"))","metadata":{"execution":{"iopub.status.busy":"2022-04-17T19:25:25.938581Z","iopub.status.idle":"2022-04-17T19:25:25.939188Z","shell.execute_reply.started":"2022-04-17T19:25:25.938908Z","shell.execute_reply":"2022-04-17T19:25:25.938936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del vgg\ndel best_model","metadata":{"execution":{"iopub.status.busy":"2022-04-17T19:25:25.940345Z","iopub.status.idle":"2022-04-17T19:25:25.940895Z","shell.execute_reply.started":"2022-04-17T19:25:25.940660Z","shell.execute_reply":"2022-04-17T19:25:25.940685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = models.vgg16(pretrained=True)\n# model.classifier[0] = nn.Linear(25088, 8192)\n# model.classifier[3] = nn.Linear(8192, 1024)\n# model.classifier[6] = nn.Linear(1024, N_CLASS)\n# model.load_state_dict(torch.load(os.path.join(PATH,\"vgg.pth\"), map_location='cpu'))\n# model.eval() \n\n# dummy_input = torch.randn(BATCH_SIZE, 3, INPUT_SIZE, INPUT_SIZE)  \n# torch.onnx.export(model,   \n#                   dummy_input, \n#                   \"vgg.onnx\",\n#                   export_params=True,\n#                   do_constant_folding=True, \n#                   input_names = ['modelInput'],\n#                   output_names = ['modelOutput'])","metadata":{"execution":{"iopub.status.busy":"2022-04-17T19:25:25.941977Z","iopub.status.idle":"2022-04-17T19:25:25.942523Z","shell.execute_reply.started":"2022-04-17T19:25:25.942293Z","shell.execute_reply":"2022-04-17T19:25:25.942319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}