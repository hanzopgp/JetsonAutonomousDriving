{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nfrom tqdm import tqdm\nfrom time import time\n\nimport torchvision\nfrom torchvision import models, transforms\n\nimport torch\nfrom torch import nn\nfrom torch.utils.tensorboard import SummaryWriter","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:02:41.445258Z","iopub.execute_input":"2022-02-08T16:02:41.445544Z","iopub.status.idle":"2022-02-08T16:02:41.451597Z","shell.execute_reply.started":"2022-02-08T16:02:41.445513Z","shell.execute_reply":"2022-02-08T16:02:41.450183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def accuracy(yhat,y):\n    # si y encode les indexes\n    if len(y.shape)==1 or y.size(1)==1:\n        return (torch.argmax(yhat,1).view(y.size(0),-1)== y.view(-1,1)).double().mean()\n    # si y est encodé en onehot\n    return (torch.argmax(yhat,1).view(-1) == torch.argmax(y,1).view(-1)).double().mean()\n\ndef train(model,epochs,train_loader,test_loader,feature_extract=False):\n    model = model.to(device)\n    writer = SummaryWriter(f\"{TB_PATH}/{model.name}\")\n    \n    params_to_update = model.parameters()\n    print(\"params to learn:\")\n    if feature_extract:\n        params_to_update = []\n        for name,param in model.named_parameters():\n            if param.requires_grad == True:\n                params_to_update.append(param)\n                print(\"\\t\",name)\n    else:\n        for name,param in model.named_parameters():\n            if param.requires_grad == True:\n                print(\"\\t\",name)\n    optim = torch.optim.Adam(params_to_update,lr=1e-3)\n    \n    print(f\"running {model.name}\")\n    loss = nn.CrossEntropyLoss()\n    for epoch in tqdm(range(epochs)):\n        cumloss, cumacc, count = 0, 0, 0\n        model.train()\n        for x,y in train_loader:\n            optim.zero_grad()\n            x,y = x.to(device), y.to(device)\n            yhat = model(x)\n            l = loss(yhat,y)\n            l.backward()\n            optim.step()\n            cumloss += l*len(x)\n            cumacc += accuracy(yhat,y)*len(x)\n            count += len(x)\n        writer.add_scalar('loss/train',cumloss/count,epoch)\n        writer.add_scalar('accuracy/train',cumacc/count,epoch)\n        if epoch % 1 == 0:\n            model.eval()\n            with torch.no_grad():\n                cumloss, cumacc, count = 0, 0, 0\n                for x,y in test_loader:\n                    x,y = x.to(device), y.to(device)\n                    yhat = model(x)\n                    cumloss += loss(yhat,y)*len(x)\n                    cumacc += accuracy(yhat,y)*len(x)\n                    count += len(x)\n                writer.add_scalar(f'loss/test',cumloss/count,epoch)\n                writer.add_scalar('accuracy/test',cumacc/count,epoch)\n\ndef set_parameter_requires_grad(model, feature_extract):\n    if feature_extract:\n        for name,p in model.named_parameters():\n            p.requires_grad = False    \n                \ndef get_test_data(dataloader, size):\n    X_test, Y_test = next(iter(dataloader))\n    batch_size = len(X_test)\n    n = size//batch_size\n    for i, batch in enumerate(dataloader):\n        if i < n:\n            X_tmp, Y_tmp = batch\n            X_test = torch.cat((X_test, X_tmp), 0)\n            Y_test = torch.cat((Y_test, Y_tmp), 0)\n    return X_test, Y_test","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:02:41.454279Z","iopub.execute_input":"2022-02-08T16:02:41.454738Z","iopub.status.idle":"2022-02-08T16:02:41.483753Z","shell.execute_reply.started":"2022-02-08T16:02:41.454651Z","shell.execute_reply":"2022-02-08T16:02:41.482714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TB_PATH = \"/tmp/logs/sceance2\"\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nFCNResNet = models.segmentation.fcn_resnet50(pretrained=True)\n\n# FCNResNet.backbone[\"conv1\"] = nn.Linear(2048, 1024)\nprint(FCNResNet.eval())\n\nset_parameter_requires_grad(FCNResNet, True)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:02:41.486276Z","iopub.execute_input":"2022-02-08T16:02:41.486482Z","iopub.status.idle":"2022-02-08T16:02:42.176555Z","shell.execute_reply.started":"2022-02-08T16:02:41.486455Z","shell.execute_reply":"2022-02-08T16:02:42.175509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_size = 224\nbatch_size = 128\n\nmean=[0.485, 0.456, 0.406]\nstd=[0.229, 0.224, 0.225]\n\ntransformFCNResNetTrain=transforms.Compose([ # Cette fois on utilise pas de grayscale car nous avons un gros modele pré-entrainé\n        transforms.RandomResizedCrop(input_size), # selection aléatoire d'une zone de la taille voulue (augmentation des données en apprentissage)\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std)\n    ])\ntransformFCNResNetTest=transforms.Compose([\n        transforms.Resize(input_size), # selection de la zone centrale de la taille voulue\n        transforms.CenterCrop(input_size),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std)\n    ])\n\nFCNResNet_trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transformFCNResNetTrain)\nFCNResNet_trainloader = torch.utils.data.DataLoader(FCNResNet_trainset, batch_size=batch_size, pin_memory=True, shuffle=True)\n\nFCNResNet_testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transformFCNResNetTest)\nFCNResNet_testloader = torch.utils.data.DataLoader(FCNResNet_testset, batch_size=batch_size, pin_memory=True, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:02:42.17919Z","iopub.execute_input":"2022-02-08T16:02:42.179716Z","iopub.status.idle":"2022-02-08T16:02:43.912232Z","shell.execute_reply.started":"2022-02-08T16:02:42.179671Z","shell.execute_reply":"2022-02-08T16:02:43.911187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Entraînement du réseau\n# FCNResNet.name = \"FCNResNet\"\n# train(FCNResNet, 1, FCNResNet_trainloader, FCNResNet_testloader)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:02:43.913891Z","iopub.execute_input":"2022-02-08T16:02:43.914292Z","iopub.status.idle":"2022-02-08T16:02:43.922004Z","shell.execute_reply.started":"2022-02-08T16:02:43.914241Z","shell.execute_reply":"2022-02-08T16:02:43.920812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Accuracy\nX_test, Y_test = get_test_data(FCNResNet_testloader, 1000) \nX_test, Y_test = X_test.to(device), Y_test.to(device)\n# print(\"Acc for FCNResNet transfer learning :\", accuracy(FCNResNet(X_test), Y_test))","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:02:43.923718Z","iopub.execute_input":"2022-02-08T16:02:43.924106Z","iopub.status.idle":"2022-02-08T16:03:06.096068Z","shell.execute_reply.started":"2022-02-08T16:02:43.924057Z","shell.execute_reply":"2022-02-08T16:03:06.095086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\n\nlabel_map = [\n               (0, 0, 0),  # background\n               (128, 0, 0), # aeroplane\n               (0, 128, 0), # bicycle\n               (128, 128, 0), # bird\n               (0, 0, 128), # boat\n               (128, 0, 128), # bottle\n               (0, 128, 128), # bus \n               (128, 128, 128), # car\n               (64, 0, 0), # cat\n               (192, 0, 0), # chair\n               (64, 128, 0), # cow\n               (192, 128, 0), # dining table\n               (64, 0, 128), # dog\n               (192, 0, 128), # horse\n               (64, 128, 128), # motorbike\n               (192, 128, 128), # person\n               (0, 64, 0), # potted plant\n               (128, 64, 0), # sheep\n               (0, 192, 0), # sofa\n               (128, 192, 0), # train\n               (0, 64, 128) # tv/monitor\n]\n\ndef image_overlay(image, segmented_image):\n    alpha = 1 # transparency for the original image \n    beta = 0.8 # transparency for the segmentation map\n    gamma = 0 # scalar added to each sum\n    print(image.shape)\n    image = np.transpose(image, (1, 2, 0))\n    print(image.shape)\n    segmented_image = cv2.cvtColor(segmented_image, cv2.COLOR_RGB2BGR)\n    image = np.array(image)\n    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n#     cv2.addWeighted(image, alpha, segmented_image, beta, gamma, image)\n    return image\n\ndef draw_segmentation_map(outputs):\n    labels = torch.argmax(outputs.squeeze(), dim=0).detach().cpu().numpy()\n    # create Numpy arrays containing zeros\n    # later to be used to fill them with respective red, green, and blue pixels\n    red_map = np.zeros_like(labels).astype(np.uint8)\n    green_map = np.zeros_like(labels).astype(np.uint8)\n    blue_map = np.zeros_like(labels).astype(np.uint8)\n    \n    for label_num in range(0, len(label_map)):\n        index = labels == label_num\n        red_map[index] = np.array(label_map)[label_num, 0]\n        green_map[index] = np.array(label_map)[label_num, 1]\n        blue_map[index] = np.array(label_map)[label_num, 2]\n        \n    segmentation_map = np.stack([red_map, green_map, blue_map], axis=2)\n    return segmentation_map\n\ndef get_segment_labels(image, model, device):\n    # transform the image to tensor and load into computation device\n    image = np.transpose(image, (1, 2, 0))\n    image = Image.fromarray(np.uint8((image)*255))\n    transform = transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])])\n    image = transform(image).to(device)\n#     image = torch.permute(image, (2, 0, 1))\n#     print(image.shape)\n    image = image.unsqueeze(0) # add a batch dimension\n    outputs = model(image)\n    return outputs","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:12:23.917363Z","iopub.execute_input":"2022-02-08T16:12:23.91773Z","iopub.status.idle":"2022-02-08T16:12:23.938652Z","shell.execute_reply.started":"2022-02-08T16:12:23.917683Z","shell.execute_reply":"2022-02-08T16:12:23.937405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nfrom matplotlib import pyplot as plt\nfrom torch import Tensor\n\nX_test, Y_test = X_test.to(device), Y_test.to(device)\nFCNResNet = FCNResNet.to(device)\n\nX_test_np = np.array(X_test.cpu())\noutputs = get_segment_labels(X_test_np[0], FCNResNet, device)\nseg = draw_segmentation_map(outputs[\"aux\"])\n\n\n# X_test_np[0] = np.transpose(X_test_np[0], (1, 2, 0))\n# image = Image.fromarray(np.uint8((X_test_np[0])*255))\n\nimg = image_overlay(X_test_np[0], seg)\nimg = np.transpose(img, (0,1,2))\nprint(img.shape)\nplt.show(img)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:14:07.457725Z","iopub.execute_input":"2022-02-08T16:14:07.458242Z","iopub.status.idle":"2022-02-08T16:14:08.236746Z","shell.execute_reply.started":"2022-02-08T16:14:07.458208Z","shell.execute_reply":"2022-02-08T16:14:08.235206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_test.shape)\nfor t in (20,40,60,80,100,120):\n    t0 = time()\n    FCNResNet(X_test[:t])\n    print(\"FPS:\", t, \" --> seconds:\", (time() - t0))","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:10:48.951797Z","iopub.status.idle":"2022-02-08T16:10:48.953134Z","shell.execute_reply.started":"2022-02-08T16:10:48.952758Z","shell.execute_reply":"2022-02-08T16:10:48.952791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nPATH = \"./\"\ntorch.save(FCNResNet.state_dict(), os.path.join(PATH,\"fcnresnet.pth\"))","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:03:06.936271Z","iopub.status.idle":"2022-02-08T16:03:06.93735Z","shell.execute_reply.started":"2022-02-08T16:03:06.937028Z","shell.execute_reply":"2022-02-08T16:03:06.93706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}