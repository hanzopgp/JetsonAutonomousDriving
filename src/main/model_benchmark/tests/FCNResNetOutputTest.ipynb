{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a920ea8-b3d0-45db-9f9a-2649ce0083e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models, transforms\n",
    "import torchvision\n",
    "import os\n",
    "from time import time\n",
    "import onnx\n",
    "import onnxruntime\n",
    "from onnx import numpy_helper\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "733f0995-9c2f-417b-8338-ef6201631d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"models/\"\n",
    "input_size = 224\n",
    "batch_size = 128\n",
    "n_channel = 3\n",
    "test_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0b55f75-63ca-4071-a704-992c58d24dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_colors_list = [\n",
    "        (64, 128, 64), # animal\n",
    "        (192, 0, 128), # archway\n",
    "        (0, 128, 192), # bicyclist\n",
    "        (0, 128, 64), #bridge\n",
    "        (128, 0, 0), # building\n",
    "        (64, 0, 128), #car\n",
    "        (64, 0, 192), # car luggage pram...???...\n",
    "        (192, 128, 64), # child\n",
    "        (192, 192, 128), # column pole\n",
    "        (64, 64, 128), # fence\n",
    "        (128, 0, 192), # lane marking driving\n",
    "        (192, 0, 64), # lane maring non driving\n",
    "        (128, 128, 64), # misc text\n",
    "        (192, 0, 192), # motor cycle scooter\n",
    "        (128, 64, 64), # other moving\n",
    "        (64, 192, 128), # parking block\n",
    "        (64, 64, 0), # pedestrian\n",
    "        (128, 64, 128), # road\n",
    "        (128, 128, 192), # road shoulder\n",
    "        (0, 0, 192), # sidewalk\n",
    "        (192, 128, 128), # sign symbol\n",
    "        (128, 128, 128), # sky\n",
    "        (64, 128, 192), # suv pickup truck\n",
    "        (0, 0, 64), # traffic cone\n",
    "        (0, 64, 64), # traffic light\n",
    "        (192, 64, 128), # train\n",
    "        (128, 128, 0), # tree\n",
    "        (192, 128, 192), # truck/bus\n",
    "        (64, 0, 64), # tunnel\n",
    "        (192, 192, 0), # vegetation misc.\n",
    "        (0, 0, 0),  # 0=background/void\n",
    "        (64, 192, 0), # wall\n",
    "    ]\n",
    "\n",
    "CLASSES_TO_TRAIN = [\n",
    "        'animal', 'archway', 'bicyclist', 'bridge', 'building', 'car', \n",
    "        'cartluggagepram', 'child', 'columnpole', 'fence', 'lanemarkingdrve', \n",
    "        'lanemarkingnondrve', 'misctext', 'motorcyclescooter', 'othermoving',\n",
    "        'parkingblock', 'pedestrian', 'road', 'road shoulder', 'sidewalk',\n",
    "        'signsymbol', 'sky', 'suvpickuptruck', 'trafficcone', 'trafficlight', \n",
    "        'train', 'tree', 'truckbase', 'tunnel', 'vegetationmisc', 'void',\n",
    "        'wall'\n",
    "        ]\n",
    "\n",
    "ALL_CLASSES = ['animal', 'archway', 'bicyclist', 'bridge', 'building', 'car', \n",
    "        'cartluggagepram', 'child', 'columnpole', 'fence', 'lanemarkingdrve', \n",
    "        'lanemarkingnondrve', 'misctext', 'motorcyclescooter', 'othermoving', \n",
    "        'parkingblock', 'pedestrian', 'road', 'road shoulder', 'sidewalk', \n",
    "        'signsymbol', 'sky', 'suvpickuptruck', 'trafficcone', 'trafficlight', \n",
    "        'train', 'tree', 'truckbase', 'tunnel', 'vegetationmisc', 'void',\n",
    "        'wall']\n",
    "\n",
    "class_values = [ALL_CLASSES.index(cls.lower()) for cls in CLASSES_TO_TRAIN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e4cbca6-c6d3-4422-8c33-6d027afa89e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(dataloader, size):\n",
    "    X_test, Y_test = next(iter(dataloader))\n",
    "    batch_size = len(X_test)\n",
    "    n = size//batch_size\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        if i < n:\n",
    "            X_tmp, Y_tmp = batch\n",
    "            X_test = torch.cat((X_test, X_tmp), 0)\n",
    "            Y_test = torch.cat((Y_test, Y_tmp), 0)\n",
    "    return X_test, Y_test\n",
    "\n",
    "def draw_test_segmentation_map(outputs):\n",
    "    \"\"\"\n",
    "    This function will apply color mask as per the output that we\n",
    "    get when executing `test.py` or `test_vid.py` on a single image \n",
    "    or a video. NOT TO BE USED WHILE TRAINING OR VALIDATING.\n",
    "    \"\"\"\n",
    "    labels = torch.argmax(outputs.squeeze(), dim=0).detach().cpu().numpy()\n",
    "    red_map = np.zeros_like(labels).astype(np.uint8)\n",
    "    green_map = np.zeros_like(labels).astype(np.uint8)\n",
    "    blue_map = np.zeros_like(labels).astype(np.uint8)\n",
    "    \n",
    "    for label_num in range(0, len(label_colors_list)):\n",
    "        if label_num in class_values:\n",
    "            idx = labels == label_num\n",
    "            red_map[idx] = np.array(label_colors_list)[label_num, 0]\n",
    "            green_map[idx] = np.array(label_colors_list)[label_num, 1]\n",
    "            blue_map[idx] = np.array(label_colors_list)[label_num, 2]\n",
    "        \n",
    "    segmented_image = np.stack([red_map, green_map, blue_map], axis=2)\n",
    "    return segmented_image\n",
    "\n",
    "def image_overlay(image, segmented_image):\n",
    "    \"\"\"\n",
    "    This function will apply an overlay of the output segmentation\n",
    "    map on top of the orifinal input image. MAINLY TO BE USED WHEN\n",
    "    EXECUTING `test.py` or `test_vid.py`.\n",
    "    \"\"\"\n",
    "    alpha = 0.6 # how much transparency to apply\n",
    "    beta = 1 - alpha # alpha + beta should equal 1\n",
    "    gamma = 0 # scalar added to each sum\n",
    "    image = np.array(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    segmented_image = cv2.cvtColor(segmented_image, cv2.COLOR_RGB2BGR)\n",
    "    cv2.addWeighted(segmented_image, alpha, image, beta, gamma, image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "876175ed-8cea-4657-9b29-b8937f95f586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "mean=[0.485, 0.456, 0.406]\n",
    "std=[0.229, 0.224, 0.225]\n",
    "transformfcnTrain=transforms.Compose([ # Cette fois on utilise pas de grayscale car nous avons un gros modele pré-entrainé\n",
    "        transforms.RandomResizedCrop(input_size), # selection aléatoire d'une zone de la taille voulue (augmentation des données en apprentissage)\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "transformfcnTest=transforms.Compose([\n",
    "        transforms.Resize(input_size), # selection de la zone centrale de la taille voulue\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "fcn_trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transformfcnTrain)\n",
    "fcn_trainloader = torch.utils.data.DataLoader(fcn_trainset, batch_size=batch_size, pin_memory=True, shuffle=True)\n",
    "fcn_testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transformfcnTest)\n",
    "fcn_testloader = torch.utils.data.DataLoader(fcn_testset, batch_size=batch_size, pin_memory=True, shuffle=True)\n",
    "X_test, Y_test = get_test_data(fcn_testloader, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "404a2dd0-a9e5-4689-8cf1-3c780edc14d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karna\\anaconda3\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:381: UserWarning: You are trying to export the model with onnx:Upsample for ONNX opset version 9. This operator might cause results to not match the expected results by PyTorch.\n",
      "ONNX's Upsample/Resize operator did not match Pytorch's Interpolation until opset 11. Attributes to determine how to transform the input were added in onnx:Resize in opset 11 to support Pytorch's behavior (like coordinate_transformation_mode and nearest_mode).\n",
      "We recommend using opset 11 and above for models using this operator.\n",
      "  warnings.warn(\"You are trying to export the model with \" + onnx_op + \" for ONNX opset version \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for  5  images with ONNX inference 0.6331841945648193\n",
      "(5, 21, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "model = models.segmentation.fcn_resnet50(pretrained=True)\n",
    "model.load_state_dict(torch.load(os.path.join(PATH,\"fcnresnet.pth\"), map_location='cpu'))\n",
    "model.eval()\n",
    "dummy_input = torch.randn(test_size, n_channel, input_size, input_size)  \n",
    "torch.onnx.export(model,   \n",
    "                  dummy_input, \n",
    "                  str(PATH+\"fcnresnet.onnx\"),\n",
    "                  export_params=True,\n",
    "                  do_constant_folding=True, \n",
    "                  input_names = ['modelInput'],\n",
    "                  output_names = ['modelOutput'])\n",
    "X_test = X_test[:test_size]\n",
    "sess = onnxruntime.InferenceSession(str(PATH+\"fcnresnet.onnx\"))\n",
    "input_name = sess.get_inputs()[0].name\n",
    "output_name = sess.get_outputs()[0].name\n",
    "\n",
    "t0 = time()\n",
    "pred = sess.run([output_name], {input_name: np.array(X_test).astype(np.float32)})[0]\n",
    "print(\"Time for \",test_size,\" images with ONNX inference\", (time() - t0))\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d304c78-5fa7-42e0-9f98-d6538fc4f408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 3, 224)\n",
      "(3, 224, 224)\n",
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "segmented_image = draw_test_segmentation_map(model(X_test)[\"out\"])[0]\n",
    "print(segmented_image.shape)\n",
    "segmented_image = segmented_image.transpose((1, 0, 2))\n",
    "print(segmented_image.shape)\n",
    "print(X_test[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3eafe6cb-5a6e-4e7e-a6f1-d4970895b997",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.1) c:\\users\\appveyor\\appdata\\local\\temp\\1\\pip-req-build-kh7iq4w7\\opencv\\modules\\imgproc\\src\\color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function '__cdecl cv::impl::`anonymous-namespace'::CvtHelper<struct cv::impl::`anonymous namespace'::Set<3,4,-1>,struct cv::impl::A0x206ccf44::Set<3,4,-1>,struct cv::impl::A0x206ccf44::Set<0,2,5>,2>::CvtHelper(const class cv::_InputArray &,const class cv::_OutputArray &,int)'\n> Invalid number of channels in input image:\n>     'VScn::contains(scn)'\n> where\n>     'scn' is 224\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20412/2773391503.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msegmented_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_RGB2BGR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfinal_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage_overlay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msegmented_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'image'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.5.1) c:\\users\\appveyor\\appdata\\local\\temp\\1\\pip-req-build-kh7iq4w7\\opencv\\modules\\imgproc\\src\\color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function '__cdecl cv::impl::`anonymous-namespace'::CvtHelper<struct cv::impl::`anonymous namespace'::Set<3,4,-1>,struct cv::impl::A0x206ccf44::Set<3,4,-1>,struct cv::impl::A0x206ccf44::Set<0,2,5>,2>::CvtHelper(const class cv::_InputArray &,const class cv::_OutputArray &,int)'\n> Invalid number of channels in input image:\n>     'VScn::contains(scn)'\n> where\n>     'scn' is 224\n"
     ]
    }
   ],
   "source": [
    "cv2.cvtColor(segmented_image, cv2.COLOR_RGB2BGR)\n",
    "final_image = image_overlay(X_test[0], segmented_image)\n",
    "cv2.imshow('image', final_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098b5f2c-76c2-4191-a58d-f15d986d6172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = pred[0]\n",
    "# plt.imshow(X_test[0].mean(axis=0))\n",
    "# plt.show()\n",
    "\n",
    "# test_sum = np.median(test, axis=0)\n",
    "# plt.imshow(test_sum)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
