{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importation","metadata":{"execution":{"iopub.execute_input":"2022-04-11T16:15:07.161381Z","iopub.status.busy":"2022-04-11T16:15:07.160836Z","iopub.status.idle":"2022-04-11T16:15:08.995876Z","shell.execute_reply":"2022-04-11T16:15:08.995057Z","shell.execute_reply.started":"2022-04-11T16:15:07.161292Z"},"id":"NT4f2Kygv7iE"}},{"cell_type":"code","source":"# ! pip install kaggle\n# ! mkdir ~/.kaggle\n# ! cp kaggle.json ~/.kaggle/\n# ! chmod 600 ~/.kaggle/kaggle.json\n# ! kaggle datasets download -d enzodurand/boudingboxonlyhanddataset\n# ! unzip boudingboxonlyhanddataset.zip","metadata":{"id":"hhSLshnwwFP1","outputId":"b4db1ea2-e1e1-4338-ab22-511d5c65bd35","execution":{"iopub.status.busy":"2022-04-25T15:39:27.053841Z","iopub.execute_input":"2022-04-25T15:39:27.054121Z","iopub.status.idle":"2022-04-25T15:39:27.057846Z","shell.execute_reply.started":"2022-04-25T15:39:27.054089Z","shell.execute_reply":"2022-04-25T15:39:27.057149Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"import os\nimport copy\nimport cv2\n# import wandb\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom time import time\nfrom sklearn import preprocessing\nfrom matplotlib import pyplot as plt\n\nimport torchvision\nfrom torchvision import models, transforms\nfrom torchvision.io import read_image\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.utils.data import Dataset\n\n# !pip uninstall albumentations\n# !pip install albumentations==0.4.6\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2","metadata":{"id":"BSdwgPVvv7iK","outputId":"9bb2e587-33b2-41cc-9220-f2625433242c","execution":{"iopub.status.busy":"2022-04-25T15:39:27.183108Z","iopub.execute_input":"2022-04-25T15:39:27.183593Z","iopub.status.idle":"2022-04-25T15:39:27.191023Z","shell.execute_reply.started":"2022-04-25T15:39:27.183561Z","shell.execute_reply":"2022-04-25T15:39:27.190219Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# GPU/TPU setup","metadata":{"id":"1C5YIPfwzhxp"}},{"cell_type":"code","source":"## TPU\n# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n# !python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev\n# import torch_xla\n# import torch_xla.core.xla_model as xm\n# device = xm.xla_device()\n# torch.set_default_tensor_type('torch.FloatTensor')\n\n## GPU\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\n## Weight and biases\n# wandb.login()","metadata":{"id":"vVQzDrNlzfI_","outputId":"d2328c3e-a835-4f78-af06-fc5339afac63","execution":{"iopub.status.busy":"2022-04-25T15:39:27.300189Z","iopub.execute_input":"2022-04-25T15:39:27.300452Z","iopub.status.idle":"2022-04-25T15:39:27.305748Z","shell.execute_reply.started":"2022-04-25T15:39:27.300407Z","shell.execute_reply":"2022-04-25T15:39:27.304953Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"id":"CfM_nSBaiXcB","outputId":"9e21e657-01d3-4e87-8ec7-ace6895e068f","execution":{"iopub.status.busy":"2022-04-25T15:39:27.413898Z","iopub.execute_input":"2022-04-25T15:39:27.414703Z","iopub.status.idle":"2022-04-25T15:39:28.335113Z","shell.execute_reply.started":"2022-04-25T15:39:27.414658Z","shell.execute_reply":"2022-04-25T15:39:28.334197Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# Global variables ","metadata":{"id":"1jRIVkpdv7iM"}},{"cell_type":"code","source":"INPUT_SIZE = 400\nN_CLASS = 4\nWHERE = \"kaggle\"","metadata":{"id":"huwjGSaZv7iN","execution":{"iopub.status.busy":"2022-04-25T15:39:28.337024Z","iopub.execute_input":"2022-04-25T15:39:28.337318Z","iopub.status.idle":"2022-04-25T15:39:28.347664Z","shell.execute_reply.started":"2022-04-25T15:39:28.337270Z","shell.execute_reply":"2022-04-25T15:39:28.346829Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"if WHERE==\"colab\":\n    PATH_LABELS = \"/content/index_label_bbox.csv\"\n    PATH_IMG = \"/content/output/output\"\n    PATH_LABELS_VALID = \"/content/index_label_bbox_validation.csv\"\n    PATH_IMG_VALID = \"/content/output_validation/output_validation\"\n    BATCH_SIZE = 32\nelif WHERE==\"kaggle\":\n    PATH_LABELS = \"../input/boudingboxonlyhanddataset/index_label_bbox.csv\"\n    PATH_IMG = \"../input/boudingboxonlyhanddataset/output/output\"\n    PATH_LABELS_VALID = \"../input/boudingboxonlyhanddataset/index_label_bbox_validation.csv\"\n    PATH_IMG_VALID = \"../input/boudingboxonlyhanddataset/output_validation/output_validation\"\n    BATCH_SIZE = 64\nelif WHERE==\"home\":\n    PATH_LABELS = \"../../../data_labels/bounding_box_model/done/index_label_bbox.csv\"\n    PATH_IMG = \"../../../data_labels/bounding_box_model/done/output\"\n    PATH_LABELS_VALID = \"../../../data_labels/bounding_box_model/done_validation/index_label_bbox_validation.csv\"\n    PATH_IMG_VALID = \"../../../data_labels/bounding_box_model/done_validation/output_validation\"\n    BATCH_SIZE = 4","metadata":{"id":"e-dNYYCyz8E4","execution":{"iopub.status.busy":"2022-04-25T15:39:28.349285Z","iopub.execute_input":"2022-04-25T15:39:28.349736Z","iopub.status.idle":"2022-04-25T15:39:28.362677Z","shell.execute_reply.started":"2022-04-25T15:39:28.349696Z","shell.execute_reply":"2022-04-25T15:39:28.361769Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"# Data functions","metadata":{"execution":{"iopub.execute_input":"2022-04-11T16:20:53.566375Z","iopub.status.busy":"2022-04-11T16:20:53.56609Z","iopub.status.idle":"2022-04-11T16:20:53.569977Z","shell.execute_reply":"2022-04-11T16:20:53.569332Z","shell.execute_reply.started":"2022-04-11T16:20:53.566327Z"},"id":"nSGukie2v7iN"}},{"cell_type":"code","source":"class HandGestureDataset(Dataset):\n    def __init__(self, annotations_file, img_dir, transform=None):\n        self.img_labels = pd.read_csv(annotations_file)\n        self.img_dir = img_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.img_labels)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, os.listdir(self.img_dir)[idx])\n        image = read_image(img_path)\n        path = str(\"output/\"+os.listdir(self.img_dir)[idx]).split(\"/\")[0]\n        line = self.img_labels[\"index\"] == str(path+\"/\"+os.listdir(self.img_dir)[idx])\n        x, y, x_end, y_end = self.img_labels.loc[line][\"x\"].item(),\\\n                                self.img_labels.loc[line][\"y\"].item(),\\\n                                self.img_labels.loc[line][\"x_end\"].item(),\\\n                                self.img_labels.loc[line][\"y_end\"].item()\n        x, y, x_end, y_end = x/INPUT_SIZE, y/INPUT_SIZE, x_end/INPUT_SIZE, y_end/INPUT_SIZE\n        image = image.permute(1,2,0)\n        image = image/255\n        if self.transform:\n            transformed = self.transform(image=np.array(image), bboxes=[[x,y,x_end,y_end]])\n        transformed_image = transformed['image']\n        transformed_bboxes = transformed['bboxes']\n        return transformed_image, transformed_bboxes\n#         return image, [x, y, x_end, y_end]","metadata":{"id":"uwmn_pJKv7iO","execution":{"iopub.status.busy":"2022-04-25T15:39:28.365110Z","iopub.execute_input":"2022-04-25T15:39:28.367650Z","iopub.status.idle":"2022-04-25T15:39:28.388900Z","shell.execute_reply.started":"2022-04-25T15:39:28.367607Z","shell.execute_reply":"2022-04-25T15:39:28.387950Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def draw_predictions(image, preds):\n    startX, startY, endX, endY = preds\n    # scale the predicted bounding box coordinates based on the image\n    # dimensions    \n    startX = int(startX * INPUT_SIZE)\n    startY = int(startY * INPUT_SIZE)\n    endX = int(endX * INPUT_SIZE)\n    endY = int(endY * INPUT_SIZE)\n#     print(startX, startY, endX, endY)\n    # draw the predicted bounding box on the image\n    image = image.numpy().copy()\n    cv2.rectangle(image, (startX, startY), (endX, endY), (0, 255, 0), 2)\n    # show the output image\n    plt.imshow(image)\n    plt.show()\n\ndef prepare_data_vgg(data_type):\n    ## Parameters fitting vgg/imagenet\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n\n    transformVGGTrainAlbu = A.Compose([\n        A.VerticalFlip(p=0.3),\n        A.HorizontalFlip(p=0.5),\n        A.Blur(p=0.3, blur_limit=5),\n\n        A.RandomBrightnessContrast(p=0.3),\n        A.RandomGamma(p=0.3),\n        A.ChannelShuffle(p=0.3),\n        A.Rotate(p=0.5, limit=60),\n# \n#         A.Downscale(p=0.3, scale_min=0.6, scale_max=0.9),\n#         A.ShiftScaleRotate(p=0.3),\n#         A.ElasticTransform(p=0.3, border_mode=cv2.BORDER_REFLECT_101, alpha_affine=40),\n#         A.RGBShift(r_shift_limit=0.3, g_shift_limit=0.3, b_shift_limit=30, p=0.3),\n        \n#         A.Normalize(mean=mean, std=std),\n        A.Resize(INPUT_SIZE, INPUT_SIZE, p=1),\n        ToTensorV2(),\n    ], bbox_params=A.BboxParams(format='albumentations', label_fields=\"\"))\n    \n    transformVGGValidAlbu = A.Compose([\n#         A.Normalize(mean=mean, std=std),\n        A.Resize(INPUT_SIZE, INPUT_SIZE, p=1),\n        ToTensorV2(),\n    ], bbox_params=A.BboxParams(format='albumentations', label_fields=\"\"))\n\n    if data_type == \"custom\":\n        ## Custom dataset\n        VGG_dataset_train = HandGestureDataset(PATH_LABELS, PATH_IMG, transformVGGTrainAlbu)\n        VGG_dataset_valid = HandGestureDataset(PATH_LABELS_VALID, PATH_IMG_VALID, transformVGGValidAlbu)\n        VGG_trainloader = torch.utils.data.DataLoader(VGG_dataset_train, batch_size=BATCH_SIZE, pin_memory=True, shuffle=True)\n        VGG_validloader = torch.utils.data.DataLoader(VGG_dataset_valid, batch_size=BATCH_SIZE, pin_memory=True, shuffle=True)\n\n    return VGG_trainloader, VGG_validloader","metadata":{"id":"VQreQL2ov7iP","execution":{"iopub.status.busy":"2022-04-25T15:39:28.390567Z","iopub.execute_input":"2022-04-25T15:39:28.391143Z","iopub.status.idle":"2022-04-25T15:39:28.415877Z","shell.execute_reply.started":"2022-04-25T15:39:28.391104Z","shell.execute_reply":"2022-04-25T15:39:28.415175Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# Loading data into pytorch dataset and dataloader objects","metadata":{"execution":{"iopub.execute_input":"2022-02-15T15:49:41.695334Z","iopub.status.busy":"2022-02-15T15:49:41.69497Z","iopub.status.idle":"2022-02-15T15:49:46.639507Z","shell.execute_reply":"2022-02-15T15:49:46.638679Z","shell.execute_reply.started":"2022-02-15T15:49:41.695293Z"},"id":"I7EYl1LCv7iQ"}},{"cell_type":"code","source":"VGG_trainloader, VGG_validloader = prepare_data_vgg(\"custom\")","metadata":{"id":"gSLUtb3rv7iQ","execution":{"iopub.status.busy":"2022-04-25T15:39:28.421125Z","iopub.execute_input":"2022-04-25T15:39:28.423909Z","iopub.status.idle":"2022-04-25T15:39:28.447629Z","shell.execute_reply.started":"2022-04-25T15:39:28.423865Z","shell.execute_reply":"2022-04-25T15:39:28.446979Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# for img, bbox in VGG_trainloader:\n#     res = []\n#     for e in bbox:\n#         res_ = []\n#         for elt in e:\n#             res_.append(elt.numpy())\n#         res.append(np.array(res_))\n#     res = np.array(res).T.squeeze()\n#     cpt = 0\n#     for i, l in zip(img, res):\n#         draw_predictions(i.permute(1,2,0), l)\n\n# for img, bbox in VGG_validloader:\n#     res = []\n#     for e in bbox:\n#         res_ = []\n#         for elt in e:\n#             res_.append(elt.numpy())\n#         res.append(np.array(res_))\n#     res = np.array(res).T.squeeze()\n#     cpt = 0\n#     for i, l in zip(img, res):\n#         draw_predictions(i.permute(1,2,0), l)","metadata":{"id":"Dxv-Unzwv7iR","execution":{"iopub.status.busy":"2022-04-25T15:39:28.451404Z","iopub.execute_input":"2022-04-25T15:39:28.453397Z","iopub.status.idle":"2022-04-25T15:39:28.460283Z","shell.execute_reply.started":"2022-04-25T15:39:28.453358Z","shell.execute_reply":"2022-04-25T15:39:28.459390Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"# Model functions","metadata":{"id":"0yNIY1tPv7iR"}},{"cell_type":"code","source":"def train(model, epochs, train_loader, valid_loader, learning_rate, patience, feature_extract=False):\n    ## Early stopping variables\n    es = EarlyStopping(patience=patience)\n    terminate_training = False\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_loss = np.inf\n    model = model.to(device)\n    ## Training only the parameters where we require gradient since we are fine-tuning\n    params_to_update = model.parameters()\n    print(\"params to learn:\")\n    if feature_extract:\n        params_to_update = []\n        for name,param in model.named_parameters():\n            if param.requires_grad == True:\n                params_to_update.append(param)\n                print(\"\\t\", name)\n    else:\n        for name,param in model.named_parameters():\n            if param.requires_grad == True:\n                print(\"\\t\", name)\n                \n    ## Setting up our optimizer\n    optim = torch.optim.Adam(params_to_update, lr=learning_rate)\n\n    ## Setting up our loss function\n    loss = nn.MSELoss()\n\n    ## Running the train loop\n    print(f\"running {model.name}\")\n    for epoch in range(epochs):\n        cumloss, count = 0, 0\n        model.train()\n        for x,y in train_loader:\n            optim.zero_grad()\n            x = x.to(device)\n            x = x.float()\n            res = []\n            for e in y:\n                res_ = []\n                for elt in e:\n                    res_.append(elt.numpy())\n                res.append(np.array(res_))\n            res = np.array(res).T.squeeze()\n#             print(\"/\"*20)\n#             print(res)\n#             print(\"/\"*20)\n            y = torch.as_tensor(res)\n            y = y.to(torch.float32)\n            y = y.to(device)\n            yhat = model(x)\n            l = loss(yhat, y)\n            l.backward()\n            # xm.optimizer_step(optim, barrier=True)\n            optim.step()\n            cumloss += l * len(x)\n            count += len(x)\n        print(\"epoch :\", epoch, end=\"\")\n        loss_ = cumloss.cpu().item()/count\n#         wandb.log({'train_loss': loss_})\n        print(\", train_loss: \", loss_, end=\"\")\n        if epoch % 1 == 0:\n            model.eval()\n            with torch.no_grad():\n                valid_cumloss, count = 0, 0\n                for x,y in valid_loader:\n                    x = x.to(device)\n                    x = x.float()\n                    res = []\n                    for e in y:\n                        res_ = []\n                        for elt in e:\n                            res_.append(elt.numpy())\n                        res.append(np.array(res_))\n                    res = np.array(res).T.squeeze()\n#                     print(\"ù\"*20)\n#                     print(res)\n#                     print(\"ù\"*20)\n                    y = torch.as_tensor(res)\n                    y = y.to(torch.float32)\n                    y = y.to(device)\n                    yhat = model(x)\n                    valid_cumloss += loss(yhat,y) * len(x)\n                    count += len(x)\n                valid_loss_ = valid_cumloss.cpu().item()/count\n#                 wandb.log({'valid_loss': valid_loss_})\n                print(\", valid_loss: \", valid_loss_)\n                ## Early stopping\n                if valid_cumloss/count < best_loss:\n                    best_loss = valid_cumloss/count\n                    best_model_wts = copy.deepcopy(model.state_dict())\n                if es.step(valid_cumloss.cpu().item()/count):\n                    terminate_training = True\n                    break\n        if terminate_training:\n            break\n    print('Best val loss: {:4f}'.format(best_loss))\n    ## Returns the best model\n    model.load_state_dict(best_model_wts)\n    return model\n\ndef set_parameter_requires_grad(model, feature_extract):\n    if feature_extract:\n        for name,p in model.named_parameters():\n            if \"features\" in name:\n                p.requires_grad = False    \n            else:\n                p.requires_grad = True  ","metadata":{"_kg_hide-input":true,"id":"TmOsJNGTv7iS","execution":{"iopub.status.busy":"2022-04-25T15:39:28.462997Z","iopub.execute_input":"2022-04-25T15:39:28.463240Z","iopub.status.idle":"2022-04-25T15:39:28.498503Z","shell.execute_reply.started":"2022-04-25T15:39:28.463208Z","shell.execute_reply":"2022-04-25T15:39:28.497751Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"# Loading the model and modifying the classifier part","metadata":{"execution":{"iopub.execute_input":"2022-04-11T16:22:20.614896Z","iopub.status.busy":"2022-04-11T16:22:20.614632Z","iopub.status.idle":"2022-04-11T16:22:20.618197Z","shell.execute_reply":"2022-04-11T16:22:20.617386Z","shell.execute_reply.started":"2022-04-11T16:22:20.614868Z"},"id":"LdrT-bVhv7iS"}},{"cell_type":"code","source":"## Loading vgg16 model pretrained on imagenet\nvgg = models.vgg16(pretrained=True)\n\nvgg.classifier = nn.Sequential(nn.Linear(25088, 8192), \n                               nn.ReLU(), \n                               nn.Dropout(0.5),        \n                               nn.Linear(8192, 2048), \n                               nn.ReLU(), \n                               nn.Dropout(0.5),        \n                               nn.Linear(2048, 512),\n                               nn.ReLU(), \n                               nn.Dropout(0.5),        \n                               nn.Linear(512, N_CLASS),\n                               nn.Sigmoid())\n\nprint(vgg.eval())\n\n## Sets all the requires grad of the classifier layers to True\nset_parameter_requires_grad(vgg, True)","metadata":{"id":"X_bEUNDLv7iT","outputId":"74d04a24-a4b5-436c-a9d9-8cae0997a8e2","execution":{"iopub.status.busy":"2022-04-25T15:39:28.503427Z","iopub.execute_input":"2022-04-25T15:39:28.504437Z","iopub.status.idle":"2022-04-25T15:39:32.470249Z","shell.execute_reply.started":"2022-04-25T15:39:28.504398Z","shell.execute_reply":"2022-04-25T15:39:32.469500Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"# Implementing early stopping","metadata":{"id":"oiqed-DRv7iT"}},{"cell_type":"code","source":"class EarlyStopping(object):\n    def __init__(self, mode='min', min_delta=0, patience=10, percentage=False):\n        self.mode = mode\n        self.min_delta = min_delta\n        self.patience = patience\n        self.best = None\n        self.num_bad_epochs = 0\n        self.is_better = None\n        self._init_is_better(mode, min_delta, percentage)\n        if patience == 0:\n            self.is_better = lambda a, b: True\n            self.step = lambda a: False\n\n    def step(self, metrics):\n        if self.best is None:\n            self.best = metrics\n            return False\n        if np.isnan(metrics):\n            return True\n        if self.is_better(metrics, self.best):\n            self.num_bad_epochs = 0\n            self.best = metrics\n        else:\n            self.num_bad_epochs += 1\n        if self.num_bad_epochs >= self.patience:\n            return True\n        return False\n\n    def _init_is_better(self, mode, min_delta, percentage):\n        if mode not in {'min', 'max'}:\n            raise ValueError('mode ' + mode + ' is unknown!')\n        if not percentage:\n            if mode == 'min':\n                self.is_better = lambda a, best: a < best - min_delta\n            if mode == 'max':\n                self.is_better = lambda a, best: a > best + min_delta\n        else:\n            if mode == 'min':\n                self.is_better = lambda a, best: a < best - (\n                            best * min_delta / 100)\n            if mode == 'max':\n                self.is_better = lambda a, best: a > best + (\n                            best * min_delta / 100)","metadata":{"id":"N4pImz6Av7iU","execution":{"iopub.status.busy":"2022-04-25T15:39:32.472821Z","iopub.execute_input":"2022-04-25T15:39:32.473438Z","iopub.status.idle":"2022-04-25T15:39:32.486251Z","shell.execute_reply.started":"2022-04-25T15:39:32.473405Z","shell.execute_reply":"2022-04-25T15:39:32.485445Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"# Training only the modified parts of the classifier","metadata":{"id":"tRpWlPvEv7iU"}},{"cell_type":"code","source":"# os.environ['WANDB_NOTEBOOK_NAME'] = '4096_5e-6'\n# wandb.init(project=\"jetson-autonomous-driving\")","metadata":{"id":"46vOGoBkecro","execution":{"iopub.status.busy":"2022-04-25T15:39:32.488000Z","iopub.execute_input":"2022-04-25T15:39:32.488364Z","iopub.status.idle":"2022-04-25T15:39:32.497299Z","shell.execute_reply.started":"2022-04-25T15:39:32.488329Z","shell.execute_reply":"2022-04-25T15:39:32.496342Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"print(len(VGG_trainloader))\nprint(len(VGG_validloader))\n\n## Fine-tuning the model on our data\nvgg.name = \"VGG\"\n\nbest_model = train(model=vgg, \n                   epochs=1000, \n                   train_loader=VGG_trainloader, \n                   valid_loader=VGG_validloader, \n                   learning_rate=5e-5,\n                   patience=20) ## metric for earlystopping : val_loss ","metadata":{"id":"JXlDQQfhv7iV","outputId":"9e28c8a1-b134-4f27-d9da-e440e1d4bef0","execution":{"iopub.status.busy":"2022-04-25T15:39:32.498843Z","iopub.execute_input":"2022-04-25T15:39:32.499445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Checking predictions","metadata":{"id":"9_RQMAV0v7iV"}},{"cell_type":"code","source":"# with torch.no_grad():\n#     for item in VGG_trainloader:\n#         x, y = item[\"image\"], item[\"label\"]\n#         base_img = item[\"image\"]\n#         x = item[\"image\"].to(device)\n#         res = []\n#         for e in y:\n#             res.append(np.array(e))\n#         res = np.array(res).T\n#         y = torch.as_tensor(res)\n#         y = y.to(torch.float32)\n#         y = y.to(device)\n#         yhat = best_model(x)\n#         for i in range(4):\n#             draw_predictions(base_img[i].permute(1,2,0), yhat.cpu()[i])\n\nwith torch.no_grad():\n    for x, y in VGG_validloader:\n        x = x.to(device)\n        x = x.float()\n        yhat = best_model(x)\n        for i in range(4):\n            draw_predictions(x[i].cpu().permute(1,2,0), yhat.cpu()[i])","metadata":{"id":"FDqVuSEIv7iW","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Saving the model in .pth and .onnx extension","metadata":{"id":"yOuDvy_7v7iW"}},{"cell_type":"code","source":"PATH = \"./\"\ntorch.save(best_model.state_dict(), os.path.join(PATH,\"boundingbox_vgg_last.pth\"))\n# from google.colab import files\n# files.download(os.path.join(PATH,\"boundingbox_vgg_last.pth\"))","metadata":{"id":"69YZ-O8Pv7iW","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# del vgg\n# del best_model","metadata":{"id":"k3zZ0xCAv7iX","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = models.vgg16(pretrained=True)\n# model.classifier[0] = nn.Linear(25088, 8192)\n# model.classifier[3] = nn.Linear(8192, 1024)\n# model.classifier[6] = nn.Linear(1024, N_CLASS)\n# model.load_state_dict(torch.load(os.path.join(PATH,\"vgg.pth\"), map_location='cpu'))\n# model.eval() \n\n# dummy_input = torch.randn(BATCH_SIZE, 3, INPUT_SIZE, INPUT_SIZE)  \n# torch.onnx.export(model,   \n#                   dummy_input, \n#                   \"vgg.onnx\",\n#                   export_params=True,\n#                   do_constant_folding=True, \n#                   input_names = ['modelInput'],\n#                   output_names = ['modelOutput'])","metadata":{"id":"fhRGHT_zv7iX","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"3nyDFGuUv7iX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"DLpVF-Vov7iX"},"execution_count":null,"outputs":[]}]}