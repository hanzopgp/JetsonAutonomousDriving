{"cells":[{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-04-11T16:15:07.161381Z","iopub.status.busy":"2022-04-11T16:15:07.160836Z","iopub.status.idle":"2022-04-11T16:15:08.995876Z","shell.execute_reply":"2022-04-11T16:15:08.995057Z","shell.execute_reply.started":"2022-04-11T16:15:07.161292Z"}},"source":["# Importation"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T11:58:07.911562Z","iopub.status.busy":"2022-04-18T11:58:07.910544Z","iopub.status.idle":"2022-04-18T11:58:11.223143Z","shell.execute_reply":"2022-04-18T11:58:11.221959Z","shell.execute_reply.started":"2022-04-18T11:58:07.911406Z"},"trusted":true},"outputs":[],"source":["import os\n","import copy\n","import cv2\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from time import time\n","from sklearn import preprocessing\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import f1_score \n","from matplotlib import pyplot as plt\n","\n","# import torchvision\n","from torchvision import models, transforms\n","from torchvision.io import read_image\n","\n","import torch\n","import torch.nn.functional as F\n","from torch import nn\n","from torch.utils.data import Dataset"]},{"cell_type":"markdown","metadata":{},"source":["# Global variables "]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T11:58:11.225720Z","iopub.status.busy":"2022-04-18T11:58:11.225349Z","iopub.status.idle":"2022-04-18T11:58:11.234233Z","shell.execute_reply":"2022-04-18T11:58:11.233215Z","shell.execute_reply.started":"2022-04-18T11:58:11.225677Z"},"trusted":true},"outputs":[],"source":["INPUT_SIZE = 400\n","BATCH_SIZE = 64\n","N_CLASS = 4\n","\n","# PATH_LABELS = \"../input/boudingboxonlyhanddataset/index_label_bbox.csv\"\n","# PATH_IMG = \"../input/boudingboxonlyhanddataset/output/output\"\n","# PATH_LABELS_VALID = \"../input/boudingboxonlyhanddataset/index_label_bbox_validation.csv\"\n","# PATH_IMG_VALID = \"../input/boudingboxonlyhanddataset/output_validation/output_validation\"\n","\n","PATH_LABELS = \"../../../data_labels/bouding_box_model/done/index_label_bbox.csv\"\n","PATH_IMG = \"../../../data_labels/bouding_box_model/done/output/\"\n","PATH_LABELS_VALID = \"../../../data_labels/bouding_box_model/done_validation/index_label_bbox_validation.csv\"\n","PATH_IMG_VALID = \"../../../data_labels/bouding_box_model/done_validation/output_validation/\""]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-04-11T16:20:53.566375Z","iopub.status.busy":"2022-04-11T16:20:53.56609Z","iopub.status.idle":"2022-04-11T16:20:53.569977Z","shell.execute_reply":"2022-04-11T16:20:53.569332Z","shell.execute_reply.started":"2022-04-11T16:20:53.566327Z"}},"source":["# Data functions"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T11:58:11.236305Z","iopub.status.busy":"2022-04-18T11:58:11.236071Z","iopub.status.idle":"2022-04-18T11:58:11.248085Z","shell.execute_reply":"2022-04-18T11:58:11.247309Z","shell.execute_reply.started":"2022-04-18T11:58:11.236276Z"},"trusted":true},"outputs":[],"source":["class HandGestureDataset(Dataset):\n","    def __init__(self, annotations_file, img_dir, transform=None):\n","        self.img_labels = pd.read_csv(annotations_file)\n","        self.img_dir = img_dir\n","        self.transform = transform\n","    \n","    def __len__(self):\n","        return len(self.img_labels)\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.img_dir, os.listdir(self.img_dir)[idx])\n","        image = read_image(img_path)\n","        line = self.img_labels[\"index\"] == str(\"output/\"+os.listdir(self.img_dir)[idx])\n","        x, y, end_x, end_y = self.img_labels.loc[line][\"x\"].item(),\\\n","                               self.img_labels.loc[line][\"y\"].item(),\\\n","                               self.img_labels.loc[line][\"x_end\"].item(),\\\n","                               self.img_labels.loc[line][\"y_end\"].item()\n","        x, y, end_x, end_y = x/INPUT_SIZE, y/INPUT_SIZE, end_x/INPUT_SIZE, end_y/INPUT_SIZE\n","        if self.transform:\n","            image = self.transform(image)\n","#         image = F.normalize(image, dim = 0)\n","        return {'image':image, 'label':[x,y,end_x,end_y]}"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T11:58:11.250531Z","iopub.status.busy":"2022-04-18T11:58:11.250098Z","iopub.status.idle":"2022-04-18T11:58:11.263487Z","shell.execute_reply":"2022-04-18T11:58:11.262296Z","shell.execute_reply.started":"2022-04-18T11:58:11.250483Z"},"trusted":true},"outputs":[],"source":["def prepare_data_vgg(data_type):\n","    ## Parameters fitting vgg/imagenet\n","    mean=[0.485, 0.456, 0.406]\n","    std=[0.229, 0.224, 0.225]\n","\n","    ## pytorch transformer objects\n","    transformVGGTrain=transforms.Compose([\n","            transforms.ToPILImage(),\n","        \n","            ## can try\n","            transforms.ColorJitter(brightness=0.5, hue=0.5),\n","#             transforms.RandomSolarize(threshold=192.0),\n","            transforms.RandomAdjustSharpness(sharpness_factor=3),\n","            transforms.RandomAutocontrast(),\n","            transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.01, 5)),\n","        \n","            ## don't use or it will ruin bounding box labels\n","#             transforms.RandomRotation(degrees=(-15, 15)),\n","#             transforms.RandomPerspective(distortion_scale=0.4, p=0.2),\n","#             transforms.RandomAffine(degrees=(0, 5), translate=(0, 0.18), scale=(0.7, 1)),\n","        \n","            ## too much\n","#             transforms.RandomPosterize(bits=2),\n","#             transforms.RandomInvert(),\n","#             transforms.RandomEqualize(),\n","        \n","            transforms.Resize(size=(INPUT_SIZE, INPUT_SIZE)),\n","            transforms.ToTensor(),\n","#             transforms.Normalize(mean, std) ## test with and without\n","        ])\n","    transformVGGValid=transforms.Compose([\n","            transforms.ToPILImage(),\n","            transforms.Resize(size=(INPUT_SIZE, INPUT_SIZE)),\n","            transforms.ToTensor(),\n","#             transforms.Normalize(mean, std) ## test with and without\n","        ])\n","\n","    if data_type == \"custom\":\n","        ## Custom dataset\n","        VGG_dataset_train = HandGestureDataset(PATH_LABELS, PATH_IMG, transformVGGTrain)\n","        VGG_dataset_valid = HandGestureDataset(PATH_LABELS_VALID, PATH_IMG_VALID, transformVGGValid)\n","        VGG_trainloader = torch.utils.data.DataLoader(VGG_dataset_train, batch_size=BATCH_SIZE, pin_memory=True, shuffle=True)\n","        VGG_validloader = torch.utils.data.DataLoader(VGG_dataset_valid, batch_size=BATCH_SIZE, pin_memory=True, shuffle=True)\n","    \n","    return VGG_trainloader, VGG_validloader"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-02-15T15:49:41.695334Z","iopub.status.busy":"2022-02-15T15:49:41.69497Z","iopub.status.idle":"2022-02-15T15:49:46.639507Z","shell.execute_reply":"2022-02-15T15:49:46.638679Z","shell.execute_reply.started":"2022-02-15T15:49:41.695293Z"}},"source":["# Loading data into pytorch dataset and dataloader objects"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T11:58:11.265540Z","iopub.status.busy":"2022-04-18T11:58:11.265294Z","iopub.status.idle":"2022-04-18T11:58:11.301628Z","shell.execute_reply":"2022-04-18T11:58:11.300387Z","shell.execute_reply.started":"2022-04-18T11:58:11.265514Z"},"trusted":true},"outputs":[],"source":["VGG_trainloader, VGG_validloader = prepare_data_vgg(\"custom\")"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T11:58:11.303751Z","iopub.status.busy":"2022-04-18T11:58:11.303355Z","iopub.status.idle":"2022-04-18T11:58:11.309138Z","shell.execute_reply":"2022-04-18T11:58:11.308066Z","shell.execute_reply.started":"2022-04-18T11:58:11.303704Z"},"trusted":true},"outputs":[],"source":["# for img in VGG_trainloader:\n","#     for i in img[\"image\"][:1]:\n","#         i = i.permute(1,2,0)\n","#         plt.imshow(np.array(i))\n","#         plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Model functions"]},{"cell_type":"code","execution_count":15,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-04-18T11:58:11.311833Z","iopub.status.busy":"2022-04-18T11:58:11.311177Z","iopub.status.idle":"2022-04-18T11:58:11.337631Z","shell.execute_reply":"2022-04-18T11:58:11.336657Z","shell.execute_reply.started":"2022-04-18T11:58:11.311779Z"},"trusted":true},"outputs":[],"source":["def train(model, epochs, train_loader, valid_loader, learning_rate, patience, feature_extract=False):\n","    ## Early stopping variables\n","    es = EarlyStopping(patience=patience)\n","    terminate_training = False\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_loss = np.inf\n","    model = model.to(device)\n","    ## Training only the parameters where we require gradient since we are fine-tuning\n","    params_to_update = model.parameters()\n","    print(\"params to learn:\")\n","    if feature_extract:\n","        params_to_update = []\n","        for name,param in model.named_parameters():\n","            if param.requires_grad == True:\n","                params_to_update.append(param)\n","                print(\"\\t\", name)\n","    else:\n","        for name,param in model.named_parameters():\n","            if param.requires_grad == True:\n","                print(\"\\t\", name)\n","                \n","    ## Setting up our optimizer\n","    optim = torch.optim.Adam(params_to_update, lr=learning_rate)\n","    \n","    ## Setting up our loss function\n","    loss = nn.MSELoss()\n","    \n","    ## Running the train loop\n","    print(f\"running {model.name}\")\n","    for epoch in range(epochs):\n","        cumloss, count = 0, 0\n","        model.train()\n","        for item in tqdm(train_loader):\n","            optim.zero_grad()\n","            x, y = item[\"image\"], item[\"label\"]\n","            x = item[\"image\"].to(device)\n","            \n","            res = []\n","            for e in y:\n","                res.append(np.array(e))\n","            res = np.array(res).T\n","            y = torch.as_tensor(res)\n","            y = y.to(torch.float32)\n","            y = y.to(device)\n","            \n","            yhat = model(x)\n","\n","            l = loss(yhat, y)\n","            l.backward()\n","            optim.step()\n","            cumloss += l * len(x)\n","            count += len(x)\n","        print(\"epoch :\", epoch, end=\"\")\n","        print(\", train_loss: \", cumloss.cpu().item()/count, end=\"\")\n","        if epoch % 1 == 0:\n","            model.eval()\n","            with torch.no_grad():\n","                valid_cumloss, count = 0, 0\n","                for x,y in valid_loader:\n","                    x, y = item[\"image\"], item[\"label\"]\n","                    x = item[\"image\"].to(device)\n","                    res = []\n","                    for e in y:\n","                        res.append(np.array(e))\n","                    res = np.array(res).T\n","                    y = torch.as_tensor(res)\n","                    y = y.to(torch.float32)\n","                    y = y.to(device)\n","                    yhat = model(x)\n","                    valid_cumloss += loss(yhat,y) * len(x)\n","                    count += len(x)\n","                print(\", valid_loss: \", valid_cumloss.cpu().item()/count)\n","                ## Early stopping\n","                if valid_cumloss/count < best_loss:\n","                    best_loss = valid_cumloss/count\n","                    best_model_wts = copy.deepcopy(model.state_dict())\n","                if es.step(valid_cumloss.cpu().item()/count):\n","                    terminate_training = True\n","                    break\n","        if terminate_training:\n","            break\n","    print('Best val loss: {:4f}'.format(best_loss))\n","    ## Returns the best model\n","    model.load_state_dict(best_model_wts)\n","    return model\n","\n","def set_parameter_requires_grad(model, feature_extract):\n","    if feature_extract:\n","        for name,p in model.named_parameters():\n","            if \"features\" in name:\n","                p.requires_grad = False    \n","            else:\n","                p.requires_grad = True  "]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-04-11T16:22:20.614896Z","iopub.status.busy":"2022-04-11T16:22:20.614632Z","iopub.status.idle":"2022-04-11T16:22:20.618197Z","shell.execute_reply":"2022-04-11T16:22:20.617386Z","shell.execute_reply.started":"2022-04-11T16:22:20.614868Z"}},"source":["# Loading the model and modifying the classifier part\n","### Maybe we could try to modify only the last classifier layer ?"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T11:58:11.339763Z","iopub.status.busy":"2022-04-18T11:58:11.339400Z","iopub.status.idle":"2022-04-18T11:58:25.947622Z","shell.execute_reply":"2022-04-18T11:58:25.946904Z","shell.execute_reply.started":"2022-04-18T11:58:11.339717Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["False\n"]},{"ename":"AssertionError","evalue":"Torch not compiled with CUDA enabled","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[1;32m/mnt/c/Users/karna/Desktop/Projects/PythonProject/JetsonAutonomousDriving/src/main/only_bounding_box_model/vgg-bounding-box.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/mnt/c/Users/karna/Desktop/Projects/PythonProject/JetsonAutonomousDriving/src/main/only_bounding_box_model/vgg-bounding-box.ipynb#ch0000013vscode-remote?line=1'>2</a>\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/mnt/c/Users/karna/Desktop/Projects/PythonProject/JetsonAutonomousDriving/src/main/only_bounding_box_model/vgg-bounding-box.ipynb#ch0000013vscode-remote?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available())\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/mnt/c/Users/karna/Desktop/Projects/PythonProject/JetsonAutonomousDriving/src/main/only_bounding_box_model/vgg-bounding-box.ipynb#ch0000013vscode-remote?line=3'>4</a>\u001b[0m torch\u001b[39m.\u001b[39;49mzeros(\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mcuda()\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/mnt/c/Users/karna/Desktop/Projects/PythonProject/JetsonAutonomousDriving/src/main/only_bounding_box_model/vgg-bounding-box.ipynb#ch0000013vscode-remote?line=5'>6</a>\u001b[0m \u001b[39m## Loading vgg16 model pretrained on imagenet\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/mnt/c/Users/karna/Desktop/Projects/PythonProject/JetsonAutonomousDriving/src/main/only_bounding_box_model/vgg-bounding-box.ipynb#ch0000013vscode-remote?line=6'>7</a>\u001b[0m vgg \u001b[39m=\u001b[39m vgg16(pretrained\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n","File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/cuda/__init__.py:208\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='file:///home/hanzopgp/miniconda3/lib/python3.9/site-packages/torch/cuda/__init__.py?line=203'>204</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/hanzopgp/miniconda3/lib/python3.9/site-packages/torch/cuda/__init__.py?line=204'>205</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/hanzopgp/miniconda3/lib/python3.9/site-packages/torch/cuda/__init__.py?line=205'>206</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///home/hanzopgp/miniconda3/lib/python3.9/site-packages/torch/cuda/__init__.py?line=206'>207</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> <a href='file:///home/hanzopgp/miniconda3/lib/python3.9/site-packages/torch/cuda/__init__.py?line=207'>208</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///home/hanzopgp/miniconda3/lib/python3.9/site-packages/torch/cuda/__init__.py?line=208'>209</a>\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/hanzopgp/miniconda3/lib/python3.9/site-packages/torch/cuda/__init__.py?line=209'>210</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/hanzopgp/miniconda3/lib/python3.9/site-packages/torch/cuda/__init__.py?line=210'>211</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n","\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"]}],"source":["TB_PATH = \"/tmp/logs/sceance2\"\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(torch.cuda.is_available())\n","torch.zeros(1).cuda()\n","\n","## Loading vgg16 model pretrained on imagenet\n","vgg = vgg16(pretrained=True)\n","\n","vgg.classifier = nn.Sequential(nn.Linear(25088, 128), \n","                               nn.ReLU(), \n","                               nn.Dropout(0.5),        \n","                               nn.Linear(128, 64), \n","                               nn.ReLU(), \n","#                                nn.Dropout(0.5),        \n","                               nn.Linear(64, 32),\n","                               nn.ReLU(), \n","#                                nn.Dropout(0.5),        \n","                               nn.Linear(32, N_CLASS),\n","                               nn.Sigmoid())\n","\n","print(vgg.eval())\n","\n","## Sets all the requires grad of the classifier layers to True\n","set_parameter_requires_grad(vgg, True)"]},{"cell_type":"markdown","metadata":{},"source":["# Implementing early stopping"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T11:58:25.949868Z","iopub.status.busy":"2022-04-18T11:58:25.949095Z","iopub.status.idle":"2022-04-18T11:58:25.963637Z","shell.execute_reply":"2022-04-18T11:58:25.962806Z","shell.execute_reply.started":"2022-04-18T11:58:25.949814Z"},"trusted":true},"outputs":[],"source":["class EarlyStopping(object):\n","    def __init__(self, mode='min', min_delta=0, patience=10, percentage=False):\n","        self.mode = mode\n","        self.min_delta = min_delta\n","        self.patience = patience\n","        self.best = None\n","        self.num_bad_epochs = 0\n","        self.is_better = None\n","        self._init_is_better(mode, min_delta, percentage)\n","        if patience == 0:\n","            self.is_better = lambda a, b: True\n","            self.step = lambda a: False\n","\n","    def step(self, metrics):\n","        if self.best is None:\n","            self.best = metrics\n","            return False\n","        if np.isnan(metrics):\n","            return True\n","        if self.is_better(metrics, self.best):\n","            self.num_bad_epochs = 0\n","            self.best = metrics\n","        else:\n","            self.num_bad_epochs += 1\n","        if self.num_bad_epochs >= self.patience:\n","            return True\n","        return False\n","\n","    def _init_is_better(self, mode, min_delta, percentage):\n","        if mode not in {'min', 'max'}:\n","            raise ValueError('mode ' + mode + ' is unknown!')\n","        if not percentage:\n","            if mode == 'min':\n","                self.is_better = lambda a, best: a < best - min_delta\n","            if mode == 'max':\n","                self.is_better = lambda a, best: a > best + min_delta\n","        else:\n","            if mode == 'min':\n","                self.is_better = lambda a, best: a < best - (\n","                            best * min_delta / 100)\n","            if mode == 'max':\n","                self.is_better = lambda a, best: a > best + (\n","                            best * min_delta / 100)"]},{"cell_type":"markdown","metadata":{},"source":["# Training only the modified parts of the classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T11:58:25.966171Z","iopub.status.busy":"2022-04-18T11:58:25.965782Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["params to learn:\n","\t classifier.0.weight\n","\t classifier.0.bias\n","\t classifier.3.weight\n","\t classifier.3.bias\n","\t classifier.5.weight\n","\t classifier.5.bias\n","\t classifier.7.weight\n","\t classifier.7.bias\n","running VGG\n"]},{"name":"stderr","output_type":"stream","text":[" 24%|██▎       | 4/17 [02:30<08:09, 37.68s/it]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32mc:\\Users\\karna\\Desktop\\Projects\\PythonProject\\JetsonAutonomousDriving\\src\\main\\only_bounding_box_model\\vgg-bounding-box.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/karna/Desktop/Projects/PythonProject/JetsonAutonomousDriving/src/main/only_bounding_box_model/vgg-bounding-box.ipynb#ch0000017?line=0'>1</a>\u001b[0m \u001b[39m## Fine-tuning the model on our data\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/karna/Desktop/Projects/PythonProject/JetsonAutonomousDriving/src/main/only_bounding_box_model/vgg-bounding-box.ipynb#ch0000017?line=1'>2</a>\u001b[0m vgg\u001b[39m.\u001b[39mname \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mVGG\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/karna/Desktop/Projects/PythonProject/JetsonAutonomousDriving/src/main/only_bounding_box_model/vgg-bounding-box.ipynb#ch0000017?line=3'>4</a>\u001b[0m best_model \u001b[39m=\u001b[39m train(model\u001b[39m=\u001b[39;49mvgg, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/karna/Desktop/Projects/PythonProject/JetsonAutonomousDriving/src/main/only_bounding_box_model/vgg-bounding-box.ipynb#ch0000017?line=4'>5</a>\u001b[0m                    epochs\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/karna/Desktop/Projects/PythonProject/JetsonAutonomousDriving/src/main/only_bounding_box_model/vgg-bounding-box.ipynb#ch0000017?line=5'>6</a>\u001b[0m                    train_loader\u001b[39m=\u001b[39;49mVGG_trainloader, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/karna/Desktop/Projects/PythonProject/JetsonAutonomousDriving/src/main/only_bounding_box_model/vgg-bounding-box.ipynb#ch0000017?line=6'>7</a>\u001b[0m                    valid_loader\u001b[39m=\u001b[39;49mVGG_validloader, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/karna/Desktop/Projects/PythonProject/JetsonAutonomousDriving/src/main/only_bounding_box_model/vgg-bounding-box.ipynb#ch0000017?line=7'>8</a>\u001b[0m                    learning_rate\u001b[39m=\u001b[39;49m\u001b[39m3e-4\u001b[39;49m, \u001b[39m## learning rate for Adam optimizer\u001b[39;49;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/karna/Desktop/Projects/PythonProject/JetsonAutonomousDriving/src/main/only_bounding_box_model/vgg-bounding-box.ipynb#ch0000017?line=8'>9</a>\u001b[0m                    patience\u001b[39m=\u001b[39;49m\u001b[39m15\u001b[39;49m)\n","\u001b[1;32mc:\\Users\\karna\\Desktop\\Projects\\PythonProject\\JetsonAutonomousDriving\\src\\main\\only_bounding_box_model\\vgg-bounding-box.ipynb Cell 12'\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, epochs, train_loader, valid_loader, learning_rate, patience, feature_extract)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/karna/Desktop/Projects/PythonProject/JetsonAutonomousDriving/src/main/only_bounding_box_model/vgg-bounding-box.ipynb#ch0000011?line=42'>43</a>\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/karna/Desktop/Projects/PythonProject/JetsonAutonomousDriving/src/main/only_bounding_box_model/vgg-bounding-box.ipynb#ch0000011?line=43'>44</a>\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/karna/Desktop/Projects/PythonProject/JetsonAutonomousDriving/src/main/only_bounding_box_model/vgg-bounding-box.ipynb#ch0000011?line=45'>46</a>\u001b[0m yhat \u001b[39m=\u001b[39m model(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/karna/Desktop/Projects/PythonProject/JetsonAutonomousDriving/src/main/only_bounding_box_model/vgg-bounding-box.ipynb#ch0000011?line=47'>48</a>\u001b[0m l \u001b[39m=\u001b[39m loss(yhat, y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/karna/Desktop/Projects/PythonProject/JetsonAutonomousDriving/src/main/only_bounding_box_model/vgg-bounding-box.ipynb#ch0000011?line=48'>49</a>\u001b[0m l\u001b[39m.\u001b[39mbackward()\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/karna/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/karna/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/karna/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/karna/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/karna/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/karna/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/karna/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\vgg.py:66\u001b[0m, in \u001b[0;36mVGG.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/karna/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torchvision/models/vgg.py?line=64'>65</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m---> <a href='file:///c%3A/Users/karna/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torchvision/models/vgg.py?line=65'>66</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures(x)\n\u001b[0;32m     <a href='file:///c%3A/Users/karna/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torchvision/models/vgg.py?line=66'>67</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool(x)\n\u001b[0;32m     <a href='file:///c%3A/Users/karna/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torchvision/models/vgg.py?line=67'>68</a>\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(x, \u001b[39m1\u001b[39m)\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/karna/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/karna/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/karna/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/karna/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/karna/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/karna/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/karna/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/karna/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/container.py?line=138'>139</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/karna/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/container.py?line=139'>140</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/karna/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/container.py?line=140'>141</a>\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    <a href='file:///c%3A/Users/karna/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/container.py?line=141'>142</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/karna/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/karna/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/karna/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/karna/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/karna/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/karna/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/karna/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\conv.py:447\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/karna/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/conv.py?line=445'>446</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> <a href='file:///c%3A/Users/karna/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/conv.py?line=446'>447</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\conv.py:443\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/karna/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/conv.py?line=438'>439</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    <a href='file:///c%3A/Users/karna/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/conv.py?line=439'>440</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    <a href='file:///c%3A/Users/karna/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/conv.py?line=440'>441</a>\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    <a href='file:///c%3A/Users/karna/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/conv.py?line=441'>442</a>\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> <a href='file:///c%3A/Users/karna/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/conv.py?line=442'>443</a>\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    <a href='file:///c%3A/Users/karna/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/torch/nn/modules/conv.py?line=443'>444</a>\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["## Fine-tuning the model on our data\n","vgg.name = \"VGG\"\n","\n","best_model = train(model=vgg, \n","                   epochs=200, \n","                   train_loader=VGG_trainloader, \n","                   valid_loader=VGG_validloader, \n","                   learning_rate=3e-4, ## learning rate for Adam optimizer\n","                   patience=15) ## metric for earlystopping : val_loss "]},{"cell_type":"markdown","metadata":{},"source":["# Checking predictions"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def draw_predictions(image, preds):\n","    startX, startY, endX, endY = preds\n","    # scale the predicted bounding box coordinates based on the image\n","    # dimensions    \n","    startX = int(startX * INPUT_SIZE)\n","    startY = int(startY * INPUT_SIZE)\n","    endX = int(endX * INPUT_SIZE)\n","    endY = int(endY * INPUT_SIZE)\n","#     print(startX, startY, endX, endY)\n","    # draw the predicted bounding box on the image\n","    image = image.numpy().copy()\n","    cv2.rectangle(image, (startX, startY), (endX, endY), (0, 255, 0), 2)\n","    # show the output image\n","    plt.imshow(image)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["with torch.no_grad():\n","    for item in VGG_validloader:\n","        x, y = item[\"image\"], item[\"label\"]\n","        base_img = item[\"image\"]\n","        x = item[\"image\"].to(device)\n","        res = []\n","        for e in y:\n","            res.append(np.array(e))\n","        res = np.array(res).T\n","        y = torch.as_tensor(res)\n","        y = y.to(torch.float32)\n","        y = y.to(device)\n","        yhat = best_model(x)\n","        for i in range(10):\n","            draw_predictions(base_img[i].permute(1,2,0), yhat.cpu()[i])"]},{"cell_type":"markdown","metadata":{},"source":["# Saving the model in .pth and .onnx extension"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["PATH = \"./\"\n","torch.save(best_model.state_dict(), os.path.join(PATH,\"boundingbox_vgg.pth\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# del vgg\n","# del best_model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# model = models.vgg16(pretrained=True)\n","# model.classifier[0] = nn.Linear(25088, 8192)\n","# model.classifier[3] = nn.Linear(8192, 1024)\n","# model.classifier[6] = nn.Linear(1024, N_CLASS)\n","# model.load_state_dict(torch.load(os.path.join(PATH,\"vgg.pth\"), map_location='cpu'))\n","# model.eval() \n","\n","# dummy_input = torch.randn(BATCH_SIZE, 3, INPUT_SIZE, INPUT_SIZE)  \n","# torch.onnx.export(model,   \n","#                   dummy_input, \n","#                   \"vgg.onnx\",\n","#                   export_params=True,\n","#                   do_constant_folding=True, \n","#                   input_names = ['modelInput'],\n","#                   output_names = ['modelOutput'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":4}
