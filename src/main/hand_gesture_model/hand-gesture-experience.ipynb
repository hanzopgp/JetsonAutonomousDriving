{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"# %matplotlib inline\n\nimport time\nimport os\nimport cv2\nimport random\nimport imageio\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport pandas as pd\n\nfrom tqdm import tqdm\nfrom io import BytesIO\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Global variables","metadata":{}},{"cell_type":"code","source":"OS = \"linux\"\nTPU = True\nvalid_size = 0.2\nvideo_path = \"../input/hand-gesture/data_background/test.mkv\" ## On kaggle\n# video_path = \"data_background/test.mkv\" ## On computer\n# label_name = [\"palm_horizontal\", \"L\", \"fist_horizontal\", \"fist_vertical\", \"thumb_up\", \"index\", \"ok\", \"palm_vertical\", \"C\", \"thumb_down\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if TPU:\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Device:', tpu.master())\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    except:\n        strategy = tf.distribute.get_strategy()\n    print('Number of replicas:', strategy.num_replicas_in_sync)\n\n    AUTOTUNE = tf.data.experimental.AUTOTUNE\n    BATCH_SIZE = 16 * strategy.num_replicas_in_sync\nelse:\n    BATCH_SIZE = 128","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading data","metadata":{}},{"cell_type":"code","source":"def load_labelling_data():\n    X = []\n    y = []\n    if OS == \"windows\":\n        split_ = \"\\\\\"\n    else:\n        split_ = \"/\"\n    for root, _, files in tqdm(os.walk(\"../\", topdown=False)): \n        for name in files:\n            path = os.path.join(root, name)\n            if path.endswith(\"jpg\"):\n                # if path.split(split_)[-1][0] != \".\":\n                if path.split(split_)[-1][0].isalpha():\n                    # Loading images\n                    img = cv2.imread(path)\n                    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n                    X.append(img)\n                    # Loading labels\n                    category = path.split(split_)[-1].split(\"_\")[0]\n                    # label = int(category.split(\"_\")[0]) - 1\n                    y.append(category)\n    X = np.array(X)\n    y = np.array(y)\n    return X[:,:,:,np.newaxis], y\n\ndef load_test_data(width, height):\n    X = []\n    if OS == \"windows\":\n        split_ = \"\\\\\"\n    else:\n        split_ = \"/\"\n    for root, _, files in tqdm(os.walk(\"../\", topdown=False)): \n        for name in files:\n            path = os.path.join(root, name)\n            if path.endswith(\"jpg\"):\n                # if path.split(split_)[-1][0] != \".\":\n                if not path.split(split_)[-1][0].isalpha() and path.split(split_)[-1][0] != \".\":\n                    # Loading images\n                    img = cv2.imread(path)\n                    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n                    img = cv2.resize(img, (width, height))\n                    X.append(img)\n    X = np.array(X)\n    return X.reshape(X.shape[0], height, width, 1)\n\ndef get_background_images():\n    images = []\n    vidcap = cv2.VideoCapture(video_path)\n    success,image = vidcap.read()\n    while success:\n        img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)[:,:,np.newaxis]\n        images.append(img)\n        success, image = vidcap.read()\n    return np.array(images)\n\n# def load_dataset_data(width, height):\n#     X = []\n#     y = []\n#     if OS == \"windows\":\n#         split_ = \"\\\\\"\n#     else:\n#         split_ = \"/\"\n#     for root, dirs, files in tqdm(os.walk(\".\", topdown=False)): \n#         for name in files:\n#             path = os.path.join(root, name)\n#             if path.endswith(\"png\"):\n#                 # Loading labels\n#                 category = path.split(split_)[4]\n#                 label = int(category.split(\"_\")[0]) - 1\n#                 y.append(label)\n#                 # Loading images\n#                 img = cv2.imread(path)\n#                 img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n#                 img = cv2.resize(img, (width, height))\n#                 X.append(img)\n#     X = np.array(X)\n#     y = np.array(y)\n#     return X.reshape(X.shape[0], height, width, 1), y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_labelling, y_labelling = load_labelling_data()\nbackground_images = get_background_images()[::18]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_labelling.shape)\nprint(y_labelling.shape)\nprint(background_images.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data augmentation","metadata":{}},{"cell_type":"code","source":"plt.imshow(X_labelling[0])\nplt.show()\nplt.imshow(background_images[0])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fill(img, h, w):\n    img = cv2.resize(img, (h, w), cv2.INTER_CUBIC)\n    return img\n        \ndef horizontal_shift(img, ratio=0.0):\n    if ratio > 1 or ratio < 0:\n        return img\n    ratio = random.uniform(-ratio, ratio)\n    h, w = img.shape[:2]\n    to_shift = w*ratio\n    if ratio > 0:\n        img = img[:, :int(w-to_shift), :]\n    if ratio < 0:\n        img = img[:, int(-1*to_shift):, :]\n    img = fill(img, h, w)\n    return img\n\ndef vertical_shift(img, ratio=0.0):\n    if ratio > 1 or ratio < 0:\n        return img\n    ratio = random.uniform(-ratio, ratio)\n    h, w = img.shape[:2]\n    to_shift = h*ratio\n    if ratio > 0:\n        img = img[:int(h-to_shift), :, :]\n    if ratio < 0:\n        img = img[int(-1*to_shift):, :, :]\n    img = fill(img, h, w)\n    return img\n\ndef brightness(img, low, high):\n    value = random.uniform(low, high)\n    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n    hsv = np.array(hsv, dtype = np.float64)\n    hsv[:,:,1] = hsv[:,:,1]*value\n    hsv[:,:,1][hsv[:,:,1]>255]  = 255\n    hsv[:,:,2] = hsv[:,:,2]*value \n    hsv[:,:,2][hsv[:,:,2]>255]  = 255\n    hsv = np.array(hsv, dtype = np.uint8)\n    img = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n    return img\n\ndef zoom(img, value):\n    if value > 1 or value < 0:\n        return img\n    value = random.uniform(value, 1)\n    h, w = img.shape[:2]\n    h_taken = int(value*h)\n    w_taken = int(value*w)\n    h_start = random.randint(0, h-h_taken)\n    w_start = random.randint(0, w-w_taken)\n    img = img[h_start:h_start+h_taken, w_start:w_start+w_taken, :]\n    img = fill(img, h, w)\n    return img\n\ndef channel_shift(img, value):\n    value = int(random.uniform(-value, value))\n    img = img + value\n    img[:,:,:][img[:,:,:]>255]  = 255\n    img[:,:,:][img[:,:,:]<0]  = 0\n    img = img.astype(np.uint8)\n    return img.squeeze()\n\ndef horizontal_flip(img, flag):\n    if flag:\n        return cv2.flip(img, 1)\n    else:\n        return img\n\ndef vertical_flip(img, flag):\n    if flag:\n        return cv2.flip(img, 0)\n    else:\n        return img\n\ndef rotation(img, angle):\n    angle = int(random.uniform(-angle, angle))\n    h, w = img.shape[:2]\n    M = cv2.getRotationMatrix2D((int(w/2), int(h/2)), angle, 1)\n    img = cv2.warpAffine(img, M, (w, h))\n    return img\n\ndef get_augmented_images(X, y):\n    X_augmented = []\n    y_augmented = []\n    for i in range(X.shape[0]):\n        X_augmented.append(X[i].squeeze())\n        y_augmented.append(y[i])\n#         X_augmented.append(horizontal_shift(X[i], 0.9))\n#         y_augmented.append(y[i])\n#         X_augmented.append(vertical_shift(X[i], 0.9))\n#         y_augmented.append(y[i])\n        # X_augmented.append(brightness(X[i], 0.5, 3))\n        # y_augmented.append(y[i])\n        X_augmented.append(zoom(X[i], 0.5))\n        y_augmented.append(y[i])\n        X_augmented.append(channel_shift(X[i], 60))\n        y_augmented.append(y[i])\n        # X_augmented.append(horizontal_flip(X[i], True))\n        # y_augmented.append(y[i])\n        # X_augmented.append(vertical_flip(X[i], True))\n        # y_augmented.append(y[i])\n        X_augmented.append(rotation(X[i], 20))\n        y_augmented.append(y[i])\n        X_augmented.append(rotation(X[i], 40))\n        y_augmented.append(y[i])\n#         X_augmented.append(rotation(X[i], 60))\n#         y_augmented.append(y[i])\n        X_augmented.append(rotation(X[i], -20))\n        y_augmented.append(y[i])\n        X_augmented.append(rotation(X[i], -40))\n        y_augmented.append(y[i])\n#         X_augmented.append(rotation(X[i], -60))\n#         y_augmented.append(y[i])\n    return np.array(X_augmented), np.array(y_augmented)\n\ndef add_background_to_data(background_images, hand_images, labels):\n    final_images = []\n    new_labels = []\n    for idx, hand_img in enumerate(tqdm(hand_images)):\n        for background_img in background_images:\n            for r in range(1, 4):\n                resize = int(hand_img.shape[0]*r)\n                hand_img_resized = cv2.resize(hand_img, (resize, resize))[:,:,np.newaxis]\n                y_offset = np.random.randint(background_img.shape[0] - hand_img_resized.shape[0])\n                x_offset = np.random.randint(background_img.shape[1] - hand_img_resized.shape[1])\n                tmp = background_img.copy()\n                tmp[y_offset:y_offset+hand_img_resized.shape[0], x_offset:x_offset+hand_img_resized.shape[1]] = hand_img_resized\n                tmp = cv2.resize(tmp, (128, 128))[:,:,np.newaxis]\n                final_images.append(tmp)\n                new_labels.append(labels[idx])\n    return np.array(final_images), np.array(new_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_labelling_augmented, y_labelling_augmented = get_augmented_images(X_labelling, y_labelling)\nprint(X_labelling_augmented.shape)\nprint(y_labelling_augmented.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X, y = add_background_to_data(background_images, X_labelling_augmented, y_labelling_augmented)\nprint(X.shape)\nprint(y.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(X[1000])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_names, y = np.unique(y, return_inverse=True) ## Converts to categorical int and get label names\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=valid_size, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = X_train[::3], X_valid[::3], y_train[::3], y_valid[::3]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building model","metadata":{}},{"cell_type":"code","source":"def build_model():\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.Conv2D(32, (5, 5), activation='relu', input_shape=(X_train[0].shape))) \n    model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n#     model.add(tf.keras.layers.Dropout(rate=0.2))\n    model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu')) \n    model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n    model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n    model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n    model.add(tf.keras.layers.Dropout(rate=0.2))\n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(128, activation='relu'))\n    model.add(tf.keras.layers.Dense(len(label_names), activation='softmax'))\n    return model\n\ndef get_callbacks():\n    early = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                                             min_delta=0,\n                                             patience=15,\n                                             verbose=2,\n                                             mode='auto',\n                                             baseline=None,\n                                             restore_best_weights=True\n                                             )\n#     reduce = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n#                                                   factor=0.1,\n#                                                   patience=15,\n#                                                   verbose=2,\n#                                                   mode='auto',\n#                                                   min_delta=0.0001,\n#                                                   cooldown=0,\n#                                                   min_lr=0\n#                                                  )\n#     return [early, reduce]\n    return [early]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training model","metadata":{}},{"cell_type":"code","source":"if TPU :\n    with strategy.scope():       \n        model = build_model()\n        model.compile(optimizer='adam',\n                      loss='sparse_categorical_crossentropy',\n                      metrics=['accuracy'])\n        model.summary()\nelse :\n    model = build_model()\n    model.compile(optimizer='adam',\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n    model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(X_train, \n                    y_train, \n                    epochs=200, \n                    batch_size=BATCH_SIZE, \n                    verbose=2, \n                    validation_data=(X_valid, y_valid), \n                    callbacks=get_callbacks())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('tmp_model.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'valid'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'valid'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"X_test = load_test_data(128, 128)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(X_test[0])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = np.argmax(model.predict(X_test), axis=1)\nlabel_names","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(preds)):\n    print(\"=======================================\")\n    plt.imshow(X_test[i])\n    plt.show()\n    print(label_names[preds[i]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inference_ = False\n\nif inference_ :\n    cv2.destroyAllWindows()\n    before = time.time()\n    vid = cv2.VideoCapture(1)\n    while True:\n\n        ret, frame = vid.read()\n        cv2.imshow('frame', frame)\n        if cv2.waitKey(1) & 0xFF == ord('q'):\n            break\n        img = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        img = cv2.resize(img, (128, 128))\n        img = img.reshape(1, 128, 128, 1)\n\n        after = int(time.time() - before)\n        print(label_names[np.argmax(model.predict(img))], end='\\r')\n\n    vid.release()\n    cv2.destroyAllWindows()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}