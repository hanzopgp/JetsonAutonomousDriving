{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importation","metadata":{"execution":{"iopub.status.busy":"2022-04-11T16:15:07.160836Z","iopub.execute_input":"2022-04-11T16:15:07.161381Z","iopub.status.idle":"2022-04-11T16:15:08.995876Z","shell.execute_reply.started":"2022-04-11T16:15:07.161292Z","shell.execute_reply":"2022-04-11T16:15:08.995057Z"}}},{"cell_type":"code","source":"import os\nimport copy\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom time import time\nfrom sklearn import preprocessing\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import f1_score \nfrom matplotlib import pyplot as plt\n\nimport torchvision\nfrom torchvision import models, transforms\nfrom torchvision.io import read_image\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.utils.data import Dataset","metadata":{"execution":{"iopub.status.busy":"2022-04-17T15:57:26.786099Z","iopub.execute_input":"2022-04-17T15:57:26.786612Z","iopub.status.idle":"2022-04-17T15:57:29.420773Z","shell.execute_reply.started":"2022-04-17T15:57:26.786525Z","shell.execute_reply":"2022-04-17T15:57:29.419576Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Global variables ","metadata":{}},{"cell_type":"code","source":"INPUT_SIZE = 100\nBATCH_SIZE = 256\nN_CLASS = 5\n\nPATH_LABELS = \"../input/classifonlyhanddataset/index_label.csv\"\nPATH_IMG = \"../input/classifonlyhanddataset/output/output\"\n\nPATH_LABELS_VALID = \"../input/classifonlyhanddataset/index_label_validation.csv\"\nPATH_IMG_VALID = \"../input/classifonlyhanddataset/output_validation/output_validation\"","metadata":{"execution":{"iopub.status.busy":"2022-04-17T15:57:29.424688Z","iopub.execute_input":"2022-04-17T15:57:29.424895Z","iopub.status.idle":"2022-04-17T15:57:29.428782Z","shell.execute_reply.started":"2022-04-17T15:57:29.424871Z","shell.execute_reply":"2022-04-17T15:57:29.428150Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Data functions","metadata":{"execution":{"iopub.status.busy":"2022-04-11T16:20:53.56609Z","iopub.execute_input":"2022-04-11T16:20:53.566375Z","iopub.status.idle":"2022-04-11T16:20:53.569977Z","shell.execute_reply.started":"2022-04-11T16:20:53.566327Z","shell.execute_reply":"2022-04-11T16:20:53.569332Z"}}},{"cell_type":"code","source":"class HandGestureDataset(Dataset):\n    def __init__(self, annotations_file, img_dir, transform=None):\n        self.img_labels = pd.read_csv(annotations_file)\n        self.img_dir = img_dir\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.img_labels)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, os.listdir(self.img_dir)[idx])\n        image = read_image(img_path)\n        label = self.img_labels.loc[self.img_labels[\"index\"] == str(\"output/\"+os.listdir(self.img_dir)[idx])][\"label\"].item()\n        if self.transform:\n            image = self.transform(image)\n#         image = F.normalize(image, dim = 0)\n        return image, label","metadata":{"execution":{"iopub.status.busy":"2022-04-17T15:57:29.429936Z","iopub.execute_input":"2022-04-17T15:57:29.430331Z","iopub.status.idle":"2022-04-17T15:57:29.441454Z","shell.execute_reply.started":"2022-04-17T15:57:29.430295Z","shell.execute_reply":"2022-04-17T15:57:29.440664Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def prepare_data_vgg(data_type):\n    ## Parameters fitting vgg/imagenet\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n\n    ## pytorch transformer objects\n    transformVGGTrain=transforms.Compose([\n            transforms.ToPILImage(),\n        \n            transforms.ColorJitter(brightness=0.5, hue=0.5),\n            transforms.RandomPerspective(distortion_scale=0.5, p=0.2),\n            transforms.RandomAffine(degrees=(0, 5), translate=(0, 0.18), scale=(0.7, 1)),\n#             transforms.RandomSolarize(threshold=192.0),\n            transforms.RandomAdjustSharpness(sharpness_factor=3),\n            transforms.RandomAutocontrast(),\n            transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.01, 2)),\n            transforms.RandomRotation(degrees=(-30, 30)),\n        \n            ## too much\n#             transforms.RandomPosterize(bits=2),\n#             transforms.RandomInvert(),\n#             transforms.RandomEqualize(),\n        \n            transforms.Resize(size=(INPUT_SIZE, INPUT_SIZE)),\n            transforms.ToTensor(),\n#             transforms.Normalize(mean, std) ## test with and without\n        ])\n    transformVGGValid=transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize(size=(INPUT_SIZE, INPUT_SIZE)),\n            transforms.ToTensor(),\n#             transforms.Normalize(mean, std) ## test with and without\n        ])\n\n    if data_type == \"custom\":\n        ## Custom dataset\n        VGG_dataset_train = HandGestureDataset(PATH_LABELS, PATH_IMG, transformVGGTrain)\n        VGG_dataset_valid = HandGestureDataset(PATH_LABELS_VALID, PATH_IMG_VALID, transformVGGValid)\n        VGG_trainloader = torch.utils.data.DataLoader(VGG_dataset_train, batch_size=BATCH_SIZE, pin_memory=True, shuffle=True)\n        VGG_validloader = torch.utils.data.DataLoader(VGG_dataset_valid, batch_size=BATCH_SIZE, pin_memory=True, shuffle=True)\n\n    return VGG_trainloader, VGG_validloader","metadata":{"execution":{"iopub.status.busy":"2022-04-17T15:57:29.443619Z","iopub.execute_input":"2022-04-17T15:57:29.444005Z","iopub.status.idle":"2022-04-17T15:57:29.455639Z","shell.execute_reply.started":"2022-04-17T15:57:29.443968Z","shell.execute_reply":"2022-04-17T15:57:29.454936Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Loading data into pytorch dataset and dataloader objects","metadata":{"execution":{"iopub.status.busy":"2022-02-15T15:49:41.69497Z","iopub.execute_input":"2022-02-15T15:49:41.695334Z","iopub.status.idle":"2022-02-15T15:49:46.639507Z","shell.execute_reply.started":"2022-02-15T15:49:41.695293Z","shell.execute_reply":"2022-02-15T15:49:46.638679Z"}}},{"cell_type":"code","source":"VGG_trainloader, VGG_validloader = prepare_data_vgg(\"custom\")","metadata":{"execution":{"iopub.status.busy":"2022-04-17T15:57:29.457665Z","iopub.execute_input":"2022-04-17T15:57:29.458530Z","iopub.status.idle":"2022-04-17T15:57:29.482888Z","shell.execute_reply.started":"2022-04-17T15:57:29.458491Z","shell.execute_reply":"2022-04-17T15:57:29.482294Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# for img in next(iter(VGG_trainloader)):\n#     for i in img[:20]:\n#         i = i.permute(1,2,0)\n#         plt.imshow(np.array(i))\n#         plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T15:57:29.484311Z","iopub.execute_input":"2022-04-17T15:57:29.484478Z","iopub.status.idle":"2022-04-17T15:57:29.491168Z","shell.execute_reply.started":"2022-04-17T15:57:29.484457Z","shell.execute_reply":"2022-04-17T15:57:29.490231Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Model functions","metadata":{}},{"cell_type":"code","source":"def accuracy(yhat,y):\n    if len(y.shape) == 1 or y.size(1) == 1:\n        return (torch.argmax(yhat, 1).view(y.size(0), -1) == y.view(-1, 1)).double().mean()\n    return (torch.argmax(yhat, 1). view(-1) == torch.argmax(y, 1).view(-1)).double().mean()\n\ndef train(model, epochs, train_loader, valid_loader, learning_rate, patience, label_encoder, feature_extract=False):\n    ## Early stopping variables\n    es = EarlyStopping(patience=patience)\n    terminate_training = False\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    model = model.to(device)\n    ## Training only the parameters where we require gradient since we are fine-tuning\n    params_to_update = model.parameters()\n    print(\"params to learn:\")\n    if feature_extract:\n        params_to_update = []\n        for name,param in model.named_parameters():\n            if param.requires_grad == True:\n                params_to_update.append(param)\n                print(\"\\t\", name)\n    else:\n        for name,param in model.named_parameters():\n            if param.requires_grad == True:\n                print(\"\\t\", name)\n                \n    ## Setting up our optimizer\n    optim = torch.optim.Adam(params_to_update, lr=learning_rate)\n    \n    ## Setting up our loss function\n    loss = nn.CrossEntropyLoss()\n    \n    ## Running the train loop\n    print(f\"running {model.name}\")\n    for epoch in range(epochs):\n        cumloss, cumacc, count = 0, 0, 0\n        model.train()\n        for x,y in train_loader:\n            optim.zero_grad()\n            x = x.to(device)\n            y = label_encoder.fit_transform(y)\n            y = torch.as_tensor(y)\n            y = y.to(device)\n            yhat = model(x)\n            l = loss(yhat, y)\n            l.backward()\n            optim.step()\n            cumloss += l * len(x)\n            cumacc += accuracy(yhat, y) * len(x)\n            count += len(x)\n        print(\"epoch :\", epoch, end=\"\")\n        print(\", train_loss: \", cumloss.cpu().item()/count, end=\"\")\n        print(\", train_acc: \", cumacc.cpu().item()/count, end=\"\")\n        if epoch % 1 == 0:\n            model.eval()\n            with torch.no_grad():\n                valid_cumloss, valid_cumacc, count = 0, 0, 0\n                for x,y in valid_loader:\n                    x = x.to(device)\n                    y = label_encoder.fit_transform(y)\n                    y = torch.as_tensor(y)\n                    y = y.to(device)\n                    yhat = model(x)\n                    valid_cumloss += loss(yhat,y) * len(x)\n                    valid_cumacc += accuracy(yhat,y) * len(x)\n                    count += len(x)\n                print(\", valid_loss: \", valid_cumloss.cpu().item()/count, end=\"\")\n                print(\", valid_acc: \", valid_cumacc.cpu().item()/count)  \n#                 print(\", valid_f1_score:\", f1_score(y.data, yhat))\n#                 print(y.cpu())\n#                 print(np.argmax(yhat.cpu(), axis=1))\n#                 print(classification_report(y.cpu(), np.argmax(yhat.cpu(), axis=1)))\n                ## Early stopping\n                if valid_cumacc/count > best_acc:\n                    best_acc = valid_cumacc/count\n                    best_model_wts = copy.deepcopy(model.state_dict())\n                if es.step(valid_cumloss.cpu().item()/count):\n                    terminate_training = True\n                    break\n        if terminate_training:\n            break\n    print('Best val Acc: {:4f}'.format(best_acc))\n    ## Returns the best model\n    model.load_state_dict(best_model_wts)\n    return model\n\ndef set_parameter_requires_grad(model, feature_extract):\n    if feature_extract:\n        for name,p in model.named_parameters():\n            if \"features\" in name:\n                p.requires_grad = False    \n            else:\n                p.requires_grad = True  ","metadata":{"execution":{"iopub.status.busy":"2022-04-17T15:57:29.494351Z","iopub.execute_input":"2022-04-17T15:57:29.494538Z","iopub.status.idle":"2022-04-17T15:57:29.539190Z","shell.execute_reply.started":"2022-04-17T15:57:29.494511Z","shell.execute_reply":"2022-04-17T15:57:29.538361Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Loading the model and modifying the classifier part\n### Maybe we could try to modify only the last classifier layer ?","metadata":{"execution":{"iopub.status.busy":"2022-04-11T16:22:20.614632Z","iopub.execute_input":"2022-04-11T16:22:20.614896Z","iopub.status.idle":"2022-04-11T16:22:20.618197Z","shell.execute_reply.started":"2022-04-11T16:22:20.614868Z","shell.execute_reply":"2022-04-11T16:22:20.617386Z"}}},{"cell_type":"code","source":"TB_PATH = \"/tmp/logs/sceance2\"\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n## Loading vgg16 model pretrained on imagenet\nvgg = models.vgg16(pretrained=True)\n\n## Modifies the vgg network classifier layers to fit our problem\n# vgg.classifier[0] = nn.Linear(25088, 8192)\n# vgg.classifier[3] = nn.Linear(8192, 1024)\n# vgg.classifier[6] = nn.Linear(1024, N_CLASS)\n\n# vgg.classifier = nn.Sequential(nn.Linear(25088, 512), # test 2048\n#                                nn.ReLU(), \n#                                nn.Dropout(0.45),       # test 0.5\n#                                nn.Linear(512, 100),\n#                                nn.ReLU(), \n#                                nn.Dropout(0.45),\n#                                nn.Linear(100, N_CLASS),                   \n#                                nn.Sigmoid())\n\nvgg.classifier = nn.Sequential(nn.Linear(25088, 100),\n                               nn.ReLU(), \n                               nn.Dropout(0.45),        \n                               nn.Linear(100, N_CLASS), \n                               nn.Softmax(dim=1)) \n\nprint(vgg.eval())\n\n## Sets all the requires grad of the classifier layers to True\nset_parameter_requires_grad(vgg, True)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T15:57:29.544352Z","iopub.execute_input":"2022-04-17T15:57:29.544902Z","iopub.status.idle":"2022-04-17T15:57:33.874392Z","shell.execute_reply.started":"2022-04-17T15:57:29.544868Z","shell.execute_reply":"2022-04-17T15:57:33.873586Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Implementing early stopping","metadata":{}},{"cell_type":"code","source":"class EarlyStopping(object):\n    def __init__(self, mode='min', min_delta=0, patience=10, percentage=False):\n        self.mode = mode\n        self.min_delta = min_delta\n        self.patience = patience\n        self.best = None\n        self.num_bad_epochs = 0\n        self.is_better = None\n        self._init_is_better(mode, min_delta, percentage)\n        if patience == 0:\n            self.is_better = lambda a, b: True\n            self.step = lambda a: False\n\n    def step(self, metrics):\n        if self.best is None:\n            self.best = metrics\n            return False\n        if np.isnan(metrics):\n            return True\n        if self.is_better(metrics, self.best):\n            self.num_bad_epochs = 0\n            self.best = metrics\n        else:\n            self.num_bad_epochs += 1\n        if self.num_bad_epochs >= self.patience:\n            return True\n        return False\n\n    def _init_is_better(self, mode, min_delta, percentage):\n        if mode not in {'min', 'max'}:\n            raise ValueError('mode ' + mode + ' is unknown!')\n        if not percentage:\n            if mode == 'min':\n                self.is_better = lambda a, best: a < best - min_delta\n            if mode == 'max':\n                self.is_better = lambda a, best: a > best + min_delta\n        else:\n            if mode == 'min':\n                self.is_better = lambda a, best: a < best - (\n                            best * min_delta / 100)\n            if mode == 'max':\n                self.is_better = lambda a, best: a > best + (\n                            best * min_delta / 100)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T15:57:33.875895Z","iopub.execute_input":"2022-04-17T15:57:33.876357Z","iopub.status.idle":"2022-04-17T15:57:33.889083Z","shell.execute_reply.started":"2022-04-17T15:57:33.876315Z","shell.execute_reply":"2022-04-17T15:57:33.888059Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Training only the modified parts of the classifier","metadata":{}},{"cell_type":"code","source":"## Fine-tuning the model on our data\nvgg.name = \"VGG\"\n\nle = preprocessing.LabelEncoder()\n\nbest_model = train(model=vgg, \n                   epochs=500, \n                   train_loader=VGG_trainloader, \n                   valid_loader=VGG_validloader, \n                   learning_rate=3e-4, ## learning rate for Adam optimizer\n                   patience=10, ## metric for earlystopping : val_loss\n                   label_encoder=le) ","metadata":{"execution":{"iopub.status.busy":"2022-04-17T15:57:33.893373Z","iopub.execute_input":"2022-04-17T15:57:33.893612Z","iopub.status.idle":"2022-04-17T16:58:25.797574Z","shell.execute_reply.started":"2022-04-17T15:57:33.893583Z","shell.execute_reply":"2022-04-17T16:58:25.796805Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Checking which classes are not correctly classified in valid set","metadata":{}},{"cell_type":"code","source":"yhats = []\nys = []\n\nwith torch.no_grad():\n    for x,y in VGG_validloader:\n        x = x.to(device)\n        y = le.fit_transform(y)\n        y = torch.as_tensor(y)\n        y = y.to(device)\n        yhat = best_model(x)\n        yhats.append(yhat.cpu())\n        ys.append(y.cpu())","metadata":{"execution":{"iopub.status.busy":"2022-04-17T16:58:25.799007Z","iopub.execute_input":"2022-04-17T16:58:25.799264Z","iopub.status.idle":"2022-04-17T16:58:33.158889Z","shell.execute_reply.started":"2022-04-17T16:58:25.799228Z","shell.execute_reply":"2022-04-17T16:58:33.158164Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"yhats = np.array(yhats)\nys = np.array(ys)\n\n\nfor yhat, y in zip(yhats, ys):\n    print(classification_report(y, yhat.argmax(axis=1)))","metadata":{"execution":{"iopub.status.busy":"2022-04-17T16:58:33.160265Z","iopub.execute_input":"2022-04-17T16:58:33.160532Z","iopub.status.idle":"2022-04-17T16:58:33.192515Z","shell.execute_reply.started":"2022-04-17T16:58:33.160497Z","shell.execute_reply":"2022-04-17T16:58:33.191788Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Checking which kind of image could be the problem","metadata":{}},{"cell_type":"code","source":"LABELS = ['FINGER', 'FIST', 'LEFT', 'PALM', 'RIGHT'] \n\nwith torch.no_grad():\n    for x,y in VGG_validloader:\n        x = x.to(device)\n        y = le.fit_transform(y)\n        y = torch.as_tensor(y)\n        y = y.to(device)\n        yhat = best_model(x)\n        for i in range(50):\n            plt.imshow(x.cpu()[i].permute(1,2,0))\n            str_ = LABELS[yhat.cpu()[i].argmax().item()]\n            plt.title(\"Pred:\" + str_)\n            plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T16:58:33.193969Z","iopub.execute_input":"2022-04-17T16:58:33.194439Z","iopub.status.idle":"2022-04-17T16:59:39.628195Z","shell.execute_reply.started":"2022-04-17T16:58:33.194404Z","shell.execute_reply":"2022-04-17T16:59:39.627471Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Saving the model in .pth and .onnx extension","metadata":{}},{"cell_type":"code","source":"PATH = \"./\"\ntorch.save(best_model.state_dict(), os.path.join(PATH,\"best_model.pth\"))","metadata":{"execution":{"iopub.status.busy":"2022-04-17T16:59:39.632013Z","iopub.execute_input":"2022-04-17T16:59:39.634235Z","iopub.status.idle":"2022-04-17T16:59:39.778908Z","shell.execute_reply.started":"2022-04-17T16:59:39.634195Z","shell.execute_reply":"2022-04-17T16:59:39.778182Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"del vgg\ndel best_model","metadata":{"execution":{"iopub.status.busy":"2022-04-17T16:59:39.783237Z","iopub.execute_input":"2022-04-17T16:59:39.785329Z","iopub.status.idle":"2022-04-17T16:59:39.790928Z","shell.execute_reply.started":"2022-04-17T16:59:39.785287Z","shell.execute_reply":"2022-04-17T16:59:39.790319Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# model = models.vgg16(pretrained=True)\n# model.classifier[0] = nn.Linear(25088, 8192)\n# model.classifier[3] = nn.Linear(8192, 1024)\n# model.classifier[6] = nn.Linear(1024, N_CLASS)\n# model.load_state_dict(torch.load(os.path.join(PATH,\"vgg.pth\"), map_location='cpu'))\n# model.eval() \n\n# dummy_input = torch.randn(BATCH_SIZE, 3, INPUT_SIZE, INPUT_SIZE)  \n# torch.onnx.export(model,   \n#                   dummy_input, \n#                   \"vgg.onnx\",\n#                   export_params=True,\n#                   do_constant_folding=True, \n#                   input_names = ['modelInput'],\n#                   output_names = ['modelOutput'])","metadata":{"execution":{"iopub.status.busy":"2022-04-17T16:59:39.792150Z","iopub.execute_input":"2022-04-17T16:59:39.792589Z","iopub.status.idle":"2022-04-17T16:59:39.803395Z","shell.execute_reply.started":"2022-04-17T16:59:39.792555Z","shell.execute_reply":"2022-04-17T16:59:39.802680Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}