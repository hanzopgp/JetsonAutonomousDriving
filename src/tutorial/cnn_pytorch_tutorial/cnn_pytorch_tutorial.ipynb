{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Module 2 : Réseaux convolutionnels pour le traitement de l'image  -  Formation Cdiscount\n\n## Mercredi 26 Mai 2021\n\nNicolas Baskiotis (nicolas.baskiotis@lip6.fr) Benjamin Piwowarski (benjamin.piwowarski@lip6.fr) -- MLIA/LIP6, Sorbonne Université","metadata":{}},{"cell_type":"markdown","source":"Les objectifs de ce module sont :\n* Prise en main des réseaux convolutionnels (CNN)\n* Apprentissage d'un CNN\n* Introspection d'un CNN\n* Fine-tuning d'un réseau pré-appris\n\nNous travaillerons dans un premier temps avec les données MNIST puis avec le jeu de données CIFAR d'images de 10 classes.","metadata":{}},{"cell_type":"code","source":"!pip install GPUtil\nfrom GPUtil import showUtilization as gpu_usage\nfrom numba import cuda\n# !pip install torch\n# !pip install torchvision\n# !pip install datamaestro\nfrom tqdm import tqdm\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch import optim\nimport torchvision\nfrom torchvision import transforms\nfrom torch.utils.tensorboard import SummaryWriter\nimport time\nimport os\nfrom tensorboard import notebook\n# from datamaestro import prepare_dataset\nfrom tensorboard import notebook\nfrom torch.utils.data import TensorDataset, DataLoader,Dataset\nimport matplotlib.pyplot as plt","metadata":{"id":"OQfQDwsPAGpM","execution":{"iopub.status.busy":"2022-01-30T14:04:06.400277Z","iopub.execute_input":"2022-01-30T14:04:06.400552Z","iopub.status.idle":"2022-01-30T14:04:14.671715Z","shell.execute_reply.started":"2022-01-30T14:04:06.400504Z","shell.execute_reply":"2022-01-30T14:04:14.670877Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"TB_PATH = \"/tmp/logs/sceance2\"\n# %load_ext tensorboard\n# %tensorboard --logdir /tmp/logs/sceance2\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-01-30T14:04:14.674247Z","iopub.execute_input":"2022-01-30T14:04:14.674551Z","iopub.status.idle":"2022-01-30T14:04:14.679020Z","shell.execute_reply.started":"2022-01-30T14:04:14.674499Z","shell.execute_reply":"2022-01-30T14:04:14.678335Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"markdown","source":"# Premier réseau convolutionnel\n\nNous allons reprendre les données MNIST dans un premier temps.","metadata":{"id":"y_6O63E325AP"}},{"cell_type":"code","source":"TRAIN_BATCH_SIZE = 256\nTEST_BATCH_SIZE = 512\n\n# Téléchargement des données\nfrom tensorflow.keras.datasets import mnist\n(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n# mnist_ds = prepare_dataset(\"com.lecun.mnist\")\n# mnist_train_images, mnist_train_labels = mnist_ds.train.images.data(), mnist_ds.train.labels.data()\n# mnist_test_images, mnist_test_labels =  mnist_ds.test.images.data(), mnist_ds.test.labels.data()\n\n# On transforme les images en vecteurs de réels et on rescale entre 0 et 1\nmnist_train_images = torch.FloatTensor(X_train).unsqueeze(1) / 255.\nmnist_train_labels = torch.LongTensor(Y_train)\nmnist_test_images = torch.FloatTensor(X_test).unsqueeze(1) / 255.\nmnist_test_labels = torch.LongTensor(Y_test)\n\nmnist_train_images, mnist_train_labels = mnist_train_images.to(device), mnist_train_labels.to(device)\nmnist_test_images, mnist_test_labels = mnist_test_images.to(device), mnist_test_labels.to(device)\n\n# On utilise un DataLoader pour faciliter les manipulations, on fixe arbitrairement la taille du mini batch à 32\nmnist_train_loader = DataLoader(TensorDataset(mnist_train_images,mnist_train_labels),batch_size=TRAIN_BATCH_SIZE,shuffle=True)\nmnist_test_loader = DataLoader(TensorDataset(mnist_test_images,mnist_test_labels),batch_size=TEST_BATCH_SIZE,shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T14:04:14.680643Z","iopub.execute_input":"2022-01-30T14:04:14.681173Z","iopub.status.idle":"2022-01-30T14:04:15.279928Z","shell.execute_reply.started":"2022-01-30T14:04:14.681136Z","shell.execute_reply":"2022-01-30T14:04:15.279140Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"markdown","source":"On reprend la même boucle d'apprentissage.","metadata":{}},{"cell_type":"code","source":"def accuracy(yhat,y):\n    # si y encode les indexes\n    if len(y.shape)==1 or y.size(1)==1:\n        return (torch.argmax(yhat,1).view(y.size(0),-1)== y.view(-1,1)).double().mean()\n    # si y est encodé en onehot\n    return (torch.argmax(yhat,1).view(-1) == torch.argmax(y,1).view(-1)).double().mean()\n\n\ndef train(model,epochs,train_loader,test_loader):\n    writer = SummaryWriter(f\"{TB_PATH}/{model.name}\")\n    optim = torch.optim.Adam(model.parameters(),lr=1e-3)\n    model = model.to(device)\n    print(f\"running {model.name}\")\n    loss = nn.CrossEntropyLoss()\n    for epoch in tqdm(range(epochs)):\n        cumloss, cumacc, count = 0, 0, 0\n        model.train()\n        for x,y in train_loader:\n            optim.zero_grad()\n            x,y = x.to(device), y.to(device)\n            yhat = model(x)\n            l = loss(yhat,y)\n            l.backward()\n            optim.step()\n            cumloss += l*len(x)\n            cumacc += accuracy(yhat,y)*len(x)\n            count += len(x)\n        writer.add_scalar('loss/train',cumloss/count,epoch)\n        writer.add_scalar('accuracy/train',cumacc/count,epoch)\n        if epoch % 1 == 0:\n            model.eval()\n            with torch.no_grad():\n                cumloss, cumacc, count = 0, 0, 0\n                for x,y in test_loader:\n                    x,y = x.to(device), y.to(device)\n                    yhat = model(x)\n                    cumloss += loss(yhat,y)*len(x)\n                    cumacc += accuracy(yhat,y)*len(x)\n                    count += len(x)\n                writer.add_scalar(f'loss/test',cumloss/count,epoch)\n                writer.add_scalar('accuracy/test',cumacc/count,epoch)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T14:04:15.281982Z","iopub.execute_input":"2022-01-30T14:04:15.282259Z","iopub.status.idle":"2022-01-30T14:04:15.296599Z","shell.execute_reply.started":"2022-01-30T14:04:15.282221Z","shell.execute_reply":"2022-01-30T14:04:15.295845Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"markdown","source":"## Réseau de convolution (CNN) \n\nImplémentez un réseau avec deux couches initiales de convolution <a href=https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d>**Conv2d**</a>, chacune comportant 16 filtres de taille 5x5. Chaque couche est suivie d'une activation ReLU et d'un <a href=https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d>**max-pooling**</a> de taille 3x3. On gardera un stride de 1 pour les convolutions et le pooling.\n\nQuelle est la taille du tenseur de sortie des couches de convolution ? (vous pouvez consulter ce <a href=https://arxiv.org/pdf/1603.07285.pdf>guide sur l'arithmétique des convolutions</a>).\n\nA la sortie des couches de convolutions, nous avons besoin d'un classifieur fully-connected. Utilisez deux couches de linéaires avec une activation ReLU.\n\nUsuellement, le sous-réseau convolutionnel est stocké dans une variable *self.features* (comme son rôle est d'extraire les features de l'image), et le sous-réseau fully connected dans une variable *self.classifier*.\n\nImplémentez la méthode **forward()** du réseau.\nEntraînez votre réseau sur MNIST.","metadata":{"id":"J7PEnxetNHso"}},{"cell_type":"code","source":"# Implémentation du ConvNet\nclass ConvNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.name = \"ConvNetV1\"\n        self.features()\n        self.classifier()\n        \n    def features(self):\n        self.conv1 = nn.Conv2d(1, 16, 5)   # black & white image -> 1 channel in ; 16 filters ; 5 by 5 kernel ; stride = 1 ; padding = 0\n        self.conv2 = nn.Conv2d(16, 256, 5) # 16 channel in ; 256 channel out (16*16 filters) ; 5 by 5 kernel ; stride = 1 ; padding = 0\n        self.pool = nn.MaxPool2d(3, 3)     # 3 by 3 kernel for max pooling\n        # input images are 28x28,\n                # after going through conv1 --> (28 - (conv_kernel_size=5 - 1)) = 24 \n                #                           --> (24 // pool_kernel_size=3) = 8\n                #                           --> 8*8 images with 16 channels \n                # after going through conv2 --> (8 - (conv_kernel_size=5 - 1)) = 4 \n                #                           --> (4 // pool_kernel_size=3) = 1\n                #                           --> 1*1 images with 16*16 channels \n        # our feature tensors are 1*1 and we have 256 channels --> 256*1 = 256\n        \n        # another exemple : \n        # self.conv1 = nn.Conv2d(1, 8, 3)  # black & white image -> 1 channel in ; 8 channel out ; 3 by 3 kernel ; stride = 1\n        # self.conv2 = nn.Conv2d(8, 64, 3) # 8 channel in ; 64 channel out ; 3 by 3 kernel ; stride = 1\n        # self.pool = nn.MaxPool2d(3, 3)   # 3 by 3 kernel for max pooling\n        # input images are 28x28,\n                # after going through conv1 --> (28 - (conv_kernel_size=3 - 1)) = 26 \n                #                           --> (26 // pool_kernel_size=3) = 8\n                #                           --> 8*8 images with 8 channels \n                # after going through conv2 --> (8 - (conv_kernel_size=3 - 1)) = 6 \n                #                           --> (6 // pool_kernel_size=3) = 2\n                #                           --> 2*2 images with 8*8 channels \n        # our feature tensors are 2*2 and we have 64 channels --> 2*2*64 = 256\n\n    def classifier(self):\n        self.fc1 = nn.Linear(256, 128)\n        self.fc2 = nn.Linear(128, 64) \n        self.fc3 = nn.Linear(64, 10)\n        \n    def forward(self, x):\n        # [256, 1, 28, 28]\n        x = self.pool(F.relu(self.conv1(x)))\n        # [256, 16, 8, 8]\n        x = self.pool(F.relu(self.conv2(x))) \n        # [256, 256, 1, 1]\n        x = torch.flatten(x, 1)\n        # [256, 256]\n        x = F.relu(self.fc1(x))\n        # [256, 128]\n        x = F.relu(self.fc2(x))\n        # [256, 64]\n        x = F.softmax(self.fc3(x))\n        # [256, 10]\n        return x","metadata":{"id":"qVVEjPJZee3M","outputId":"75100368-569e-45e9-eddb-d0d67baffe8d","execution":{"iopub.status.busy":"2022-01-30T14:04:35.998118Z","iopub.execute_input":"2022-01-30T14:04:35.998382Z","iopub.status.idle":"2022-01-30T14:04:36.010402Z","shell.execute_reply.started":"2022-01-30T14:04:35.998352Z","shell.execute_reply":"2022-01-30T14:04:36.009722Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Apprentissage du ConvNet\nmodel = ConvNet()\ntrain(model, 2, mnist_train_loader, mnist_test_loader)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T14:04:48.669459Z","iopub.execute_input":"2022-01-30T14:04:48.670082Z","iopub.status.idle":"2022-01-30T14:04:50.952827Z","shell.execute_reply.started":"2022-01-30T14:04:48.670035Z","shell.execute_reply":"2022-01-30T14:04:50.951593Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"yhat = model(mnist_test_images)\nprint(yhat.shape)\nprint(mnist_test_labels.shape)\nprint(accuracy(yhat, mnist_test_labels))","metadata":{"execution":{"iopub.status.busy":"2022-01-30T14:04:50.954464Z","iopub.execute_input":"2022-01-30T14:04:50.954730Z","iopub.status.idle":"2022-01-30T14:04:50.975143Z","shell.execute_reply.started":"2022-01-30T14:04:50.954693Z","shell.execute_reply":"2022-01-30T14:04:50.974310Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"markdown","source":"## Visualisation du CNN\n\nUne première manière d'introspecter un CNN est de visualiser les sorties des différentes couches et les filtres associés. Pour cela, on enregistre la sortie de chaque couche d'intérêt lors de la passe forward.\nLe code suivant permet d'obtenir cette succession d'images : la première image est l'image originale, chaque colonne correspond à un filtre. Les résultats sont ensuite regroupés par couche de convolution, les trois images dans une même colonne correspondent :1) aux poids de la convolution, 2) la sortie de la couche de convolution, 3) la sortie du pooling.\n\nQu'observez vous ? Comparez les différences entre un réseau utilisant un max-pooling et un réseau utilisant un <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html\">average pooling</a>. Vous pouvez également faire varier la taille et le nombre de filtres.\n","metadata":{"id":"mqqatL_wnw3x"}},{"cell_type":"code","source":"def analyse_conv(model,img,nb_filtres=16):\n    print(img.shape)\n    x = img.unsqueeze(0).to(device) # Modified unsqueeze because of wrong shape\n    print(x.shape)\n    img_conv = []\n    img_pool = []\n    for m in model._modules.values(): # Deleted .features\n        print(\"Layers :\", m)\n        x = m.forward(x)\n        if isinstance(m,nn.Conv2d):\n            img_conv.append((x.squeeze(0),m.weight))\n        if isinstance(m,nn.MaxPool2d) or isinstance(m,nn.AvgPool2d):\n            img_pool.append(x.squeeze(0))\n    plt.figure()\n    plt.imshow(img.permute(1,2,0).to('cpu'),cmap='gray')\n    # nombre de filtres\n    ksmax = min(nb_filtres, max([p[0].size(0) for p in img_conv]))\n    fig, axs = plt.subplots(3*len(img_conv),ksmax,figsize=(20,5))\n    for i,((img_c,w),img_p) in enumerate(zip(img_conv,img_pool)):\n        for j in range(min(nb_filtres,img_c.size(0))):\n            axs[3*i,j].imshow(np.array(w[j,0].to('cpu').detach()),cmap=\"gray\")\n            axs[3*i+1,j].imshow(np.array(img_c[j].to('cpu').detach()),cmap=\"gray\")                             \n        for j in range(min(nb_filtres,img_p.size(0))):\n            axs[3*i+2,j].imshow(np.array(img_p[j].to('cpu').detach()),cmap=\"gray\")","metadata":{"execution":{"iopub.status.busy":"2022-01-30T14:05:44.126088Z","iopub.execute_input":"2022-01-30T14:05:44.126371Z","iopub.status.idle":"2022-01-30T14:05:44.138181Z","shell.execute_reply.started":"2022-01-30T14:05:44.126340Z","shell.execute_reply":"2022-01-30T14:05:44.137476Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"# analyse_conv(model, mnist_test_images[0])","metadata":{"execution":{"iopub.status.busy":"2022-01-30T14:05:44.599010Z","iopub.execute_input":"2022-01-30T14:05:44.599551Z","iopub.status.idle":"2022-01-30T14:05:44.603250Z","shell.execute_reply.started":"2022-01-30T14:05:44.599498Z","shell.execute_reply":"2022-01-30T14:05:44.602582Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"markdown","source":"## Saliency Map\n\nLa visualisation des filtres ne permet pas bien de comprendre le rôle de chaque filtre dans la classification. Ils permettent d'extraire des features élémentaires qui combinées ensemble font sens pour un réseau fully-connected mais dont l'interprétation n'est pas évidente pour l'oeil humain.\n\nUne première méthode pour détecter quelles zones de l'image ont le plus impacté la décision sont les cartes de saillance. L'objectif des Saliency Maps est de détecter les pixels d'entrée qui ont le plus impacté la décision. L'idée est d'utiliser le gradient *par rapport* à l'image pour ranker les pixels. En effet, un gradient fort pour un pixel d'entrée indique qu'il faut changer faiblement sa valeur pour que la classe infére change (et a contrario, un gradient nul indique que le pixel n'est pas pris en compte pour la classification selon cette classe).\nLes étapes à suivre sont les suivantes :\n* le flag *requires_grad* est mis à True pour l'image (pour pouvoir calculer la rétro-propagation)\n* une passe forward est faite sur l'image\n* le backward est calculé sur le score de sortie de la classe d'intérêt\n* On affiche la valeur absolue du gradient  par rapport à l'entrée obtenu. Si l'image à plusieurs canaux, on prend le max de chacun de ces canaux.\n\n","metadata":{"id":"dNczoJevfoRn"}},{"cell_type":"code","source":"def getSaliency(model,img,label):\n    model.zero_grad()\n    img = img.to(device)\n    img.requires_grad = True\n    img.grad = None\n    outputs = nn.Softmax(dim=1)(model(img.unsqueeze(0)))\n    output=outputs[0,label] \n    output.backward()\n    sal=img.grad.abs()\n    if sal.dim()>2:\n        sal=torch.max(sal,dim=0)[0]\n    fig=plt.figure(figsize=(8, 8))\n    fig.add_subplot(1, 2, 1)\n    plt.imshow(img.detach().cpu().permute(1,2,0),cmap=\"gray\")\n    fig.add_subplot(1, 2, 2)\n    plt.imshow(sal.to('cpu'),cmap=\"seismic\",interpolation=\"bilinear\")\n    return sal","metadata":{"id":"BheIXf_j25Af","outputId":"b7bdc74b-b846-4607-c5cc-3e1416a26694","execution":{"iopub.status.busy":"2022-01-30T14:05:45.572442Z","iopub.execute_input":"2022-01-30T14:05:45.573123Z","iopub.status.idle":"2022-01-30T14:05:45.580329Z","shell.execute_reply.started":"2022-01-30T14:05:45.573085Z","shell.execute_reply":"2022-01-30T14:05:45.579651Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"for i in range(10):\n    x,y = mnist_train_loader.dataset[i]\n    # getSaliency(model,x,y)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T14:05:46.282983Z","iopub.execute_input":"2022-01-30T14:05:46.283794Z","iopub.status.idle":"2022-01-30T14:05:46.288135Z","shell.execute_reply.started":"2022-01-30T14:05:46.283743Z","shell.execute_reply":"2022-01-30T14:05:46.287337Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"markdown","source":"## Données CIFAR\n\nLa base de données CIFAR10  contient  60000 images couleur (RGB) 32x32 pixels. Les images appartiennent à 10 catégories (6000 images par classe): 'plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship' et 'truck'. Le\ndataset est composé de 50000 exemples d'apprentissage et 10000 de test.\n","metadata":{"id":"Vg5QWPIXDh2j"}},{"cell_type":"code","source":"def get_stats(dataloader):\n    n_batch = 0\n    chan = 0\n    chan_squared = 0\n    for elt, _ in dataloader:\n        chan += torch.mean(elt)\n        chan_squared += torch.mean(elt**2)\n        n_batch += 1\n    mean = chan/n_batch\n    std = np.sqrt((chan_squared/n_batch - mean**2))\n    return mean, std","metadata":{"execution":{"iopub.status.busy":"2022-01-30T14:05:47.424852Z","iopub.execute_input":"2022-01-30T14:05:47.425457Z","iopub.status.idle":"2022-01-30T14:05:47.431325Z","shell.execute_reply.started":"2022-01-30T14:05:47.425411Z","shell.execute_reply":"2022-01-30T14:05:47.430282Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"batchsize = 128              \n\ncifar_trainset = torchvision.datasets.CIFAR10(root='/tmp/data', train=True, download=True, transform=transforms.ToTensor())\ncifar_train_loader = torch.utils.data.DataLoader(cifar_trainset, batch_size=batchsize, pin_memory=True, shuffle=True)\n\nmean, std = get_stats(cifar_train_loader)\n\nprint(mean)\nprint(std)\n\ntransform = transforms.Compose(\n    [\n        transforms.ToTensor(),\n        transforms.Grayscale(num_output_channels=1),\n        transforms.Normalize(mean, std)\n    ])\n\ncifar_trainset = torchvision.datasets.CIFAR10(root='/tmp/data', train=True, download=True, transform=transform)\ncifar_train_loader = torch.utils.data.DataLoader(cifar_trainset, batch_size=batchsize, pin_memory=True, shuffle=True)\ncifar_testset = torchvision.datasets.CIFAR10(root='/tmp/data', train=False, download=True, transform=transform)\ncifar_test_loader = torch.utils.data.DataLoader(cifar_testset, batch_size=batchsize, pin_memory=True, shuffle=False)","metadata":{"id":"riJMyn5UE91a","outputId":"dae73d3e-e25b-4a2d-b61f-be3da78d8db4","execution":{"iopub.status.busy":"2022-01-30T14:05:50.709999Z","iopub.execute_input":"2022-01-30T14:05:50.710288Z","iopub.status.idle":"2022-01-30T14:06:04.423187Z","shell.execute_reply.started":"2022-01-30T14:05:50.710256Z","shell.execute_reply":"2022-01-30T14:06:04.422452Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"code","source":"print(cifar_trainset.classes)\nX_train, _ = next(iter(cifar_test_loader))\nprint(X_train.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T14:06:04.424742Z","iopub.execute_input":"2022-01-30T14:06:04.424974Z","iopub.status.idle":"2022-01-30T14:06:04.468953Z","shell.execute_reply.started":"2022-01-30T14:06:04.424940Z","shell.execute_reply":"2022-01-30T14:06:04.468214Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"markdown","source":"* Testez le réseau précédent avec 32 filtres et un réseau linéaire type *Linear(in_dim,120)->ReLU->Linear(120,80)->Relu->Linear(80,10)*  sur cette base de données et comparez les résultats. \n* Expérimenter également d'autres architectures de convolution (nombre de filtres, taille des filtres, différents strides, éventuellement padding). \n* Comparez le nombre de paramètres des réseaux\n* Visualisez la carte de saillance et les filtres du réseau.","metadata":{}},{"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)","metadata":{"id":"fl9mgy3IEeJc","outputId":"70b95a5f-74e0-4073-a760-4281c3fbc0a5","execution":{"iopub.status.busy":"2022-01-30T14:07:49.381514Z","iopub.execute_input":"2022-01-30T14:07:49.382097Z","iopub.status.idle":"2022-01-30T14:07:49.386791Z","shell.execute_reply.started":"2022-01-30T14:07:49.382059Z","shell.execute_reply":"2022-01-30T14:07:49.385730Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"## Définition du réseau feed-forward\n\nclass FeedFor1(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.name = \"FeedForwardV1\"\n        self.fc1 = nn.Linear(32*32, 512)\n        self.fc2 = nn.Linear(512, 512)\n        self.fc3 = nn.Linear(512, 256)\n        self.fc4 = nn.Linear(256, 10)\n        \n    def forward(self, x):\n        x = x.view(-1,32*32)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        x = F.softmax(self.fc4(x))\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-01-30T14:07:49.511018Z","iopub.execute_input":"2022-01-30T14:07:49.511266Z","iopub.status.idle":"2022-01-30T14:07:49.520133Z","shell.execute_reply.started":"2022-01-30T14:07:49.511236Z","shell.execute_reply":"2022-01-30T14:07:49.517511Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"## Définition du réseau convolutionnel\n## Utiliser nn.init.xavier_uniform pour l'initialisation des couches de convolutions\n\nclass ConvNet2(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.name = \"ConvNetV2\"\n        self.features()\n        self.classifier()\n        \n    def features(self):\n        self.conv1 = nn.Conv2d(1, 32, 5)           # 32 - (5-1) / 3 = 9 --> 9x9 images with 32 filters\n        nn.init.xavier_uniform(self.conv1.weight)\n        self.conv2 = nn.Conv2d(32, 1024, 5)        # 9 - (5-1) / 3 = 1 --> 1x1 images with 1024 filters --> 1024 indim for classifier\n        nn.init.xavier_uniform(self.conv2.weight)\n        self.pool = nn.MaxPool2d(3, 3)    \n\n    def classifier(self):\n        self.fc1 = nn.Linear(1024, 128) \n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, 10)\n        \n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.softmax(self.fc3(x))\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-01-30T14:13:59.792940Z","iopub.execute_input":"2022-01-30T14:13:59.793630Z","iopub.status.idle":"2022-01-30T14:13:59.804407Z","shell.execute_reply.started":"2022-01-30T14:13:59.793596Z","shell.execute_reply":"2022-01-30T14:13:59.803603Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"feed = FeedFor1()\nconv = ConvNet2()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T14:13:59.926779Z","iopub.execute_input":"2022-01-30T14:13:59.927663Z","iopub.status.idle":"2022-01-30T14:13:59.952352Z","shell.execute_reply.started":"2022-01-30T14:13:59.927619Z","shell.execute_reply":"2022-01-30T14:13:59.951744Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"code","source":"## Entraînement du réseau feed-forward\ntrain(feed, 1, cifar_train_loader, cifar_test_loader)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T14:14:03.969659Z","iopub.execute_input":"2022-01-30T14:14:03.970218Z","iopub.status.idle":"2022-01-30T14:14:18.466788Z","shell.execute_reply.started":"2022-01-30T14:14:03.970179Z","shell.execute_reply":"2022-01-30T14:14:18.465968Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"# Entraînement du réseau convolutionnel\ntrain(conv, 1, cifar_train_loader, cifar_test_loader)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T14:14:18.468644Z","iopub.execute_input":"2022-01-30T14:14:18.469077Z","iopub.status.idle":"2022-01-30T14:14:32.898099Z","shell.execute_reply.started":"2022-01-30T14:14:18.469036Z","shell.execute_reply":"2022-01-30T14:14:32.897391Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"code","source":"# Affichage du nombre de paramètres\nprint(\"Number of parameters for feedforward nn :\", count_parameters(feed))\nprint(\"Number of parameters for convolutional nn :\", count_parameters(conv))","metadata":{"execution":{"iopub.status.busy":"2022-01-30T14:14:32.899552Z","iopub.execute_input":"2022-01-30T14:14:32.900020Z","iopub.status.idle":"2022-01-30T14:14:32.905513Z","shell.execute_reply.started":"2022-01-30T14:14:32.899982Z","shell.execute_reply":"2022-01-30T14:14:32.904590Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"def get_test_data(dataloader, size):\n    X_test, Y_test = next(iter(dataloader))\n    batch_size = len(X_test)\n    n = size//batch_size\n    for i, batch in enumerate(dataloader):\n        if i < n:\n            X_tmp, Y_tmp = batch\n            X_test = torch.cat((X_test, X_tmp), 0)\n            Y_test = torch.cat((Y_test, Y_tmp), 0)\n    return X_test, Y_test\n\nX_test, Y_test = get_test_data(cifar_test_loader, len(cifar_test_loader)*batchsize)\n\nX_test, Y_test = X_test.to(device), Y_test.to(device)\n\nprint(X_test.shape)\nprint(Y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T14:17:14.881971Z","iopub.execute_input":"2022-01-30T14:17:14.882224Z","iopub.status.idle":"2022-01-30T14:17:17.680919Z","shell.execute_reply.started":"2022-01-30T14:17:14.882197Z","shell.execute_reply":"2022-01-30T14:17:17.680086Z"},"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"code","source":"yha_feed = feed(X_test)\nprint(yhat.shape)\nprint(\"Acc for feedforward nn : \", accuracy(yha_feed, Y_test))\n\nyhat_conv = conv(X_test)\nprint(yhat.shape)\nprint(\"Acc for convolutional nn :\", accuracy(yhat_conv, Y_test))","metadata":{"execution":{"iopub.status.busy":"2022-01-30T14:17:17.682361Z","iopub.execute_input":"2022-01-30T14:17:17.682942Z","iopub.status.idle":"2022-01-30T14:17:17.791963Z","shell.execute_reply.started":"2022-01-30T14:17:17.682903Z","shell.execute_reply":"2022-01-30T14:17:17.791209Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"code","source":"## Analyse des filtres du réseau\n# analyse_conv(conv, )","metadata":{"execution":{"iopub.status.busy":"2022-01-30T14:17:25.960244Z","iopub.execute_input":"2022-01-30T14:17:25.960548Z","iopub.status.idle":"2022-01-30T14:17:25.968790Z","shell.execute_reply.started":"2022-01-30T14:17:25.960497Z","shell.execute_reply":"2022-01-30T14:17:25.968091Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"code","source":"## Carte de saillance du réseau\nfor i in range(10):\n    x,y = cifar_train_loader.dataset[i]\n    # getSaliency(conv,x,y)","metadata":{"id":"ng4hpbqNLed4","outputId":"acbd7b8f-e41d-476a-bdc3-2b3323b4401d","execution":{"iopub.status.busy":"2022-01-30T14:17:26.203338Z","iopub.execute_input":"2022-01-30T14:17:26.203694Z","iopub.status.idle":"2022-01-30T14:17:26.219578Z","shell.execute_reply.started":"2022-01-30T14:17:26.203648Z","shell.execute_reply":"2022-01-30T14:17:26.218727Z"},"trusted":true},"execution_count":137,"outputs":[]},{"cell_type":"markdown","source":"# Data Augmentation\n\nPour améliorer les résultats, une technique courante est d'augmenter les données par des variantes des images du corpus. Cela permet de gagner en robustesse vis à vis de diverses transformations en forçant le réseau à apprendre des invariants (e.g. d'échelle, de rotation, d'inversion, de luminosité, etc.). \n\nInsérez quelques transformations de données lors du chargement des données (la liste des transformations disponibles se trouvent dans <a href=https://pytorch.org/vision/stable/transforms.html> torchvision.transforms</a>, par exemple **RandomHorizontalFlip()**, **RandomResizedCrop()**) et relancez l'apprentissage pour voir l'effet. Les transformations sont à insérer dans le **transforms.Compose()** avant la transformation en tenseur.","metadata":{"id":"q4m-5e7bOlnQ"}},{"cell_type":"code","source":"## Définition de la transformation pour Data Augmentation et création du réseau et des dataloader.\nbatchsize = 64            \n\ncifar_trainset = torchvision.datasets.CIFAR10(root='/tmp/data', train=True, download=True, transform=transforms.ToTensor())\ncifar_train_loader = torch.utils.data.DataLoader(cifar_trainset, batch_size=batchsize, pin_memory=True, shuffle=True)\n\nmean, std = get_stats(cifar_train_loader)\n\ntransform = transforms.Compose(\n    [\n        transforms.ToTensor(),\n        transforms.Grayscale(num_output_channels=1),\n        transforms.Normalize(mean, std),\n#         transforms.RandomHorizontalFlip(),\n#         transforms.RandomResizedCrop(32*32)\n    ])\n\ncifar_trainset = torchvision.datasets.CIFAR10(root='/tmp/data', train=True, download=True, transform=transform)\ncifar_train_loader = torch.utils.data.DataLoader(cifar_trainset, batch_size=batchsize, pin_memory=True, shuffle=True)\ncifar_testset = torchvision.datasets.CIFAR10(root='/tmp/data', train=False, download=True, transform=transform)\ncifar_test_loader = torch.utils.data.DataLoader(cifar_testset, batch_size=batchsize, pin_memory=True, shuffle=False)","metadata":{"id":"4qHiO_6QPhkJ","outputId":"978da56c-b6b3-448e-88ab-bc60ba77a3b4","execution":{"iopub.status.busy":"2022-01-30T14:17:27.650209Z","iopub.execute_input":"2022-01-30T14:17:27.650674Z","iopub.status.idle":"2022-01-30T14:17:35.714374Z","shell.execute_reply.started":"2022-01-30T14:17:27.650634Z","shell.execute_reply":"2022-01-30T14:17:35.713589Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"code","source":"# import gc\n\n# def free_gpu_cache():\n#     print(\"Initial GPU Usage\")\n#     gpu_usage()     \n#     gc.collect()\n#     torch.cuda.empty_cache()\n#     cuda.select_device(0)\n#     cuda.close()\n#     device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n#     cuda.select_device(0)\n#     print(\"GPU Usage after emptying the cache\")\n#     gpu_usage()\n#     return device\n\n# if feed : del feed\n# if conv : del conv\n# device = free_gpu_cache() ","metadata":{"execution":{"iopub.status.busy":"2022-01-30T14:17:35.716080Z","iopub.execute_input":"2022-01-30T14:17:35.716410Z","iopub.status.idle":"2022-01-30T14:17:35.721025Z","shell.execute_reply.started":"2022-01-30T14:17:35.716369Z","shell.execute_reply":"2022-01-30T14:17:35.719986Z"},"trusted":true},"execution_count":139,"outputs":[]},{"cell_type":"code","source":"conv = ConvNet2()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T14:17:35.722356Z","iopub.execute_input":"2022-01-30T14:17:35.722741Z","iopub.status.idle":"2022-01-30T14:17:35.744507Z","shell.execute_reply.started":"2022-01-30T14:17:35.722615Z","shell.execute_reply":"2022-01-30T14:17:35.743777Z"},"trusted":true},"execution_count":140,"outputs":[]},{"cell_type":"code","source":"## Apprentissage du réseau\ntrain(conv, 1, cifar_train_loader, cifar_test_loader)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T14:18:50.826168Z","iopub.execute_input":"2022-01-30T14:18:50.826546Z","iopub.status.idle":"2022-01-30T14:19:07.174544Z","shell.execute_reply.started":"2022-01-30T14:18:50.826485Z","shell.execute_reply":"2022-01-30T14:19:07.173827Z"},"trusted":true},"execution_count":144,"outputs":[]},{"cell_type":"code","source":"## Accuracy\nX_test, Y_test = get_test_data(cifar_test_loader, len(cifar_test_loader)*batchsize)\nX_test, Y_test = X_test.to(device), Y_test.to(device)\nprint(\"Acc for convolutional nn :\", accuracy(conv(X_test), Y_test))","metadata":{"execution":{"iopub.status.busy":"2022-01-30T14:19:07.176187Z","iopub.execute_input":"2022-01-30T14:19:07.176612Z","iopub.status.idle":"2022-01-30T14:19:10.663015Z","shell.execute_reply.started":"2022-01-30T14:19:07.176571Z","shell.execute_reply":"2022-01-30T14:19:10.661374Z"},"trusted":true},"execution_count":145,"outputs":[]},{"cell_type":"markdown","source":"# Modèles pré-entraînés / Transfert\n\nPyTorch propose un certain nombre de modèles pré-entraînés sur le très gros corpus d'images ImageNet. Ces modèles très lourds demandent beaucoup de ressources pour être entraînés efficacement. Mais une fois leur entraînement effectué, ils peuvent être appliqués assez facilement sur d'autres corpus que ImageNet, moyennant quelques adaptations. Dans la suite nous considérons le modèle <a href=https://pytorch.org/hub/pytorch_vision_alexnet/>AlexNet</a> pour l'extraction de features. La sortie du réseau doit être adaptée et ré-entraînée pour permettre de classer des images sur notre corpus CIFAR. \n\nCommençons par collecter le réseau entraîné et étudions sa structure: ","metadata":{"id":"EDFJTu95Hx7H"}},{"cell_type":"code","source":"from torchvision import models\nalexnet = models.alexnet(pretrained=True)\nprint(alexnet)","metadata":{"id":"Jsq26b2sJIfp","outputId":"7dcb6954-fca9-4fa2-f5e1-15d83188e1cf","execution":{"iopub.status.busy":"2022-01-30T14:19:40.833088Z","iopub.execute_input":"2022-01-30T14:19:40.833355Z","iopub.status.idle":"2022-01-30T14:19:42.825375Z","shell.execute_reply.started":"2022-01-30T14:19:40.833325Z","shell.execute_reply":"2022-01-30T14:19:42.824538Z"},"trusted":true},"execution_count":146,"outputs":[]},{"cell_type":"markdown","source":"## Fine-Tuning d'AlexNet","metadata":{}},{"cell_type":"markdown","source":"Que faut-il modifier pour l'adapter à notre cas ? En outre on aimerait que lors de l'apprentissage seuls les poids des modules modifiés soient ajustés. Penser à fixer les autres.","metadata":{"id":"l2d7RA8NMRn0"}},{"cell_type":"code","source":"# Il faut modifier le nombre de neurones en sortie car nous n'avons que 10 classes à prédire.\n# Il faut modifier le nombre de neurones sur les couches dans le classifieur car il risque fortement d'overfit, du fait de notre nombre de classes.\n\nalexnet.classifier[1] = nn.Linear(9216, 2048)\nalexnet.classifier[4] = nn.Linear(2048, 1024)\nalexnet.classifier[6] = nn.Linear(1024, 10)\nalexnet.eval()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T14:19:45.831446Z","iopub.execute_input":"2022-01-30T14:19:45.831727Z","iopub.status.idle":"2022-01-30T14:19:46.185140Z","shell.execute_reply.started":"2022-01-30T14:19:45.831696Z","shell.execute_reply":"2022-01-30T14:19:46.184408Z"},"trusted":true},"execution_count":147,"outputs":[]},{"cell_type":"code","source":"# Ici nous voulons entrainer seulement la partie classifieur et non la partie feature extracting du CNN\n# Permet de mettre à True/False tous les requires_grad des paramètres du réseau\n\ndef set_parameter_requires_grad(model, feature_extract):\n    if feature_extract:\n        for name,p in model.named_parameters():\n            if \"features\" in name:\n                p.requires_grad = False    \n            else:\n                p.requires_grad = True    \n            \nset_parameter_requires_grad(alexnet, True)","metadata":{"id":"CQND64P3Me9E","outputId":"af363505-a80d-4133-a975-8b3b7ac16ca5","execution":{"iopub.status.busy":"2022-01-30T14:19:48.159289Z","iopub.execute_input":"2022-01-30T14:19:48.159573Z","iopub.status.idle":"2022-01-30T14:19:48.167901Z","shell.execute_reply.started":"2022-01-30T14:19:48.159523Z","shell.execute_reply":"2022-01-30T14:19:48.167151Z"},"trusted":true},"execution_count":148,"outputs":[]},{"cell_type":"markdown","source":"Il s'agit également de remettre le modèle dans les mêmes conditions qu'il a été appris (taille de l'entrée 224, normalisation selon moyennes et variances de ImageNet, etc.).","metadata":{"id":"zn4ee8T2OF1S"}},{"cell_type":"code","source":"input_size=224\n\nmean=[0.485, 0.456, 0.406]\nstd=[0.229, 0.224, 0.225]\n\ntransformAlexTrain=transforms.Compose([ # Cette fois on utilise pas de grayscale car nous avons un gros modele pré-entrainé\n        transforms.RandomResizedCrop(input_size), # selection aléatoire d'une zone de la taille voulue (augmentation des données en apprentissage)\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std)\n    ])\ntransformAlexTest=transforms.Compose([\n        transforms.Resize(input_size), # selection de la zone centrale de la taille voulue\n        transforms.CenterCrop(input_size),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std)\n    ])\n\nalex_trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transformAlexTrain)\nalex_trainloader = torch.utils.data.DataLoader(alex_trainset, batch_size=batchsize, pin_memory=True, shuffle=True)\n\nalex_testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transformAlexTest)\nalex_testloader = torch.utils.data.DataLoader(alex_testset, batch_size=batchsize, pin_memory=True, shuffle=True)","metadata":{"id":"6ck8ws25NqJY","outputId":"3f35442f-8211-4c10-dddd-90232d37913c","execution":{"iopub.status.busy":"2022-01-30T14:19:50.104329Z","iopub.execute_input":"2022-01-30T14:19:50.104890Z","iopub.status.idle":"2022-01-30T14:19:57.483256Z","shell.execute_reply.started":"2022-01-30T14:19:50.104850Z","shell.execute_reply":"2022-01-30T14:19:57.482464Z"},"trusted":true},"execution_count":149,"outputs":[]},{"cell_type":"code","source":"def train(model,epochs,train_loader,test_loader,feature_extract=False):\n    model = model.to(device)\n    writer = SummaryWriter(f\"{TB_PATH}/{model.name}\")\n    \n    params_to_update = model.parameters()\n    print(\"Params to learn:\")\n    if feature_extract:\n        params_to_update = []\n        for name,param in model.named_parameters():\n            if param.requires_grad == True:\n                params_to_update.append(param)\n                print(name)\n    else:\n        for name,param in model.named_parameters():\n            if param.requires_grad == True:\n                print(name)\n    optim = torch.optim.Adam(params_to_update,lr=1e-3)\n    \n    print(f\"running {model.name}\")\n    loss = nn.CrossEntropyLoss()\n    for epoch in tqdm(range(epochs)):\n        cumloss, cumacc, count = 0, 0, 0\n        model.train()\n        for x,y in train_loader:\n            optim.zero_grad()\n            x,y = x.to(device), y.to(device)\n            yhat = model(x)\n            l = loss(yhat,y)\n            l.backward()\n            optim.step()\n            cumloss += l*len(x)\n            cumacc += accuracy(yhat,y)*len(x)\n            count += len(x)\n        writer.add_scalar('loss/train',cumloss/count,epoch)\n        writer.add_scalar('accuracy/train',cumacc/count,epoch)\n        if epoch % 1 == 0:\n            model.eval()\n            with torch.no_grad():\n                cumloss, cumacc, count = 0, 0, 0\n                for x,y in test_loader:\n                    x,y = x.to(device), y.to(device)\n                    yhat = model(x)\n                    cumloss += loss(yhat,y)*len(x)\n                    cumacc += accuracy(yhat,y)*len(x)\n                    count += len(x)\n                writer.add_scalar(f'loss/test',cumloss/count,epoch)\n                writer.add_scalar('accuracy/test',cumacc/count,epoch)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T14:22:24.538896Z","iopub.execute_input":"2022-01-30T14:22:24.539149Z","iopub.status.idle":"2022-01-30T14:22:24.551475Z","shell.execute_reply.started":"2022-01-30T14:22:24.539122Z","shell.execute_reply":"2022-01-30T14:22:24.550660Z"},"trusted":true},"execution_count":150,"outputs":[]},{"cell_type":"markdown","source":"Faites le Fine-tuning de alexnet sur les données CIFAR. Regardez les cartes de saillances obtenues.","metadata":{}},{"cell_type":"code","source":"## Entraînement du réseau\nalexnet.name = \"AlexNet\"\ntrain(alexnet, 1, alex_trainloader, alex_testloader)","metadata":{"id":"fBas4XoPdBnh","outputId":"22f467dd-7bb5-4324-82e6-094eae781010","execution":{"iopub.status.busy":"2022-01-30T14:22:30.605909Z","iopub.execute_input":"2022-01-30T14:22:30.606430Z","iopub.status.idle":"2022-01-30T14:24:20.292296Z","shell.execute_reply.started":"2022-01-30T14:22:30.606396Z","shell.execute_reply":"2022-01-30T14:24:20.291594Z"},"trusted":true},"execution_count":151,"outputs":[]},{"cell_type":"code","source":"## Accuracy\nX_test, Y_test = get_test_data(alex_testloader, 1000) \nX_test, Y_test = X_test.to(device), Y_test.to(device)\nprint(\"Acc for alexnet transfer learning :\", accuracy(alexnet(X_test), Y_test))","metadata":{"execution":{"iopub.status.busy":"2022-01-30T14:24:35.729059Z","iopub.execute_input":"2022-01-30T14:24:35.729314Z","iopub.status.idle":"2022-01-30T14:24:55.519884Z","shell.execute_reply.started":"2022-01-30T14:24:35.729286Z","shell.execute_reply":"2022-01-30T14:24:55.519088Z"},"trusted":true},"execution_count":152,"outputs":[]},{"cell_type":"code","source":"## Carte de saillance du réseau\n# inputs,labels=iter(alex_testloader).next()\n# for i in range(len(cifar_trainset.classes)):\n#     print(\"Pour \",cifar_trainset.classes[i])\n#     getSaliency(alexnet,inputs[0],i)","metadata":{"id":"HJ3eACA8p2Ev","outputId":"18813f4d-a5af-41de-9d08-2132ba0608c3","execution":{"iopub.status.busy":"2022-01-30T14:25:02.147103Z","iopub.execute_input":"2022-01-30T14:25:02.147595Z","iopub.status.idle":"2022-01-30T14:25:02.151399Z","shell.execute_reply.started":"2022-01-30T14:25:02.147554Z","shell.execute_reply":"2022-01-30T14:25:02.150026Z"},"trusted":true},"execution_count":153,"outputs":[]},{"cell_type":"markdown","source":"## Class Activation Maps (CAM)\nUne autre technique d'introspection est le Class Activation Maps. Cette technique permet de visualiser quels sont les régions qui ont fait le plus réagir les différents filtres qui ont servis à la classification. Elle part de la constatation que la sortie d'un filtre de la dernière couche convolutionnelle indique spatialement quelles sont les régions de l'image qui ont fait réagir le filtre (la sortie est généralement de taille plus petite - *downscalé* - mais on peut la mettre à l'échelle). Cependant, il est difficile d'analyser avec la succession des couches non-linéaires en aval le rôle de chaque sortie convolutionnelle dans le processus de classification. Cependant, un réseau plus simple - uniquement linéaire par exemple - permettrait de donner une indication à l'importance de chaque filtre (au prix d'une erreur plus grosse en classification). \nCette technique nécessite donc la modification des dernières couches du réseau de la manière suivante : \n* un pooling de moyennage globale (un average pooling de la taille de l'image) est appliquée à chaque filtre de convolution de la dernière couche convolutionnelle : seul le signal moyen de chaque filtre est retenu, sans plus aucune information spatiale.\n* un réseau linéaire est ensuite utilisé du nombre de filtres vers le nombre de classes qui va permettre de mettre en évidence l'intérêt de chaque filtre dans la classification.\n\nLe réseau ainsi modifié est fine-tuné sur le corpus. L'Activation Map est obtenu en sommant les sorties de la dernière couche convolutionnelle pondérées par les poids du réseau linéaire. \nModifiez le réseau, puis ré-entraîneé les couches modifiées. ","metadata":{"id":"pWh_Nx-4CowB"}},{"cell_type":"code","source":"# Couche identité\nclass Identity(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-01-30T14:25:04.097389Z","iopub.execute_input":"2022-01-30T14:25:04.098239Z","iopub.status.idle":"2022-01-30T14:25:04.102894Z","shell.execute_reply.started":"2022-01-30T14:25:04.098192Z","shell.execute_reply":"2022-01-30T14:25:04.101834Z"},"trusted":true},"execution_count":154,"outputs":[]},{"cell_type":"code","source":"## Remplacement de la couche classifier par un module d'average pooling et un linéaire.\nalexnet = models.alexnet(pretrained=True)\nprint(alexnet)\nalexnet.features[12] = nn.AvgPool2d(kernel_size=3, stride=2, padding=0, ceil_mode=False)\nalexnet.classifier[0] = Identity()\nalexnet.classifier[1] = Identity()\nalexnet.classifier[2] = Identity()\nalexnet.classifier[3] = Identity()\nalexnet.classifier[4] = Identity()\nalexnet.classifier[5] = Identity()\nalexnet.classifier[6] = nn.Linear(9216, 10)\nprint(alexnet)","metadata":{"id":"1uRKU2UsDMK8","outputId":"109481d4-b844-4cb7-e23d-b85743fb5342","execution":{"iopub.status.busy":"2022-01-30T14:25:04.952005Z","iopub.execute_input":"2022-01-30T14:25:04.952522Z","iopub.status.idle":"2022-01-30T14:25:05.782944Z","shell.execute_reply.started":"2022-01-30T14:25:04.952477Z","shell.execute_reply":"2022-01-30T14:25:05.782164Z"},"trusted":true},"execution_count":155,"outputs":[]},{"cell_type":"code","source":"## Entrainement du réseau\nset_parameter_requires_grad(alexnet, True)\nalexnet.name = \"AlexNetAvg\"\ntrain(alexnet, 1, alex_trainloader, alex_testloader)","metadata":{"id":"s4YjVJwRHvd9","outputId":"26704c22-80eb-4abe-8a16-eba567f02991","execution":{"iopub.status.busy":"2022-01-30T14:25:19.221897Z","iopub.execute_input":"2022-01-30T14:25:19.222158Z","iopub.status.idle":"2022-01-30T14:27:05.652075Z","shell.execute_reply.started":"2022-01-30T14:25:19.222129Z","shell.execute_reply":"2022-01-30T14:27:05.651360Z"},"trusted":true},"execution_count":156,"outputs":[]},{"cell_type":"markdown","source":"Il ne reste plus qu'à écrire la fonction generate_cam qui affiche une image d'activation par classe.","metadata":{"id":"Y-egbZeDHRxu"}},{"cell_type":"code","source":"def generate_cam(model,input_image,target_class=None):\n    ## Calcul du forward sur l'image\n    with torch.no_grad():\n        input_image=input_image.to(device)\n        x = model.features(input_image)\n        out=model.classifier(x)\n        out=torch.nn.functional.softmax(out,-1)\n    if target_class is None:\n        target_class = torch.max(out,dim=-1)[1].item()\n    print(\"target_class\",target_class)\n    ## Récupération des poids du linéaire\n    weights = dict(model.classifier.named_modules())[\"Linear\"].weight.data  \n    fig = plt.figure(figsize=(16, 8))\n    fig.add_subplot(1,2, 1)\n    img=input_image.to(\"cpu\")*torch.tensor(std).view(3,1,1)+torch.tensor(mean).view(3,1,1)\n    img=torch.nn.functional.interpolate(img, size=(244, 244), mode=\"bilinear\", align_corners=False)\n    plt.imshow(img.cpu().squeeze().permute(1,2,0))\n    ## Calcul de CAM\n    y=x*weights[target_class].view(1,-1,1,1)\n    y=(y.sum(1))  \n    fig.add_subplot(1, 2, 2)\n    y=torch.nn.functional.interpolate(y.unsqueeze(0),size=(244,244),mode=\"bilinear\",align_corners=False)\n    plt.imshow(y.cpu().squeeze(),cmap=\"afmhot\")\n    plt.show()","metadata":{"id":"7nJX5DzTcXRa","execution":{"iopub.status.busy":"2022-01-30T14:27:41.237583Z","iopub.execute_input":"2022-01-30T14:27:41.238308Z","iopub.status.idle":"2022-01-30T14:27:41.248774Z","shell.execute_reply.started":"2022-01-30T14:27:41.238268Z","shell.execute_reply":"2022-01-30T14:27:41.248055Z"},"trusted":true},"execution_count":157,"outputs":[]},{"cell_type":"code","source":"# inputs,labels=iter(alex_trainloader).next()\n# generate_cam(alexnet,inputs[0].unsqueeze(0))","metadata":{"id":"pH1DGZd225Ah","outputId":"f3a1381f-29a3-4f13-a005-f30d4cab671b","execution":{"iopub.status.busy":"2022-01-30T14:27:42.317940Z","iopub.execute_input":"2022-01-30T14:27:42.318834Z","iopub.status.idle":"2022-01-30T14:27:42.323160Z","shell.execute_reply.started":"2022-01-30T14:27:42.318780Z","shell.execute_reply":"2022-01-30T14:27:42.322293Z"},"trusted":true},"execution_count":158,"outputs":[]},{"cell_type":"markdown","source":"On peut aussi charger des images du web et voir ce que notre classifieur donne. Par exemple:","metadata":{"id":"qW-1Yc8QehFU"}},{"cell_type":"code","source":"# !wget \"https://www.fidanimo.com/sites/default/files/2020-10/dog-sitter.jpg\"\n# !wget https://assets.siemens-energy.com/siemens/assets/api/uuid:78a9c83e-219e-4fd5-b948-2bf07276916d/width:640/quality:high/4320x3240-keyvisual-cargo.jpg\n# from PIL import Image\n# imageDog = transformAlexTest(Image.open(\"dog-sitter.jpg\")).unsqueeze(0).to(device, torch.float)\n# imageShip = transformAlexTest(Image.open(\"4320x3240-keyvisual-cargo.jpg\")).unsqueeze(0).to(device,torch.float)","metadata":{"id":"-9iglZFY25Ak","outputId":"54b0b02f-74ce-4585-a272-b43d2989440d","execution":{"iopub.status.busy":"2022-01-30T14:27:42.636911Z","iopub.execute_input":"2022-01-30T14:27:42.637620Z","iopub.status.idle":"2022-01-30T14:27:42.641502Z","shell.execute_reply.started":"2022-01-30T14:27:42.637580Z","shell.execute_reply":"2022-01-30T14:27:42.640669Z"},"trusted":true},"execution_count":159,"outputs":[]},{"cell_type":"code","source":"# for i in range(10):\n#     print(cifar_trainset.classes[i])\n#     generate_cam(alexnet,imageDog,i)\n# generate_cam(alexnet,imageShip)","metadata":{"id":"4jvss7_a25Am","outputId":"3137f78a-f047-4e10-823c-9d5e092baf7f","execution":{"iopub.status.busy":"2022-01-30T14:27:42.728736Z","iopub.execute_input":"2022-01-30T14:27:42.729203Z","iopub.status.idle":"2022-01-30T14:27:42.733051Z","shell.execute_reply.started":"2022-01-30T14:27:42.729168Z","shell.execute_reply":"2022-01-30T14:27:42.732391Z"},"trusted":true},"execution_count":160,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}