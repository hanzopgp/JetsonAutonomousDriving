{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2 : Réseaux convolutionnels pour le traitement de l'image  -  Formation Cdiscount\n",
    "\n",
    "## Mercredi 26 Mai 2021\n",
    "\n",
    "Nicolas Baskiotis (nicolas.baskiotis@lip6.fr) Benjamin Piwowarski (benjamin.piwowarski@lip6.fr) -- MLIA/LIP6, Sorbonne Université"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les objectifs de ce module sont :\n",
    "* Prise en main des réseaux convolutionnels (CNN)\n",
    "* Apprentissage d'un CNN\n",
    "* Introspection d'un CNN\n",
    "* Fine-tuning d'un réseau pré-appris\n",
    "\n",
    "Nous travaillerons dans un premier temps avec les données MNIST puis avec le jeu de données CIFAR d'images de 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:04:06.400552Z",
     "iopub.status.busy": "2022-01-30T14:04:06.400277Z",
     "iopub.status.idle": "2022-01-30T14:04:14.671715Z",
     "shell.execute_reply": "2022-01-30T14:04:14.670877Z",
     "shell.execute_reply.started": "2022-01-30T14:04:06.400504Z"
    },
    "id": "OQfQDwsPAGpM"
   },
   "outputs": [],
   "source": [
    "!pip install GPUtil\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "# !pip install torch\n",
    "# !pip install torchvision\n",
    "# !pip install datamaestro\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "import os\n",
    "from tensorboard import notebook\n",
    "# from datamaestro import prepare_dataset\n",
    "from tensorboard import notebook\n",
    "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:04:14.674551Z",
     "iopub.status.busy": "2022-01-30T14:04:14.674247Z",
     "iopub.status.idle": "2022-01-30T14:04:14.679020Z",
     "shell.execute_reply": "2022-01-30T14:04:14.678335Z",
     "shell.execute_reply.started": "2022-01-30T14:04:14.674499Z"
    }
   },
   "outputs": [],
   "source": [
    "TB_PATH = \"/tmp/logs/sceance2\"\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir /tmp/logs/sceance2\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_6O63E325AP"
   },
   "source": [
    "# Premier réseau convolutionnel\n",
    "\n",
    "Nous allons reprendre les données MNIST dans un premier temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:04:14.681173Z",
     "iopub.status.busy": "2022-01-30T14:04:14.680643Z",
     "iopub.status.idle": "2022-01-30T14:04:15.279928Z",
     "shell.execute_reply": "2022-01-30T14:04:15.279140Z",
     "shell.execute_reply.started": "2022-01-30T14:04:14.681136Z"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 256\n",
    "TEST_BATCH_SIZE = 512\n",
    "\n",
    "# Téléchargement des données\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "# mnist_ds = prepare_dataset(\"com.lecun.mnist\")\n",
    "# mnist_train_images, mnist_train_labels = mnist_ds.train.images.data(), mnist_ds.train.labels.data()\n",
    "# mnist_test_images, mnist_test_labels =  mnist_ds.test.images.data(), mnist_ds.test.labels.data()\n",
    "\n",
    "# On transforme les images en vecteurs de réels et on rescale entre 0 et 1\n",
    "mnist_train_images = torch.FloatTensor(X_train).unsqueeze(1) / 255.\n",
    "mnist_train_labels = torch.LongTensor(Y_train)\n",
    "mnist_test_images = torch.FloatTensor(X_test).unsqueeze(1) / 255.\n",
    "mnist_test_labels = torch.LongTensor(Y_test)\n",
    "\n",
    "mnist_train_images, mnist_train_labels = mnist_train_images.to(device), mnist_train_labels.to(device)\n",
    "mnist_test_images, mnist_test_labels = mnist_test_images.to(device), mnist_test_labels.to(device)\n",
    "\n",
    "# On utilise un DataLoader pour faciliter les manipulations, on fixe arbitrairement la taille du mini batch à 32\n",
    "mnist_train_loader = DataLoader(TensorDataset(mnist_train_images,mnist_train_labels),batch_size=TRAIN_BATCH_SIZE,shuffle=True)\n",
    "mnist_test_loader = DataLoader(TensorDataset(mnist_test_images,mnist_test_labels),batch_size=TEST_BATCH_SIZE,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On reprend la même boucle d'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:04:15.282259Z",
     "iopub.status.busy": "2022-01-30T14:04:15.281982Z",
     "iopub.status.idle": "2022-01-30T14:04:15.296599Z",
     "shell.execute_reply": "2022-01-30T14:04:15.295845Z",
     "shell.execute_reply.started": "2022-01-30T14:04:15.282221Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(yhat,y):\n",
    "    # si y encode les indexes\n",
    "    if len(y.shape)==1 or y.size(1)==1:\n",
    "        return (torch.argmax(yhat,1).view(y.size(0),-1)== y.view(-1,1)).double().mean()\n",
    "    # si y est encodé en onehot\n",
    "    return (torch.argmax(yhat,1).view(-1) == torch.argmax(y,1).view(-1)).double().mean()\n",
    "\n",
    "\n",
    "def train(model,epochs,train_loader,test_loader):\n",
    "    writer = SummaryWriter(f\"{TB_PATH}/{model.name}\")\n",
    "    optim = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "    model = model.to(device)\n",
    "    print(f\"running {model.name}\")\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        cumloss, cumacc, count = 0, 0, 0\n",
    "        model.train()\n",
    "        for x,y in train_loader:\n",
    "            optim.zero_grad()\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            yhat = model(x)\n",
    "            l = loss(yhat,y)\n",
    "            l.backward()\n",
    "            optim.step()\n",
    "            cumloss += l*len(x)\n",
    "            cumacc += accuracy(yhat,y)*len(x)\n",
    "            count += len(x)\n",
    "        writer.add_scalar('loss/train',cumloss/count,epoch)\n",
    "        writer.add_scalar('accuracy/train',cumacc/count,epoch)\n",
    "        if epoch % 1 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                cumloss, cumacc, count = 0, 0, 0\n",
    "                for x,y in test_loader:\n",
    "                    x,y = x.to(device), y.to(device)\n",
    "                    yhat = model(x)\n",
    "                    cumloss += loss(yhat,y)*len(x)\n",
    "                    cumacc += accuracy(yhat,y)*len(x)\n",
    "                    count += len(x)\n",
    "                writer.add_scalar(f'loss/test',cumloss/count,epoch)\n",
    "                writer.add_scalar('accuracy/test',cumacc/count,epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7PEnxetNHso"
   },
   "source": [
    "## Réseau de convolution (CNN) \n",
    "\n",
    "Implémentez un réseau avec deux couches initiales de convolution <a href=https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d>**Conv2d**</a>, chacune comportant 16 filtres de taille 5x5. Chaque couche est suivie d'une activation ReLU et d'un <a href=https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d>**max-pooling**</a> de taille 3x3. On gardera un stride de 1 pour les convolutions et le pooling.\n",
    "\n",
    "Quelle est la taille du tenseur de sortie des couches de convolution ? (vous pouvez consulter ce <a href=https://arxiv.org/pdf/1603.07285.pdf>guide sur l'arithmétique des convolutions</a>).\n",
    "\n",
    "A la sortie des couches de convolutions, nous avons besoin d'un classifieur fully-connected. Utilisez deux couches de linéaires avec une activation ReLU.\n",
    "\n",
    "Usuellement, le sous-réseau convolutionnel est stocké dans une variable *self.features* (comme son rôle est d'extraire les features de l'image), et le sous-réseau fully connected dans une variable *self.classifier*.\n",
    "\n",
    "Implémentez la méthode **forward()** du réseau.\n",
    "Entraînez votre réseau sur MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:04:35.998382Z",
     "iopub.status.busy": "2022-01-30T14:04:35.998118Z",
     "iopub.status.idle": "2022-01-30T14:04:36.010402Z",
     "shell.execute_reply": "2022-01-30T14:04:36.009722Z",
     "shell.execute_reply.started": "2022-01-30T14:04:35.998352Z"
    },
    "id": "qVVEjPJZee3M",
    "outputId": "75100368-569e-45e9-eddb-d0d67baffe8d"
   },
   "outputs": [],
   "source": [
    "# Implémentation du ConvNet\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.name = \"ConvNetV1\"\n",
    "        self.features()\n",
    "        self.classifier()\n",
    "        \n",
    "    def features(self):\n",
    "        self.conv1 = nn.Conv2d(1, 16, 5)   # black & white image -> 1 channel in ; 16 filters ; 5 by 5 kernel ; stride = 1 ; padding = 0\n",
    "        self.conv2 = nn.Conv2d(16, 256, 5) # 16 channel in ; 256 channel out (16*16 filters) ; 5 by 5 kernel ; stride = 1 ; padding = 0\n",
    "        self.pool = nn.MaxPool2d(3, 3)     # 3 by 3 kernel for max pooling\n",
    "        # input images are 28x28,\n",
    "                # after going through conv1 --> (28 - (conv_kernel_size=5 - 1)) = 24 \n",
    "                #                           --> (24 // pool_kernel_size=3) = 8\n",
    "                #                           --> 8*8 images with 16 channels \n",
    "                # after going through conv2 --> (8 - (conv_kernel_size=5 - 1)) = 4 \n",
    "                #                           --> (4 // pool_kernel_size=3) = 1\n",
    "                #                           --> 1*1 images with 16*16 channels \n",
    "        # our feature tensors are 1*1 and we have 256 channels --> 256*1 = 256\n",
    "        \n",
    "        # another exemple : \n",
    "        # self.conv1 = nn.Conv2d(1, 8, 3)  # black & white image -> 1 channel in ; 8 channel out ; 3 by 3 kernel ; stride = 1\n",
    "        # self.conv2 = nn.Conv2d(8, 64, 3) # 8 channel in ; 64 channel out ; 3 by 3 kernel ; stride = 1\n",
    "        # self.pool = nn.MaxPool2d(3, 3)   # 3 by 3 kernel for max pooling\n",
    "        # input images are 28x28,\n",
    "                # after going through conv1 --> (28 - (conv_kernel_size=3 - 1)) = 26 \n",
    "                #                           --> (26 // pool_kernel_size=3) = 8\n",
    "                #                           --> 8*8 images with 8 channels \n",
    "                # after going through conv2 --> (8 - (conv_kernel_size=3 - 1)) = 6 \n",
    "                #                           --> (6 // pool_kernel_size=3) = 2\n",
    "                #                           --> 2*2 images with 8*8 channels \n",
    "        # our feature tensors are 2*2 and we have 64 channels --> 2*2*64 = 256\n",
    "\n",
    "    def classifier(self):\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.fc2 = nn.Linear(128, 64) \n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # [256, 1, 28, 28]\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # [256, 16, 8, 8]\n",
    "        x = self.pool(F.relu(self.conv2(x))) \n",
    "        # [256, 256, 1, 1]\n",
    "        x = torch.flatten(x, 1)\n",
    "        # [256, 256]\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # [256, 128]\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # [256, 64]\n",
    "        x = F.softmax(self.fc3(x))\n",
    "        # [256, 10]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:04:48.670082Z",
     "iopub.status.busy": "2022-01-30T14:04:48.669459Z",
     "iopub.status.idle": "2022-01-30T14:04:50.952827Z",
     "shell.execute_reply": "2022-01-30T14:04:50.951593Z",
     "shell.execute_reply.started": "2022-01-30T14:04:48.670035Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Apprentissage du ConvNet\n",
    "model = ConvNet()\n",
    "train(model, 2, mnist_train_loader, mnist_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:04:50.954730Z",
     "iopub.status.busy": "2022-01-30T14:04:50.954464Z",
     "iopub.status.idle": "2022-01-30T14:04:50.975143Z",
     "shell.execute_reply": "2022-01-30T14:04:50.974310Z",
     "shell.execute_reply.started": "2022-01-30T14:04:50.954693Z"
    }
   },
   "outputs": [],
   "source": [
    "yhat = model(mnist_test_images)\n",
    "print(yhat.shape)\n",
    "print(mnist_test_labels.shape)\n",
    "print(accuracy(yhat, mnist_test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqqatL_wnw3x"
   },
   "source": [
    "## Visualisation du CNN\n",
    "\n",
    "Une première manière d'introspecter un CNN est de visualiser les sorties des différentes couches et les filtres associés. Pour cela, on enregistre la sortie de chaque couche d'intérêt lors de la passe forward.\n",
    "Le code suivant permet d'obtenir cette succession d'images : la première image est l'image originale, chaque colonne correspond à un filtre. Les résultats sont ensuite regroupés par couche de convolution, les trois images dans une même colonne correspondent :1) aux poids de la convolution, 2) la sortie de la couche de convolution, 3) la sortie du pooling.\n",
    "\n",
    "Qu'observez vous ? Comparez les différences entre un réseau utilisant un max-pooling et un réseau utilisant un <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html\">average pooling</a>. Vous pouvez également faire varier la taille et le nombre de filtres.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:05:44.126371Z",
     "iopub.status.busy": "2022-01-30T14:05:44.126088Z",
     "iopub.status.idle": "2022-01-30T14:05:44.138181Z",
     "shell.execute_reply": "2022-01-30T14:05:44.137476Z",
     "shell.execute_reply.started": "2022-01-30T14:05:44.126340Z"
    }
   },
   "outputs": [],
   "source": [
    "def analyse_conv(model,img,nb_filtres=16):\n",
    "    print(img.shape)\n",
    "    x = img.unsqueeze(0).to(device) # Modified unsqueeze because of wrong shape\n",
    "    print(x.shape)\n",
    "    img_conv = []\n",
    "    img_pool = []\n",
    "    for m in model._modules.values(): # Deleted .features\n",
    "        print(\"Layers :\", m)\n",
    "        x = m.forward(x)\n",
    "        if isinstance(m,nn.Conv2d):\n",
    "            img_conv.append((x.squeeze(0),m.weight))\n",
    "        if isinstance(m,nn.MaxPool2d) or isinstance(m,nn.AvgPool2d):\n",
    "            img_pool.append(x.squeeze(0))\n",
    "    plt.figure()\n",
    "    plt.imshow(img.permute(1,2,0).to('cpu'),cmap='gray')\n",
    "    # nombre de filtres\n",
    "    ksmax = min(nb_filtres, max([p[0].size(0) for p in img_conv]))\n",
    "    fig, axs = plt.subplots(3*len(img_conv),ksmax,figsize=(20,5))\n",
    "    for i,((img_c,w),img_p) in enumerate(zip(img_conv,img_pool)):\n",
    "        for j in range(min(nb_filtres,img_c.size(0))):\n",
    "            axs[3*i,j].imshow(np.array(w[j,0].to('cpu').detach()),cmap=\"gray\")\n",
    "            axs[3*i+1,j].imshow(np.array(img_c[j].to('cpu').detach()),cmap=\"gray\")                             \n",
    "        for j in range(min(nb_filtres,img_p.size(0))):\n",
    "            axs[3*i+2,j].imshow(np.array(img_p[j].to('cpu').detach()),cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:05:44.599551Z",
     "iopub.status.busy": "2022-01-30T14:05:44.599010Z",
     "iopub.status.idle": "2022-01-30T14:05:44.603250Z",
     "shell.execute_reply": "2022-01-30T14:05:44.602582Z",
     "shell.execute_reply.started": "2022-01-30T14:05:44.599498Z"
    }
   },
   "outputs": [],
   "source": [
    "# analyse_conv(model, mnist_test_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNczoJevfoRn"
   },
   "source": [
    "## Saliency Map\n",
    "\n",
    "La visualisation des filtres ne permet pas bien de comprendre le rôle de chaque filtre dans la classification. Ils permettent d'extraire des features élémentaires qui combinées ensemble font sens pour un réseau fully-connected mais dont l'interprétation n'est pas évidente pour l'oeil humain.\n",
    "\n",
    "Une première méthode pour détecter quelles zones de l'image ont le plus impacté la décision sont les cartes de saillance. L'objectif des Saliency Maps est de détecter les pixels d'entrée qui ont le plus impacté la décision. L'idée est d'utiliser le gradient *par rapport* à l'image pour ranker les pixels. En effet, un gradient fort pour un pixel d'entrée indique qu'il faut changer faiblement sa valeur pour que la classe infére change (et a contrario, un gradient nul indique que le pixel n'est pas pris en compte pour la classification selon cette classe).\n",
    "Les étapes à suivre sont les suivantes :\n",
    "* le flag *requires_grad* est mis à True pour l'image (pour pouvoir calculer la rétro-propagation)\n",
    "* une passe forward est faite sur l'image\n",
    "* le backward est calculé sur le score de sortie de la classe d'intérêt\n",
    "* On affiche la valeur absolue du gradient  par rapport à l'entrée obtenu. Si l'image à plusieurs canaux, on prend le max de chacun de ces canaux.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:05:45.573123Z",
     "iopub.status.busy": "2022-01-30T14:05:45.572442Z",
     "iopub.status.idle": "2022-01-30T14:05:45.580329Z",
     "shell.execute_reply": "2022-01-30T14:05:45.579651Z",
     "shell.execute_reply.started": "2022-01-30T14:05:45.573085Z"
    },
    "id": "BheIXf_j25Af",
    "outputId": "b7bdc74b-b846-4607-c5cc-3e1416a26694"
   },
   "outputs": [],
   "source": [
    "def getSaliency(model,img,label):\n",
    "    model.zero_grad()\n",
    "    img = img.to(device)\n",
    "    img.requires_grad = True\n",
    "    img.grad = None\n",
    "    outputs = nn.Softmax(dim=1)(model(img.unsqueeze(0)))\n",
    "    output=outputs[0,label] \n",
    "    output.backward()\n",
    "    sal=img.grad.abs()\n",
    "    if sal.dim()>2:\n",
    "        sal=torch.max(sal,dim=0)[0]\n",
    "    fig=plt.figure(figsize=(8, 8))\n",
    "    fig.add_subplot(1, 2, 1)\n",
    "    plt.imshow(img.detach().cpu().permute(1,2,0),cmap=\"gray\")\n",
    "    fig.add_subplot(1, 2, 2)\n",
    "    plt.imshow(sal.to('cpu'),cmap=\"seismic\",interpolation=\"bilinear\")\n",
    "    return sal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:05:46.283794Z",
     "iopub.status.busy": "2022-01-30T14:05:46.282983Z",
     "iopub.status.idle": "2022-01-30T14:05:46.288135Z",
     "shell.execute_reply": "2022-01-30T14:05:46.287337Z",
     "shell.execute_reply.started": "2022-01-30T14:05:46.283743Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    x,y = mnist_train_loader.dataset[i]\n",
    "    # getSaliency(model,x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vg5QWPIXDh2j"
   },
   "source": [
    "## Données CIFAR\n",
    "\n",
    "La base de données CIFAR10  contient  60000 images couleur (RGB) 32x32 pixels. Les images appartiennent à 10 catégories (6000 images par classe): 'plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship' et 'truck'. Le\n",
    "dataset est composé de 50000 exemples d'apprentissage et 10000 de test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:05:47.425457Z",
     "iopub.status.busy": "2022-01-30T14:05:47.424852Z",
     "iopub.status.idle": "2022-01-30T14:05:47.431325Z",
     "shell.execute_reply": "2022-01-30T14:05:47.430282Z",
     "shell.execute_reply.started": "2022-01-30T14:05:47.425411Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_stats(dataloader):\n",
    "    n_batch = 0\n",
    "    chan = 0\n",
    "    chan_squared = 0\n",
    "    for elt, _ in dataloader:\n",
    "        chan += torch.mean(elt)\n",
    "        chan_squared += torch.mean(elt**2)\n",
    "        n_batch += 1\n",
    "    mean = chan/n_batch\n",
    "    std = np.sqrt((chan_squared/n_batch - mean**2))\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:05:50.710288Z",
     "iopub.status.busy": "2022-01-30T14:05:50.709999Z",
     "iopub.status.idle": "2022-01-30T14:06:04.423187Z",
     "shell.execute_reply": "2022-01-30T14:06:04.422452Z",
     "shell.execute_reply.started": "2022-01-30T14:05:50.710256Z"
    },
    "id": "riJMyn5UE91a",
    "outputId": "dae73d3e-e25b-4a2d-b61f-be3da78d8db4"
   },
   "outputs": [],
   "source": [
    "batchsize = 128              \n",
    "\n",
    "cifar_trainset = torchvision.datasets.CIFAR10(root='/tmp/data', train=True, download=True, transform=transforms.ToTensor())\n",
    "cifar_train_loader = torch.utils.data.DataLoader(cifar_trainset, batch_size=batchsize, pin_memory=True, shuffle=True)\n",
    "\n",
    "mean, std = get_stats(cifar_train_loader)\n",
    "\n",
    "print(mean)\n",
    "print(std)\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "\n",
    "cifar_trainset = torchvision.datasets.CIFAR10(root='/tmp/data', train=True, download=True, transform=transform)\n",
    "cifar_train_loader = torch.utils.data.DataLoader(cifar_trainset, batch_size=batchsize, pin_memory=True, shuffle=True)\n",
    "cifar_testset = torchvision.datasets.CIFAR10(root='/tmp/data', train=False, download=True, transform=transform)\n",
    "cifar_test_loader = torch.utils.data.DataLoader(cifar_testset, batch_size=batchsize, pin_memory=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:06:04.424974Z",
     "iopub.status.busy": "2022-01-30T14:06:04.424742Z",
     "iopub.status.idle": "2022-01-30T14:06:04.468953Z",
     "shell.execute_reply": "2022-01-30T14:06:04.468214Z",
     "shell.execute_reply.started": "2022-01-30T14:06:04.424940Z"
    }
   },
   "outputs": [],
   "source": [
    "print(cifar_trainset.classes)\n",
    "X_train, _ = next(iter(cifar_test_loader))\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Testez le réseau précédent avec 32 filtres et un réseau linéaire type *Linear(in_dim,120)->ReLU->Linear(120,80)->Relu->Linear(80,10)*  sur cette base de données et comparez les résultats. \n",
    "* Expérimenter également d'autres architectures de convolution (nombre de filtres, taille des filtres, différents strides, éventuellement padding). \n",
    "* Comparez le nombre de paramètres des réseaux\n",
    "* Visualisez la carte de saillance et les filtres du réseau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:07:49.382097Z",
     "iopub.status.busy": "2022-01-30T14:07:49.381514Z",
     "iopub.status.idle": "2022-01-30T14:07:49.386791Z",
     "shell.execute_reply": "2022-01-30T14:07:49.385730Z",
     "shell.execute_reply.started": "2022-01-30T14:07:49.382059Z"
    },
    "id": "fl9mgy3IEeJc",
    "outputId": "70b95a5f-74e0-4073-a760-4281c3fbc0a5"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:07:49.511266Z",
     "iopub.status.busy": "2022-01-30T14:07:49.511018Z",
     "iopub.status.idle": "2022-01-30T14:07:49.520133Z",
     "shell.execute_reply": "2022-01-30T14:07:49.517511Z",
     "shell.execute_reply.started": "2022-01-30T14:07:49.511236Z"
    }
   },
   "outputs": [],
   "source": [
    "## Définition du réseau feed-forward\n",
    "\n",
    "class FeedFor1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.name = \"FeedForwardV1\"\n",
    "        self.fc1 = nn.Linear(32*32, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,32*32)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.softmax(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:13:59.793630Z",
     "iopub.status.busy": "2022-01-30T14:13:59.792940Z",
     "iopub.status.idle": "2022-01-30T14:13:59.804407Z",
     "shell.execute_reply": "2022-01-30T14:13:59.803603Z",
     "shell.execute_reply.started": "2022-01-30T14:13:59.793596Z"
    }
   },
   "outputs": [],
   "source": [
    "## Définition du réseau convolutionnel\n",
    "## Utiliser nn.init.xavier_uniform pour l'initialisation des couches de convolutions\n",
    "\n",
    "class ConvNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.name = \"ConvNetV2\"\n",
    "        self.features()\n",
    "        self.classifier()\n",
    "        \n",
    "    def features(self):\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5)           # 32 - (5-1) / 3 = 9 --> 9x9 images with 32 filters\n",
    "        nn.init.xavier_uniform(self.conv1.weight)\n",
    "        self.conv2 = nn.Conv2d(32, 1024, 5)        # 9 - (5-1) / 3 = 1 --> 1x1 images with 1024 filters --> 1024 indim for classifier\n",
    "        nn.init.xavier_uniform(self.conv2.weight)\n",
    "        self.pool = nn.MaxPool2d(3, 3)    \n",
    "\n",
    "    def classifier(self):\n",
    "        self.fc1 = nn.Linear(1024, 128) \n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:13:59.927663Z",
     "iopub.status.busy": "2022-01-30T14:13:59.926779Z",
     "iopub.status.idle": "2022-01-30T14:13:59.952352Z",
     "shell.execute_reply": "2022-01-30T14:13:59.951744Z",
     "shell.execute_reply.started": "2022-01-30T14:13:59.927619Z"
    }
   },
   "outputs": [],
   "source": [
    "feed = FeedFor1()\n",
    "conv = ConvNet2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:14:03.970218Z",
     "iopub.status.busy": "2022-01-30T14:14:03.969659Z",
     "iopub.status.idle": "2022-01-30T14:14:18.466788Z",
     "shell.execute_reply": "2022-01-30T14:14:18.465968Z",
     "shell.execute_reply.started": "2022-01-30T14:14:03.970179Z"
    }
   },
   "outputs": [],
   "source": [
    "## Entraînement du réseau feed-forward\n",
    "train(feed, 1, cifar_train_loader, cifar_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:14:18.469077Z",
     "iopub.status.busy": "2022-01-30T14:14:18.468644Z",
     "iopub.status.idle": "2022-01-30T14:14:32.898099Z",
     "shell.execute_reply": "2022-01-30T14:14:32.897391Z",
     "shell.execute_reply.started": "2022-01-30T14:14:18.469036Z"
    }
   },
   "outputs": [],
   "source": [
    "# Entraînement du réseau convolutionnel\n",
    "train(conv, 1, cifar_train_loader, cifar_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:14:32.900020Z",
     "iopub.status.busy": "2022-01-30T14:14:32.899552Z",
     "iopub.status.idle": "2022-01-30T14:14:32.905513Z",
     "shell.execute_reply": "2022-01-30T14:14:32.904590Z",
     "shell.execute_reply.started": "2022-01-30T14:14:32.899982Z"
    }
   },
   "outputs": [],
   "source": [
    "# Affichage du nombre de paramètres\n",
    "print(\"Number of parameters for feedforward nn :\", count_parameters(feed))\n",
    "print(\"Number of parameters for convolutional nn :\", count_parameters(conv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:17:14.882224Z",
     "iopub.status.busy": "2022-01-30T14:17:14.881971Z",
     "iopub.status.idle": "2022-01-30T14:17:17.680919Z",
     "shell.execute_reply": "2022-01-30T14:17:17.680086Z",
     "shell.execute_reply.started": "2022-01-30T14:17:14.882197Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_test_data(dataloader, size):\n",
    "    X_test, Y_test = next(iter(dataloader))\n",
    "    batch_size = len(X_test)\n",
    "    n = size//batch_size\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        if i < n:\n",
    "            X_tmp, Y_tmp = batch\n",
    "            X_test = torch.cat((X_test, X_tmp), 0)\n",
    "            Y_test = torch.cat((Y_test, Y_tmp), 0)\n",
    "    return X_test, Y_test\n",
    "\n",
    "X_test, Y_test = get_test_data(cifar_test_loader, len(cifar_test_loader)*batchsize)\n",
    "\n",
    "X_test, Y_test = X_test.to(device), Y_test.to(device)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:17:17.682942Z",
     "iopub.status.busy": "2022-01-30T14:17:17.682361Z",
     "iopub.status.idle": "2022-01-30T14:17:17.791963Z",
     "shell.execute_reply": "2022-01-30T14:17:17.791209Z",
     "shell.execute_reply.started": "2022-01-30T14:17:17.682903Z"
    }
   },
   "outputs": [],
   "source": [
    "yha_feed = feed(X_test)\n",
    "print(yhat.shape)\n",
    "print(\"Acc for feedforward nn : \", accuracy(yha_feed, Y_test))\n",
    "\n",
    "yhat_conv = conv(X_test)\n",
    "print(yhat.shape)\n",
    "print(\"Acc for convolutional nn :\", accuracy(yhat_conv, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:17:25.960548Z",
     "iopub.status.busy": "2022-01-30T14:17:25.960244Z",
     "iopub.status.idle": "2022-01-30T14:17:25.968790Z",
     "shell.execute_reply": "2022-01-30T14:17:25.968091Z",
     "shell.execute_reply.started": "2022-01-30T14:17:25.960497Z"
    }
   },
   "outputs": [],
   "source": [
    "## Analyse des filtres du réseau\n",
    "# analyse_conv(conv, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:17:26.203694Z",
     "iopub.status.busy": "2022-01-30T14:17:26.203338Z",
     "iopub.status.idle": "2022-01-30T14:17:26.219578Z",
     "shell.execute_reply": "2022-01-30T14:17:26.218727Z",
     "shell.execute_reply.started": "2022-01-30T14:17:26.203648Z"
    },
    "id": "ng4hpbqNLed4",
    "outputId": "acbd7b8f-e41d-476a-bdc3-2b3323b4401d"
   },
   "outputs": [],
   "source": [
    "## Carte de saillance du réseau\n",
    "for i in range(10):\n",
    "    x,y = cifar_train_loader.dataset[i]\n",
    "    # getSaliency(conv,x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4m-5e7bOlnQ"
   },
   "source": [
    "# Data Augmentation\n",
    "\n",
    "Pour améliorer les résultats, une technique courante est d'augmenter les données par des variantes des images du corpus. Cela permet de gagner en robustesse vis à vis de diverses transformations en forçant le réseau à apprendre des invariants (e.g. d'échelle, de rotation, d'inversion, de luminosité, etc.). \n",
    "\n",
    "Insérez quelques transformations de données lors du chargement des données (la liste des transformations disponibles se trouvent dans <a href=https://pytorch.org/vision/stable/transforms.html> torchvision.transforms</a>, par exemple **RandomHorizontalFlip()**, **RandomResizedCrop()**) et relancez l'apprentissage pour voir l'effet. Les transformations sont à insérer dans le **transforms.Compose()** avant la transformation en tenseur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:17:27.650674Z",
     "iopub.status.busy": "2022-01-30T14:17:27.650209Z",
     "iopub.status.idle": "2022-01-30T14:17:35.714374Z",
     "shell.execute_reply": "2022-01-30T14:17:35.713589Z",
     "shell.execute_reply.started": "2022-01-30T14:17:27.650634Z"
    },
    "id": "4qHiO_6QPhkJ",
    "outputId": "978da56c-b6b3-448e-88ab-bc60ba77a3b4"
   },
   "outputs": [],
   "source": [
    "## Définition de la transformation pour Data Augmentation et création du réseau et des dataloader.\n",
    "batchsize = 64            \n",
    "\n",
    "cifar_trainset = torchvision.datasets.CIFAR10(root='/tmp/data', train=True, download=True, transform=transforms.ToTensor())\n",
    "cifar_train_loader = torch.utils.data.DataLoader(cifar_trainset, batch_size=batchsize, pin_memory=True, shuffle=True)\n",
    "\n",
    "mean, std = get_stats(cifar_train_loader)\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Normalize(mean, std),\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "#         transforms.RandomResizedCrop(32*32)\n",
    "    ])\n",
    "\n",
    "cifar_trainset = torchvision.datasets.CIFAR10(root='/tmp/data', train=True, download=True, transform=transform)\n",
    "cifar_train_loader = torch.utils.data.DataLoader(cifar_trainset, batch_size=batchsize, pin_memory=True, shuffle=True)\n",
    "cifar_testset = torchvision.datasets.CIFAR10(root='/tmp/data', train=False, download=True, transform=transform)\n",
    "cifar_test_loader = torch.utils.data.DataLoader(cifar_testset, batch_size=batchsize, pin_memory=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:17:35.716410Z",
     "iopub.status.busy": "2022-01-30T14:17:35.716080Z",
     "iopub.status.idle": "2022-01-30T14:17:35.721025Z",
     "shell.execute_reply": "2022-01-30T14:17:35.719986Z",
     "shell.execute_reply.started": "2022-01-30T14:17:35.716369Z"
    }
   },
   "outputs": [],
   "source": [
    "# import gc\n",
    "\n",
    "# def free_gpu_cache():\n",
    "#     print(\"Initial GPU Usage\")\n",
    "#     gpu_usage()     \n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "#     cuda.select_device(0)\n",
    "#     cuda.close()\n",
    "#     device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     cuda.select_device(0)\n",
    "#     print(\"GPU Usage after emptying the cache\")\n",
    "#     gpu_usage()\n",
    "#     return device\n",
    "\n",
    "# if feed : del feed\n",
    "# if conv : del conv\n",
    "# device = free_gpu_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:17:35.722741Z",
     "iopub.status.busy": "2022-01-30T14:17:35.722356Z",
     "iopub.status.idle": "2022-01-30T14:17:35.744507Z",
     "shell.execute_reply": "2022-01-30T14:17:35.743777Z",
     "shell.execute_reply.started": "2022-01-30T14:17:35.722615Z"
    }
   },
   "outputs": [],
   "source": [
    "conv = ConvNet2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:18:50.826546Z",
     "iopub.status.busy": "2022-01-30T14:18:50.826168Z",
     "iopub.status.idle": "2022-01-30T14:19:07.174544Z",
     "shell.execute_reply": "2022-01-30T14:19:07.173827Z",
     "shell.execute_reply.started": "2022-01-30T14:18:50.826485Z"
    }
   },
   "outputs": [],
   "source": [
    "## Apprentissage du réseau\n",
    "train(conv, 1, cifar_train_loader, cifar_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:19:07.176612Z",
     "iopub.status.busy": "2022-01-30T14:19:07.176187Z",
     "iopub.status.idle": "2022-01-30T14:19:10.663015Z",
     "shell.execute_reply": "2022-01-30T14:19:10.661374Z",
     "shell.execute_reply.started": "2022-01-30T14:19:07.176571Z"
    }
   },
   "outputs": [],
   "source": [
    "## Accuracy\n",
    "X_test, Y_test = get_test_data(cifar_test_loader, len(cifar_test_loader)*batchsize)\n",
    "X_test, Y_test = X_test.to(device), Y_test.to(device)\n",
    "print(\"Acc for convolutional nn :\", accuracy(conv(X_test), Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDFJTu95Hx7H"
   },
   "source": [
    "# Modèles pré-entraînés / Transfert\n",
    "\n",
    "PyTorch propose un certain nombre de modèles pré-entraînés sur le très gros corpus d'images ImageNet. Ces modèles très lourds demandent beaucoup de ressources pour être entraînés efficacement. Mais une fois leur entraînement effectué, ils peuvent être appliqués assez facilement sur d'autres corpus que ImageNet, moyennant quelques adaptations. Dans la suite nous considérons le modèle <a href=https://pytorch.org/hub/pytorch_vision_alexnet/>AlexNet</a> pour l'extraction de features. La sortie du réseau doit être adaptée et ré-entraînée pour permettre de classer des images sur notre corpus CIFAR. \n",
    "\n",
    "Commençons par collecter le réseau entraîné et étudions sa structure: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:19:40.833355Z",
     "iopub.status.busy": "2022-01-30T14:19:40.833088Z",
     "iopub.status.idle": "2022-01-30T14:19:42.825375Z",
     "shell.execute_reply": "2022-01-30T14:19:42.824538Z",
     "shell.execute_reply.started": "2022-01-30T14:19:40.833325Z"
    },
    "id": "Jsq26b2sJIfp",
    "outputId": "7dcb6954-fca9-4fa2-f5e1-15d83188e1cf"
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "print(alexnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning d'AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2d7RA8NMRn0"
   },
   "source": [
    "Que faut-il modifier pour l'adapter à notre cas ? En outre on aimerait que lors de l'apprentissage seuls les poids des modules modifiés soient ajustés. Penser à fixer les autres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:19:45.831727Z",
     "iopub.status.busy": "2022-01-30T14:19:45.831446Z",
     "iopub.status.idle": "2022-01-30T14:19:46.185140Z",
     "shell.execute_reply": "2022-01-30T14:19:46.184408Z",
     "shell.execute_reply.started": "2022-01-30T14:19:45.831696Z"
    }
   },
   "outputs": [],
   "source": [
    "# Il faut modifier le nombre de neurones en sortie car nous n'avons que 10 classes à prédire.\n",
    "# Il faut modifier le nombre de neurones sur les couches dans le classifieur car il risque fortement d'overfit, du fait de notre nombre de classes.\n",
    "\n",
    "alexnet.classifier[1] = nn.Linear(9216, 2048)\n",
    "alexnet.classifier[4] = nn.Linear(2048, 1024)\n",
    "alexnet.classifier[6] = nn.Linear(1024, 10)\n",
    "alexnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:19:48.159573Z",
     "iopub.status.busy": "2022-01-30T14:19:48.159289Z",
     "iopub.status.idle": "2022-01-30T14:19:48.167901Z",
     "shell.execute_reply": "2022-01-30T14:19:48.167151Z",
     "shell.execute_reply.started": "2022-01-30T14:19:48.159523Z"
    },
    "id": "CQND64P3Me9E",
    "outputId": "af363505-a80d-4133-a975-8b3b7ac16ca5"
   },
   "outputs": [],
   "source": [
    "# Ici nous voulons entrainer seulement la partie classifieur et non la partie feature extracting du CNN\n",
    "# Permet de mettre à True/False tous les requires_grad des paramètres du réseau\n",
    "\n",
    "def set_parameter_requires_grad(model, feature_extract):\n",
    "    if feature_extract:\n",
    "        for name,p in model.named_parameters():\n",
    "            if \"features\" in name:\n",
    "                p.requires_grad = False    \n",
    "            else:\n",
    "                p.requires_grad = True    \n",
    "            \n",
    "set_parameter_requires_grad(alexnet, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zn4ee8T2OF1S"
   },
   "source": [
    "Il s'agit également de remettre le modèle dans les mêmes conditions qu'il a été appris (taille de l'entrée 224, normalisation selon moyennes et variances de ImageNet, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:19:50.104890Z",
     "iopub.status.busy": "2022-01-30T14:19:50.104329Z",
     "iopub.status.idle": "2022-01-30T14:19:57.483256Z",
     "shell.execute_reply": "2022-01-30T14:19:57.482464Z",
     "shell.execute_reply.started": "2022-01-30T14:19:50.104850Z"
    },
    "id": "6ck8ws25NqJY",
    "outputId": "3f35442f-8211-4c10-dddd-90232d37913c"
   },
   "outputs": [],
   "source": [
    "input_size=224\n",
    "\n",
    "mean=[0.485, 0.456, 0.406]\n",
    "std=[0.229, 0.224, 0.225]\n",
    "\n",
    "transformAlexTrain=transforms.Compose([ # Cette fois on utilise pas de grayscale car nous avons un gros modele pré-entrainé\n",
    "        transforms.RandomResizedCrop(input_size), # selection aléatoire d'une zone de la taille voulue (augmentation des données en apprentissage)\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "transformAlexTest=transforms.Compose([\n",
    "        transforms.Resize(input_size), # selection de la zone centrale de la taille voulue\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "\n",
    "alex_trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transformAlexTrain)\n",
    "alex_trainloader = torch.utils.data.DataLoader(alex_trainset, batch_size=batchsize, pin_memory=True, shuffle=True)\n",
    "\n",
    "alex_testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transformAlexTest)\n",
    "alex_testloader = torch.utils.data.DataLoader(alex_testset, batch_size=batchsize, pin_memory=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:22:24.539149Z",
     "iopub.status.busy": "2022-01-30T14:22:24.538896Z",
     "iopub.status.idle": "2022-01-30T14:22:24.551475Z",
     "shell.execute_reply": "2022-01-30T14:22:24.550660Z",
     "shell.execute_reply.started": "2022-01-30T14:22:24.539122Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model,epochs,train_loader,test_loader,feature_extract=False):\n",
    "    model = model.to(device)\n",
    "    writer = SummaryWriter(f\"{TB_PATH}/{model.name}\")\n",
    "    \n",
    "    params_to_update = model.parameters()\n",
    "    print(\"Params to learn:\")\n",
    "    if feature_extract:\n",
    "        params_to_update = []\n",
    "        for name,param in model.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                params_to_update.append(param)\n",
    "                print(name)\n",
    "    else:\n",
    "        for name,param in model.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                print(name)\n",
    "    optim = torch.optim.Adam(params_to_update,lr=1e-3)\n",
    "    \n",
    "    print(f\"running {model.name}\")\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        cumloss, cumacc, count = 0, 0, 0\n",
    "        model.train()\n",
    "        for x,y in train_loader:\n",
    "            optim.zero_grad()\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            yhat = model(x)\n",
    "            l = loss(yhat,y)\n",
    "            l.backward()\n",
    "            optim.step()\n",
    "            cumloss += l*len(x)\n",
    "            cumacc += accuracy(yhat,y)*len(x)\n",
    "            count += len(x)\n",
    "        writer.add_scalar('loss/train',cumloss/count,epoch)\n",
    "        writer.add_scalar('accuracy/train',cumacc/count,epoch)\n",
    "        if epoch % 1 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                cumloss, cumacc, count = 0, 0, 0\n",
    "                for x,y in test_loader:\n",
    "                    x,y = x.to(device), y.to(device)\n",
    "                    yhat = model(x)\n",
    "                    cumloss += loss(yhat,y)*len(x)\n",
    "                    cumacc += accuracy(yhat,y)*len(x)\n",
    "                    count += len(x)\n",
    "                writer.add_scalar(f'loss/test',cumloss/count,epoch)\n",
    "                writer.add_scalar('accuracy/test',cumacc/count,epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faites le Fine-tuning de alexnet sur les données CIFAR. Regardez les cartes de saillances obtenues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:22:30.606430Z",
     "iopub.status.busy": "2022-01-30T14:22:30.605909Z",
     "iopub.status.idle": "2022-01-30T14:24:20.292296Z",
     "shell.execute_reply": "2022-01-30T14:24:20.291594Z",
     "shell.execute_reply.started": "2022-01-30T14:22:30.606396Z"
    },
    "id": "fBas4XoPdBnh",
    "outputId": "22f467dd-7bb5-4324-82e6-094eae781010"
   },
   "outputs": [],
   "source": [
    "## Entraînement du réseau\n",
    "alexnet.name = \"AlexNet\"\n",
    "train(alexnet, 1, alex_trainloader, alex_testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:24:35.729314Z",
     "iopub.status.busy": "2022-01-30T14:24:35.729059Z",
     "iopub.status.idle": "2022-01-30T14:24:55.519884Z",
     "shell.execute_reply": "2022-01-30T14:24:55.519088Z",
     "shell.execute_reply.started": "2022-01-30T14:24:35.729286Z"
    }
   },
   "outputs": [],
   "source": [
    "## Accuracy\n",
    "X_test, Y_test = get_test_data(alex_testloader, 1000) \n",
    "X_test, Y_test = X_test.to(device), Y_test.to(device)\n",
    "print(\"Acc for alexnet transfer learning :\", accuracy(alexnet(X_test), Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:25:02.147595Z",
     "iopub.status.busy": "2022-01-30T14:25:02.147103Z",
     "iopub.status.idle": "2022-01-30T14:25:02.151399Z",
     "shell.execute_reply": "2022-01-30T14:25:02.150026Z",
     "shell.execute_reply.started": "2022-01-30T14:25:02.147554Z"
    },
    "id": "HJ3eACA8p2Ev",
    "outputId": "18813f4d-a5af-41de-9d08-2132ba0608c3"
   },
   "outputs": [],
   "source": [
    "## Carte de saillance du réseau\n",
    "# inputs,labels=iter(alex_testloader).next()\n",
    "# for i in range(len(cifar_trainset.classes)):\n",
    "#     print(\"Pour \",cifar_trainset.classes[i])\n",
    "#     getSaliency(alexnet,inputs[0],i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWh_Nx-4CowB"
   },
   "source": [
    "## Class Activation Maps (CAM)\n",
    "Une autre technique d'introspection est le Class Activation Maps. Cette technique permet de visualiser quels sont les régions qui ont fait le plus réagir les différents filtres qui ont servis à la classification. Elle part de la constatation que la sortie d'un filtre de la dernière couche convolutionnelle indique spatialement quelles sont les régions de l'image qui ont fait réagir le filtre (la sortie est généralement de taille plus petite - *downscalé* - mais on peut la mettre à l'échelle). Cependant, il est difficile d'analyser avec la succession des couches non-linéaires en aval le rôle de chaque sortie convolutionnelle dans le processus de classification. Cependant, un réseau plus simple - uniquement linéaire par exemple - permettrait de donner une indication à l'importance de chaque filtre (au prix d'une erreur plus grosse en classification). \n",
    "Cette technique nécessite donc la modification des dernières couches du réseau de la manière suivante : \n",
    "* un pooling de moyennage globale (un average pooling de la taille de l'image) est appliquée à chaque filtre de convolution de la dernière couche convolutionnelle : seul le signal moyen de chaque filtre est retenu, sans plus aucune information spatiale.\n",
    "* un réseau linéaire est ensuite utilisé du nombre de filtres vers le nombre de classes qui va permettre de mettre en évidence l'intérêt de chaque filtre dans la classification.\n",
    "\n",
    "Le réseau ainsi modifié est fine-tuné sur le corpus. L'Activation Map est obtenu en sommant les sorties de la dernière couche convolutionnelle pondérées par les poids du réseau linéaire. \n",
    "Modifiez le réseau, puis ré-entraîneé les couches modifiées. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:25:04.098239Z",
     "iopub.status.busy": "2022-01-30T14:25:04.097389Z",
     "iopub.status.idle": "2022-01-30T14:25:04.102894Z",
     "shell.execute_reply": "2022-01-30T14:25:04.101834Z",
     "shell.execute_reply.started": "2022-01-30T14:25:04.098192Z"
    }
   },
   "outputs": [],
   "source": [
    "# Couche identité\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:25:04.952522Z",
     "iopub.status.busy": "2022-01-30T14:25:04.952005Z",
     "iopub.status.idle": "2022-01-30T14:25:05.782944Z",
     "shell.execute_reply": "2022-01-30T14:25:05.782164Z",
     "shell.execute_reply.started": "2022-01-30T14:25:04.952477Z"
    },
    "id": "1uRKU2UsDMK8",
    "outputId": "109481d4-b844-4cb7-e23d-b85743fb5342"
   },
   "outputs": [],
   "source": [
    "## Remplacement de la couche classifier par un module d'average pooling et un linéaire.\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "print(alexnet)\n",
    "alexnet.features[12] = nn.AvgPool2d(kernel_size=3, stride=2, padding=0, ceil_mode=False)\n",
    "alexnet.classifier[0] = Identity()\n",
    "alexnet.classifier[1] = Identity()\n",
    "alexnet.classifier[2] = Identity()\n",
    "alexnet.classifier[3] = Identity()\n",
    "alexnet.classifier[4] = Identity()\n",
    "alexnet.classifier[5] = Identity()\n",
    "alexnet.classifier[6] = nn.Linear(9216, 10)\n",
    "print(alexnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:25:19.222158Z",
     "iopub.status.busy": "2022-01-30T14:25:19.221897Z",
     "iopub.status.idle": "2022-01-30T14:27:05.652075Z",
     "shell.execute_reply": "2022-01-30T14:27:05.651360Z",
     "shell.execute_reply.started": "2022-01-30T14:25:19.222129Z"
    },
    "id": "s4YjVJwRHvd9",
    "outputId": "26704c22-80eb-4abe-8a16-eba567f02991"
   },
   "outputs": [],
   "source": [
    "## Entrainement du réseau\n",
    "set_parameter_requires_grad(alexnet, True)\n",
    "alexnet.name = \"AlexNetAvg\"\n",
    "train(alexnet, 1, alex_trainloader, alex_testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-egbZeDHRxu"
   },
   "source": [
    "Il ne reste plus qu'à écrire la fonction generate_cam qui affiche une image d'activation par classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:27:41.238308Z",
     "iopub.status.busy": "2022-01-30T14:27:41.237583Z",
     "iopub.status.idle": "2022-01-30T14:27:41.248774Z",
     "shell.execute_reply": "2022-01-30T14:27:41.248055Z",
     "shell.execute_reply.started": "2022-01-30T14:27:41.238268Z"
    },
    "id": "7nJX5DzTcXRa"
   },
   "outputs": [],
   "source": [
    "def generate_cam(model,input_image,target_class=None):\n",
    "    ## Calcul du forward sur l'image\n",
    "    with torch.no_grad():\n",
    "        input_image=input_image.to(device)\n",
    "        x = model.features(input_image)\n",
    "        out=model.classifier(x)\n",
    "        out=torch.nn.functional.softmax(out,-1)\n",
    "    if target_class is None:\n",
    "        target_class = torch.max(out,dim=-1)[1].item()\n",
    "    print(\"target_class\",target_class)\n",
    "    ## Récupération des poids du linéaire\n",
    "    weights = dict(model.classifier.named_modules())[\"Linear\"].weight.data  \n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "    fig.add_subplot(1,2, 1)\n",
    "    img=input_image.to(\"cpu\")*torch.tensor(std).view(3,1,1)+torch.tensor(mean).view(3,1,1)\n",
    "    img=torch.nn.functional.interpolate(img, size=(244, 244), mode=\"bilinear\", align_corners=False)\n",
    "    plt.imshow(img.cpu().squeeze().permute(1,2,0))\n",
    "    ## Calcul de CAM\n",
    "    y=x*weights[target_class].view(1,-1,1,1)\n",
    "    y=(y.sum(1))  \n",
    "    fig.add_subplot(1, 2, 2)\n",
    "    y=torch.nn.functional.interpolate(y.unsqueeze(0),size=(244,244),mode=\"bilinear\",align_corners=False)\n",
    "    plt.imshow(y.cpu().squeeze(),cmap=\"afmhot\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:27:42.318834Z",
     "iopub.status.busy": "2022-01-30T14:27:42.317940Z",
     "iopub.status.idle": "2022-01-30T14:27:42.323160Z",
     "shell.execute_reply": "2022-01-30T14:27:42.322293Z",
     "shell.execute_reply.started": "2022-01-30T14:27:42.318780Z"
    },
    "id": "pH1DGZd225Ah",
    "outputId": "f3a1381f-29a3-4f13-a005-f30d4cab671b"
   },
   "outputs": [],
   "source": [
    "# inputs,labels=iter(alex_trainloader).next()\n",
    "# generate_cam(alexnet,inputs[0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qW-1Yc8QehFU"
   },
   "source": [
    "On peut aussi charger des images du web et voir ce que notre classifieur donne. Par exemple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:27:42.637620Z",
     "iopub.status.busy": "2022-01-30T14:27:42.636911Z",
     "iopub.status.idle": "2022-01-30T14:27:42.641502Z",
     "shell.execute_reply": "2022-01-30T14:27:42.640669Z",
     "shell.execute_reply.started": "2022-01-30T14:27:42.637580Z"
    },
    "id": "-9iglZFY25Ak",
    "outputId": "54b0b02f-74ce-4585-a272-b43d2989440d"
   },
   "outputs": [],
   "source": [
    "# !wget \"https://www.fidanimo.com/sites/default/files/2020-10/dog-sitter.jpg\"\n",
    "# !wget https://assets.siemens-energy.com/siemens/assets/api/uuid:78a9c83e-219e-4fd5-b948-2bf07276916d/width:640/quality:high/4320x3240-keyvisual-cargo.jpg\n",
    "# from PIL import Image\n",
    "# imageDog = transformAlexTest(Image.open(\"dog-sitter.jpg\")).unsqueeze(0).to(device, torch.float)\n",
    "# imageShip = transformAlexTest(Image.open(\"4320x3240-keyvisual-cargo.jpg\")).unsqueeze(0).to(device,torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T14:27:42.729203Z",
     "iopub.status.busy": "2022-01-30T14:27:42.728736Z",
     "iopub.status.idle": "2022-01-30T14:27:42.733051Z",
     "shell.execute_reply": "2022-01-30T14:27:42.732391Z",
     "shell.execute_reply.started": "2022-01-30T14:27:42.729168Z"
    },
    "id": "4jvss7_a25Am",
    "outputId": "3137f78a-f047-4e10-823c-9d5e092baf7f"
   },
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     print(cifar_trainset.classes[i])\n",
    "#     generate_cam(alexnet,imageDog,i)\n",
    "# generate_cam(alexnet,imageShip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
